@report{handout,
   author = {Trodden P},
   title = {ACS6116 - Advanced Control Handout},
   url = {http://www.apache.org/licenses/},
   year = {2023},
}
@generic{DeAngelis2019,
   abstract = {All basic processes of ecological populations involve decisions; when and where to move, when and what to eat, and whether to fight or flee. Yet decisions and the underlying principles of decision-making have been difficult to integrate into the classical population-level models of ecology. Certainly, there is a long history of modeling individuals' searching behavior, diet selection, or conflict dynamics within social interactions. When all the individuals are given certain simple rules to govern their decision-making processes, the resultant population-level models have yielded important generalizations and theory. But it is also recognized that such models do not represent the way real individuals decide on actions. Factors that influence a decision include the organism's environment with its dynamic rewards and risks, the complex internal state of the organism, and its imperfect knowledge of the environment. In the case of animals, it may also involve complex social factors, and experience and learning, which vary among individuals. The way that all factors are weighed and processed to lead to decisions is a major area of behavioral theory. While classic population-level modeling is limited in its ability to integrate decision-making in its actual complexity, the development of individual- or agent-based models (IBM/ABMs) (we use ABM throughout to designate both "agent-based modeling" and an "agent-based model") has opened the possibility of describing the way that decisions are made, and their effects, in minute detail. Over the years, these models have increased in size and complexity. Current ABMs can simulate thousands of individuals in realistic environments, and with highly detailed internal physiology, perception and ability to process the perceptions and make decisions based on those and their internal states. The implementation of decision-making in ABMs ranges from fairly simple to highly complex; the process of an individual deciding on an action can occur through the use of logical and simple (if-then) rules to more sophisticated neural networks and genetic algorithms. The purpose of this paper is to give an overview of the ways in which decisions are integrated into a variety of ABMs and to give a prospectus on the future of modeling of decisions in ABMs.},
   author = {Donald L. DeAngelis and Stephanie G. Diaz},
   doi = {10.3389/fevo.2018.00237},
   issn = {2296701X},
   issue = {JAN},
   journal = {Frontiers in Ecology and Evolution},
   keywords = {Agent-based models,Artificial neural networks,Genetic algorithms,Optimization,Proximate decisions,Spatially explicit models},
   month = {1},
   publisher = {Frontiers Media S.A.},
   title = {Decision-making in agent-based modeling: A current review and future prospectus},
   volume = {6},
   year = {2019},
}
@report{Mulia2012,
   abstract = {Objective: Experiences of racial/ethnic bias and unfair treatment are risk factors for alcohol problems, and population differences in exposure to these social adversities (i.e., differential exposure) may contribute to alcohol-related disparities. Differential vulnerability is another plausible mechanism underlying health disparities, yet few studies have examined whether populations differ in their vulnerability to the effects of social adversity on psychological stress and the effects of psychological stress on alcohol problems. Method: Data from the 2005 U.S. National Alcohol Survey (N = 4,080 adult drinkers) were analyzed using structural equation modeling to assess an overall model of pathways linking social adversity, depressive symptoms, heavy drinking , and alcohol dependence. Multiple group analyses were conducted to assess differences in the model's relationships among Blacks versus Whites, Hispanics versus Whites, and the poor (income below the federal poverty line) versus non-poor (income above the poverty line). Results: The overall model explained 48% of the variance in alcohol dependence and revealed signifi cant pathways between social adversity and alcohol dependence involving depressive symptoms and heavy drinking. The effects of social adversity and depressive symptoms were no different among Blacks and Hispanics compared with Whites. However, the poor (vs. non-poor) showed stronger associations between unfair treatment and depressive symptoms and between depressive symptoms and heavy drinking. Conclusions: Contrary to some prior studies, these fi ndings suggest that racial disparities in alcohol problems may be more a function of racial/ethnic minorities' greater exposure, rather than vulnerability , to chronic stressors such as social adversity. However, observed differences between the poor and non-poor imply that differential vulnerability contributes to socioeconomic disparities in alcohol problems. Efforts to reduce both differential exposure and vulnerability might help to mitigate these disparities. (J.},
   author = {Nina Mulia and Sarah E Zemore},
   journal = {JOURNAL OF STUDIES ON ALCOHOL AND DRUGS},
   pages = {570-580},
   title = {Social Adversity, Stress, and Alcohol Problems: Are Racial/ Ethnic Minorities and the Poor More Vulnerable?},
   volume = {73},
   year = {2012},
}
@web_page{,
   title = {CC | Pareto efficient allocations and fairness in economics},
   url = {https://blog.cambridgecoaching.com/pareto-efficient-allocations-and-fairness-in-economics},
}
@article{Steinbacher2021,
   abstract = {In this review we discuss advances in the agent-based modeling of economic and social systems. We show the state of the art of the heuristic design of agents and how behavioral economics and laboratory experiments have improved the modeling of agent behavior. We further discuss how economic networks and social systems can be modeled and we discuss novel methodology and data sources. Lastly, we present an overview of estimation techniques to calibrate and validate agent-based models and show avenues for future research.},
   author = {Mitja Steinbacher and Matthias Raddant and Fariba Karimi and Eva Camacho Cuena and Simone Alfarano and Giulia Iori and Thomas Lux},
   doi = {10.1007/s43546-021-00103-3},
   issue = {7},
   journal = {SN Business & Economics},
   month = {7},
   publisher = {Springer Science and Business Media LLC},
   title = {Advances in the agent-based modeling of economic and social behavior},
   volume = {1},
   year = {2021},
}
@book_section{,
   author = {Fernando Sancho-Caparrini and Juan Luis Suárez},
   doi = {10.4324/9781003025245-17},
   journal = {Handbook of Computational Social Science, Volume 2},
   month = {11},
   pages = {229-243},
   publisher = {Routledge},
   title = {Agent-based modelling for cultural networks},
   year = {2021},
}
@book_section{Sieck2010,
   abstract = {The purpose of this chapter is to describe a rigorous, end-to-end methodology for modeling culture as networks of ideas that are distributed among members of a population. The method, Cultural Network Analysis (CNA), represents an interdisciplinary synthesis of techniques drawn from the fields of cognitive anthropology, cultural and cognitive psychology, naturalistic decision making, and decision analysis. CNA is used to develop cultural models for groups and populations, typically depicted as a network representation of the culturally shared concepts, causal beliefs, and values that influence key decisions. CNA can be usefully employed for a variety of applications, including the design of tools to support multinational collaborative planning and decision making, the development of situated cultural training programs, and characterizing the cognition of target audiences to support strategic communications campaigns. © 2010, IGI Global.},
   author = {Winston R. Sieck and Louise J. Rasmussen and Paul Smart},
   doi = {10.4018/978-1-61520-855-5.ch011},
   isbn = {9781615208555},
   journal = {Network Science for Military Coalition Operations: Information Exchange and Interaction},
   pages = {237-255},
   publisher = {IGI Global},
   title = {Cultural network analysis: A cognitive approach to cultural modeling},
   year = {2010},
}
@article{,
   abstract = {<p>Recent large-N studies of civil war conclude that inequality does not increase the risk of violent conflict. This article argues that such conclusions may be premature because these studies, which usually test the conflict potential of `vertical inequality' (i.e. income inequality between individuals), tend to neglect the group aspect of inequality. Case studies suggest that what matters for conflict is a concept closely linked to both economic and ethnic polarization: `horizontal inequalities', or inequalities that coincide with identity-based cleavages. Horizontal inequalities may enhance both grievances and group cohesion among the relatively deprived and thus facilitate mobilization for conflict. This article provides a quantitative test of this argument, exploring whether various forms of polarization and horizontal inequalities affect the probability of civil conflict onset across 36 developing countries in the period 1986—2004. National household data from the Demographic and Health Surveys (DHS) are used to construct measures of ethnic, social and economic polarization, as well as vertical and horizontal inequalities along two dimensions: social and economic. The article also introduces a combined measure of ethnic/socio-economic polarization as an alternative to the horizontal inequality measure. Robust results from panel and cross-section analyses show that social polarization and horizontal social inequality are positively related to conflict outbreak. Variables for purely ethnic polarization, inter-individual inequalities and combined ethnic/socio-economic polarization are not significant.</p>},
   author = {Gudrun Østby},
   doi = {10.1177/0022343307087169},
   issn = {0022-3433},
   issue = {2},
   journal = {Journal of Peace Research},
   month = {3},
   pages = {143-162},
   title = {Polarization, Horizontal Inequalities and Violent Civil Conflict},
   volume = {45},
   year = {2008},
}
@article{Taylor2007,
   author = {Mark Patrick Taylor},
   doi = {10.1007/s10745-007-9111-z},
   issn = {0300-7839},
   issue = {6},
   journal = {Human Ecology},
   month = {10},
   pages = {775-776},
   title = {The Drivers of Immigration in Contemporary Society: Unequal Distribution of Resources and Opportunities},
   volume = {35},
   year = {2007},
}
@article{Larson1990,
   abstract = {<p>Two major drought episodes, A.D. 1000 to 1015 and A.D. 1120 to 1150, contributed to significant change in adaptive strategies of the Virgin Branch Anasazi, a prehistoric population that occupied the southwestern Great Basin between A.D. 100 and A.D. 1150. The first extreme climatic event promoted the adoption of several alternative buffering strategies including intensive agricultural practices, increased reliance on storage, and the organization of large residential labor groups. The second drought, which followed 150 years of favorable climatic conditions and high levels of population growth, had a devastating impact upon the Virgin Branch Anasazi resulting in the complete abandonment of the southwestern Great Basin by that group. These two climatic events required entirely different responses, which suggest that shifts in climate are best viewed as triggering culture change. The preconditions of population growth set the various levels of sensitivity to extreme climatic events and determine the precise nature of the culture changes.</p>},
   author = {Daniel O. Larson and Joel Michaelsen},
   doi = {10.2307/281645},
   issn = {0002-7316},
   issue = {2},
   journal = {American Antiquity},
   month = {4},
   pages = {227-249},
   title = {Impacts of Climatic Variabiity and Population Growth on Virgin Branch Anasazi Cultural Developments},
   volume = {55},
   year = {1990},
}
@web_page{,
   title = {Riddles of the Anasazi | History| Smithsonian Magazine},
   url = {https://www.smithsonianmag.com/history/riddles-of-the-anasazi-85274508/},
}
@article{,
   author = {Danilo Brozović},
   doi = {10.1016/j.futures.2022.103075},
   issn = {00163287},
   journal = {Futures},
   month = {1},
   pages = {103075},
   title = {Societal collapse: A literature review},
   volume = {145},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0016328722001768},
   year = {2023},
}
@article{Steinbacher2021,
   abstract = {In this review we discuss advances in the agent-based modeling of economic and social systems. We show the state of the art of the heuristic design of agents and how behavioral economics and laboratory experiments have improved the modeling of agent behavior. We further discuss how economic networks and social systems can be modeled and we discuss novel methodology and data sources. Lastly, we present an overview of estimation techniques to calibrate and validate agent-based models and show avenues for future research.},
   author = {Mitja Steinbacher and Matthias Raddant and Fariba Karimi and Eva Camacho Cuena and Simone Alfarano and Giulia Iori and Thomas Lux},
   doi = {10.1007/s43546-021-00103-3},
   issue = {7},
   journal = {SN Business & Economics},
   month = {7},
   publisher = {Springer Science and Business Media LLC},
   title = {Advances in the agent-based modeling of economic and social behavior},
   volume = {1},
   year = {2021},
}
@book_section{Dean2000,
   author = {Jeffrey S. Dean and George J. Gumerman},
   doi = {10.1093/oso/9780195131673.003.0013},
   journal = {Dynamics in Human and Primate Societies},
   month = {2},
   pages = {179-206},
   publisher = {Oxford University Press},
   title = {Understanding Anasazi Culture Change Through Agent-Based Modeling},
   year = {2000},
}
@web_page{,
   title = {Chronology of the Ancestral Anasazi Pueblo People},
   url = {https://www.thoughtco.com/anasazi-timeline-ancestral-pueblo-people-169483},
}
@article{Grimm2006,
   abstract = {Simulation models that describe autonomous individual organisms (individual based models, IBM) or agents (agent-based models, ABM) have become a widely used tool, not only in ecology, but also in many other disciplines dealing with complex systems made up of autonomous entities. However, there is no standard protocol for describing such simulation models, which can make them difficult to understand and to duplicate. This paper presents a proposed standard protocol, ODD, for describing IBMs and ABMs, developed and tested by 28 modellers who cover a wide range of fields within ecology. This protocol consists of three blocks (Overview, Design concepts, and Details), which are subdivided into seven elements: Purpose, State variables and scales, Process overview and scheduling, Design concepts, Initialization, Input, and Submodels. We explain which aspects of a model should be described in each element, and we present an example to illustrate the protocol in use. In addition, 19 examples are available in an Online Appendix. We consider ODD as a first step for establishing a more detailed common format of the description of IBMs and ABMs. Once initiated, the protocol will hopefully evolve as it becomes used by a sufficiently large proportion of modellers. © 2006 Elsevier B.V. All rights reserved.},
   author = {Volker Grimm and Uta Berger and Finn Bastiansen and Sigrunn Eliassen and Vincent Ginot and Jarl Giske and John Goss-Custard and Tamara Grand and Simone K. Heinz and Geir Huse and Andreas Huth and Jane U. Jepsen and Christian Jørgensen and Wolf M. Mooij and Birgit Müller and Guy Pe'er and Cyril Piou and Steven F. Railsback and Andrew M. Robbins and Martha M. Robbins and Eva Rossmanith and Nadja Rüger and Espen Strand and Sami Souissi and Richard A. Stillman and Rune Vabø and Ute Visser and Donald L. DeAngelis},
   doi = {10.1016/j.ecolmodel.2006.04.023},
   issn = {03043800},
   issue = {1-2},
   journal = {Ecological Modelling},
   keywords = {Agent-based model,Individual-based model,Model description,Scientific communication,Standardization},
   month = {9},
   pages = {115-126},
   title = {A standard protocol for describing individual-based and agent-based models},
   volume = {198},
   year = {2006},
}
@report{,
   abstract = {This article presents an agent-based computational model of civil violence. Two variants of the civil violence model are presented. In the first a central authority seeks to suppress decentralized rebellion. In the second a central authority seeks to suppress communal violence between two warring ethnic groups.},
   author = {Joshua M Epstein},
   title = {Modeling civil violence: An agent-based computational approach},
   url = {www.pnas.orgcgidoi10.1073pnas.092080199},
}
@article{,
   title = {L2 bersini_2012},
}
@article{Collier2013,
   abstract = {In the last decade, agent-based modeling and simulation (ABMS) has been applied to a variety of domains, demonstrating the potential of this technique to advance science, engineering, and policy analysis. However, realizing the full potential of ABMS to find breakthrough research results requires far greater computing capability than is available through current ABMS tools. The Repast for High Performance Computing (Repast HPC) project addresses this need by developing a useful and useable next-generation ABMS system explicitly focusing on larger-scale distributed computing platforms. Repast HPC is intended to smooth the path from small-scale simulations to large-scale distributed simulations through the use of a Logo-like system. This article’s contribution is its detailed presentation of the implementation of Repast HPC as a useful and usable framework, a complete ABMS platform developed explicitly for larger-scale distributed computing systems that leverages modern C++ techniques and the ReLogo language. © 2012, The Society for Modeling and Simulation International. All rights reserved.},
   author = {Nicholson Collier and Michael North},
   doi = {10.1177/0037549712462620},
   issn = {17413133},
   issue = {10},
   journal = {SIMULATION},
   keywords = {agent-based modeling and simulation,high-performance computing,parallel and distributed computing,simulation framework},
   pages = {1215-1235},
   title = {Parallel agent-based simulation with Repast for High Performance Computing},
   volume = {89},
   year = {2013},
}
@report{Janssen2009,
   abstract = {A replication and analysis of the Artificial Anasazi model is presented. It is shown that the success of replicating historical data is based on two parameters that adjust the carrying capacity of the Long House Valley. Compared to population estimates equal to the carrying capacity the specific agent behavior contributes only a modest improvement of the model to fit the archaeological records.},
   author = {Marco A Janssen},
   keywords = {11 11 12 12 Replication,Model Analysis,Model-Based Archaeology,Population Dynamics,Social},
   month = {10},
   title = {Understanding Artificial Anasazi},
   url = {http://jasss.soc.surrey.ac.uk/12/4/13.html},
   year = {2009},
}
@report{,
   abstract = {Long House Valley in the Black Mesa area of northeastern Arizona (U.S.) was inhabited by the Kayenta Anasazi from about 1800 before Christ to about anno Domini 1300. These people were prehistoric ancestors of the modern Pueblo cultures of the Colo-rado Plateau. Paleoenvironmental research based on alluvial geo-morphology, palynology, and dendroclimatology permits accurate quantitative reconstruction of annual fluctuations in potential agricultural production (kg of maize per hectare). The archaeological record of Anasazi farming groups from anno Domini 200-1300 provides information on a millennium of sociocultural stasis, variability , change, and adaptation. We report on a multiagent computational model of this society that closely reproduces the main features of its actual history, including population ebb and flow, changing spatial settlement patterns, and eventual rapid decline. The agents in the model are monoagriculturalists, who decide both where to situate their fields as well as the location of their settlements. Nutritional needs constrain fertility. Agent heteroge-neity, difficult to model mathematically, is demonstrated to be crucial to the high fidelity of the model.},
   author = {Robert L Axtell and Joshua M Epstein and Jeffrey S Dean and George J Gumerman and Alan C Swedlund and Jason Harburger and Shubha Chakravarty and Ross Hammond and Jon Parker and Miles Parker},
   title = {Population growth and collapse in a multiagent model of the Kayenta Anasazi in Long House Valley},
   url = {www.brookings.edudynamicsmodels.},
}
@article{Bearman2004,
   abstract = {This article describes the structure of the adolescent romantic and sexual network in a population of over 800 adolescents residing in a midsized town in the midwestern United States. Precise images and measures of network structure are derived from reports of relationships that occurred over a period of 18 months between 1993 and 1995. The study offers a comparison of the structural characteristics of the observed network to simulated networks conditioned on the distribution of ties; the observed structure reveals networks characterized by longer contact chains and fewer cycles than expected. This article identifies the micromechanisms that generate networks with structural features similar to the observed network. Implications for disease transmission dynamics and social policy are explored.},
   author = {Peter S. Bearman and James Moody and Katherine Stovel},
   doi = {10.1086/386272},
   issn = {00029602},
   issue = {1},
   journal = {American Journal of Sociology},
   month = {7},
   pages = {44-91},
   title = {Chains of affection: The structure of adolescent romantic and sexual networks},
   volume = {110},
   year = {2004},
}
@report{Cohen1990,
   abstract = {This paper explores principles governing the rational balance among an agent's beliefs, goals, actions, and intentions. Such principles provide specifications for artificial agents, and approximate a theory of human action (as philosophers use the term). By making explicit the conditions under which an agent can drop his goals, i.e., by specifying how the agent is committed to his goals, the formalism captures a number of important properties of intention. Specifically, the formalism provides analyses for Bratman's three characteristic functional roles played by intentions [7, 9], and shows how agents can avoid intending all the foreseen side-effects of what they actually intend. Finally, the analysis shows how intentions can be adopted relative to a background of relevant beliefs and other intentions or goals. By relativizing one agent's intentions in terms of beliefs about another agent's intentions (or beliefs'), we derive a preliminary account of interpersonal commitments.},
   author = {Philip R Cohen and Hector J Levesque},
   title = {Intention Is Choice with Commitment*},
   year = {1990},
}
@article{,
   title = {L7 hamill_gilbert_2011},
}
@report{Boero2005,
   abstract = {The paper deals with the use of empirical data in social science agent-based models. Agent-based models are too often viewed just as highly abstract thought experiments conducted in artificial worlds, in which the purpose is to generate and not to test theoretical hypotheses in an empirical way. On the contrary, they should be viewed as models that need to be embedded into empirical data both to allow the calibration and the validation of their findings. As a consequence, the search for strategies to find and extract data from reality, and integrate agent-based models with other traditional empirical social science methods, such as qualitative, quantitative, experimental and participatory methods, becomes a fundamental step of the modelling process. The paper argues that the characteristics of the empirical target matter. According to characteristics of the target, ABMs can be differentiated into case-based models, typifications and theoretical abstractions. These differences pose different challenges for empirical data gathering, and imply the use of different validation strategies.},
   author = {Riccardo Boero and Flaminio Squazzoni},
   issue = {4},
   journal = {Journal of Artificial Societies and Social Simulation},
   keywords = {Agent-Based Models,Empirical Calibration and Validation,Taxanomy of Models},
   title = {Does Empirical Embeddedness Matter? Methodological Issues on Agent-Based Models for Analytical Social Science},
   volume = {8},
   url = {http://jasss.soc.surrey.ac.uk/8/4/6.html©CopyrightJASSS<http://jasss.soc.surrey.ac.uk/8/4/6.html>},
   year = {2005},
}
@article{Green2015,
   abstract = {The past decades have seen enormous improvements in computational inference based on statistical models, with continual enhancement in a wide range of computational tools, in competition. In Bayesian inference, first and foremost, MCMC techniques continue to evolve, moving from random walk proposals to Langevin drift, to Hamiltonian Monte Carlo, and so on, with both theoretical and algorithmic inputs opening wider access to practitioners. However, this impressive evolution in capacity is confronted by an even steeper increase in the complexity of the models and datasets to be addressed. The difficulties of modelling and then handling ever more complex datasets most likely call for a new type of tool for computational inference that dramatically reduce the dimension and size of the raw data while capturing its essential aspects. Approximate models and algorithms may thus be at the core of the next computational revolution.},
   author = {Peter J. Green and Krzysztof Łatuszyński and Marcelo Pereyra and Christian P. Robert},
   month = {2},
   title = {Bayesian computation: a perspective on the current state, and sampling backwards and forwards},
   url = {http://arxiv.org/abs/1502.01148},
   year = {2015},
}
@article{Collier2013,
   abstract = {In the last decade, agent-based modeling and simulation (ABMS) has been applied to a variety of domains, demonstrating the potential of this technique to advance science, engineering, and policy analysis. However, realizing the full potential of ABMS to find breakthrough research results requires far greater computing capability than is available through current ABMS tools. The Repast for High Performance Computing (Repast HPC) project addresses this need by developing a useful and useable next-generation ABMS system explicitly focusing on larger-scale distributed computing platforms. Repast HPC is intended to smooth the path from small-scale simulations to large-scale distributed simulations through the use of a Logo-like system. This article’s contribution is its detailed presentation of the implementation of Repast HPC as a useful and usable framework, a complete ABMS platform developed explicitly for larger-scale distributed computing systems that leverages modern C++ techniques and the ReLogo language. © 2012, The Society for Modeling and Simulation International. All rights reserved.},
   author = {Nicholson Collier and Michael North},
   doi = {10.1177/0037549712462620},
   issn = {17413133},
   issue = {10},
   journal = {SIMULATION},
   keywords = {agent-based modeling and simulation,high-performance computing,parallel and distributed computing,simulation framework},
   pages = {1215-1235},
   title = {Parallel agent-based simulation with Repast for High Performance Computing},
   volume = {89},
   year = {2013},
}
@report{Zachary1977,
   author = {Wayne W Zachary},
   issue = {4},
   journal = {Source: Journal of Anthropological Research},
   pages = {452-473},
   publisher = {Winter},
   title = {An Information Flow Model for Conflict and Fission in Small Groups},
   volume = {33},
   year = {1977},
}
@report{,
   abstract = {This article presents an agent-based computational model of civil violence. Two variants of the civil violence model are presented. In the first a central authority seeks to suppress decentralized rebellion. In the second a central authority seeks to suppress communal violence between two warring ethnic groups.},
   author = {Joshua M Epstein},
   title = {Modeling civil violence: An agent-based computational approach},
   url = {www.pnas.orgcgidoi10.1073pnas.092080199},
}
@report{Miller1996,
   abstract = {A model of learning and adaptation is used to analyze the coevolution of strategies in the repeated Prisoner's Dilemma game under both perfect and imperfect reporting. Meta-players submit finite automata strategies and update their choices through an explicit evolutionary process modeled by a genetic algorithm. Using this framework, adaptive strategic choice and the emergence of cooperation are studied through 'computational experiments.' The results of the analyses indicate that information conditions lead to significant differences among the evolving strategies. Furthermore, they suggest that the general methodology may have much wider applicability to the analysis of adaptation in economic and social systems.},
   author = {John H Miller},
   journal = {Journal of Economic Behavior and Organization &},
   keywords = {C70,D80,Evolution,Genetic algorithms,JEL clussijicution: C63,Learning,Ll3 Keywords: Adaptation,Machine learning,Repeated prisoner's dilemma game},
   pages = {87-112},
   title = {The coevolution of automata in the repeated prisoner' s dilemma},
   volume = {29},
   year = {1996},
}
@article{Grimm2006,
   abstract = {Simulation models that describe autonomous individual organisms (individual based models, IBM) or agents (agent-based models, ABM) have become a widely used tool, not only in ecology, but also in many other disciplines dealing with complex systems made up of autonomous entities. However, there is no standard protocol for describing such simulation models, which can make them difficult to understand and to duplicate. This paper presents a proposed standard protocol, ODD, for describing IBMs and ABMs, developed and tested by 28 modellers who cover a wide range of fields within ecology. This protocol consists of three blocks (Overview, Design concepts, and Details), which are subdivided into seven elements: Purpose, State variables and scales, Process overview and scheduling, Design concepts, Initialization, Input, and Submodels. We explain which aspects of a model should be described in each element, and we present an example to illustrate the protocol in use. In addition, 19 examples are available in an Online Appendix. We consider ODD as a first step for establishing a more detailed common format of the description of IBMs and ABMs. Once initiated, the protocol will hopefully evolve as it becomes used by a sufficiently large proportion of modellers. © 2006 Elsevier B.V. All rights reserved.},
   author = {Volker Grimm and Uta Berger and Finn Bastiansen and Sigrunn Eliassen and Vincent Ginot and Jarl Giske and John Goss-Custard and Tamara Grand and Simone K. Heinz and Geir Huse and Andreas Huth and Jane U. Jepsen and Christian Jørgensen and Wolf M. Mooij and Birgit Müller and Guy Pe'er and Cyril Piou and Steven F. Railsback and Andrew M. Robbins and Martha M. Robbins and Eva Rossmanith and Nadja Rüger and Espen Strand and Sami Souissi and Richard A. Stillman and Rune Vabø and Ute Visser and Donald L. DeAngelis},
   doi = {10.1016/j.ecolmodel.2006.04.023},
   issn = {03043800},
   issue = {1-2},
   journal = {Ecological Modelling},
   keywords = {Agent-based model,Individual-based model,Model description,Scientific communication,Standardization},
   month = {9},
   pages = {115-126},
   title = {A standard protocol for describing individual-based and agent-based models},
   volume = {198},
   year = {2006},
}
@report{Janssen2009,
   abstract = {A replication and analysis of the Artificial Anasazi model is presented. It is shown that the success of replicating historical data is based on two parameters that adjust the carrying capacity of the Long House Valley. Compared to population estimates equal to the carrying capacity the specific agent behavior contributes only a modest improvement of the model to fit the archaeological records.},
   author = {Marco A Janssen},
   keywords = {11 11 12 12 Replication,Model Analysis,Model-Based Archaeology,Population Dynamics,Social},
   title = {Understanding Artificial Anasazi Understanding Artificial Anasazi Journal of Artificial Societies and Social Simulation 12},
   url = {http://jasss.soc.surrey.ac.uk/12/4/13.html},
   year = {2009},
}
@article{,
   title = {L2 bersini_2012},
}
@report{,
   abstract = {Long House Valley in the Black Mesa area of northeastern Arizona (U.S.) was inhabited by the Kayenta Anasazi from about 1800 before Christ to about anno Domini 1300. These people were prehistoric ancestors of the modern Pueblo cultures of the Colo-rado Plateau. Paleoenvironmental research based on alluvial geo-morphology, palynology, and dendroclimatology permits accurate quantitative reconstruction of annual fluctuations in potential agricultural production (kg of maize per hectare). The archaeological record of Anasazi farming groups from anno Domini 200-1300 provides information on a millennium of sociocultural stasis, variability , change, and adaptation. We report on a multiagent computational model of this society that closely reproduces the main features of its actual history, including population ebb and flow, changing spatial settlement patterns, and eventual rapid decline. The agents in the model are monoagriculturalists, who decide both where to situate their fields as well as the location of their settlements. Nutritional needs constrain fertility. Agent heteroge-neity, difficult to model mathematically, is demonstrated to be crucial to the high fidelity of the model.},
   author = {Robert L Axtell and Joshua M Epstein and Jeffrey S Dean and George J Gumerman and Alan C Swedlund and Jason Harburger and Shubha Chakravarty and Ross Hammond and Jon Parker and Miles Parker},
   title = {Population growth and collapse in a multiagent model of the Kayenta Anasazi in Long House Valley},
   url = {www.brookings.edudynamicsmodels.},
}
@article{Bearman2004,
   abstract = {This article describes the structure of the adolescent romantic and sexual network in a population of over 800 adolescents residing in a midsized town in the midwestern United States. Precise images and measures of network structure are derived from reports of relationships that occurred over a period of 18 months between 1993 and 1995. The study offers a comparison of the structural characteristics of the observed network to simulated networks conditioned on the distribution of ties; the observed structure reveals networks characterized by longer contact chains and fewer cycles than expected. This article identifies the micromechanisms that generate networks with structural features similar to the observed network. Implications for disease transmission dynamics and social policy are explored.},
   author = {Peter S. Bearman and James Moody and Katherine Stovel},
   doi = {10.1086/386272},
   issn = {00029602},
   issue = {1},
   journal = {American Journal of Sociology},
   month = {7},
   pages = {44-91},
   title = {Chains of affection: The structure of adolescent romantic and sexual networks},
   volume = {110},
   year = {2004},
}
@report{Cohen1990,
   abstract = {This paper explores principles governing the rational balance among an agent's beliefs, goals, actions, and intentions. Such principles provide specifications for artificial agents, and approximate a theory of human action (as philosophers use the term). By making explicit the conditions under which an agent can drop his goals, i.e., by specifying how the agent is committed to his goals, the formalism captures a number of important properties of intention. Specifically, the formalism provides analyses for Bratman's three characteristic functional roles played by intentions [7, 9], and shows how agents can avoid intending all the foreseen side-effects of what they actually intend. Finally, the analysis shows how intentions can be adopted relative to a background of relevant beliefs and other intentions or goals. By relativizing one agent's intentions in terms of beliefs about another agent's intentions (or beliefs'), we derive a preliminary account of interpersonal commitments.},
   author = {Philip R Cohen and Hector J Levesque},
   title = {Intention Is Choice with Commitment*},
   year = {1990},
}
@report{Boero2005,
   abstract = {The paper deals with the use of empirical data in social science agent-based models. Agent-based models are too often viewed just as highly abstract thought experiments conducted in artificial worlds, in which the purpose is to generate and not to test theoretical hypotheses in an empirical way. On the contrary, they should be viewed as models that need to be embedded into empirical data both to allow the calibration and the validation of their findings. As a consequence, the search for strategies to find and extract data from reality, and integrate agent-based models with other traditional empirical social science methods, such as qualitative, quantitative, experimental and participatory methods, becomes a fundamental step of the modelling process. The paper argues that the characteristics of the empirical target matter. According to characteristics of the target, ABMs can be differentiated into case-based models, typifications and theoretical abstractions. These differences pose different challenges for empirical data gathering, and imply the use of different validation strategies.},
   author = {Riccardo Boero and Flaminio Squazzoni},
   issue = {4},
   journal = {Journal of Artificial Societies and Social Simulation},
   keywords = {Agent-Based Models,Empirical Calibration and Validation,Taxanomy of Models},
   title = {Does Empirical Embeddedness Matter? Methodological Issues on Agent-Based Models for Analytical Social Science},
   volume = {8},
   url = {http://jasss.soc.surrey.ac.uk/8/4/6.html©CopyrightJASSS<http://jasss.soc.surrey.ac.uk/8/4/6.html>},
   year = {2005},
}
@article{Green2015,
   abstract = {The past decades have seen enormous improvements in computational inference based on statistical models, with continual enhancement in a wide range of computational tools, in competition. In Bayesian inference, first and foremost, MCMC techniques continue to evolve, moving from random walk proposals to Langevin drift, to Hamiltonian Monte Carlo, and so on, with both theoretical and algorithmic inputs opening wider access to practitioners. However, this impressive evolution in capacity is confronted by an even steeper increase in the complexity of the models and datasets to be addressed. The difficulties of modelling and then handling ever more complex datasets most likely call for a new type of tool for computational inference that dramatically reduce the dimension and size of the raw data while capturing its essential aspects. Approximate models and algorithms may thus be at the core of the next computational revolution.},
   author = {Peter J. Green and Krzysztof Łatuszyński and Marcelo Pereyra and Christian P. Robert},
   month = {2},
   title = {Bayesian computation: a perspective on the current state, and sampling backwards and forwards},
   url = {http://arxiv.org/abs/1502.01148},
   year = {2015},
}
@article{Andrianakis2015,
   abstract = {Advances in scientific computing have allowed the development of complex models that are being routinely applied to problems in disease epidemiology, public health and decision making. The utility of these models depends in part on how well they can reproduce empirical data. However, fitting such models to real world data is greatly hindered both by large numbers of input and output parameters, and by long run times, such that many modelling studies lack a formal calibration methodology. We present a novel method that has the potential to improve the calibration of complex infectious disease models (hereafter called simulators). We present this in the form of a tutorial and a case study where we history match a dynamic, event-driven, individual-based stochastic HIV simulator, using extensive demographic, behavioural and epidemiological data available from Uganda. The tutorial describes history matching and emulation. History matching is an iterative procedure that reduces the simulator's input space by identifying and discarding areas that are unlikely to provide a good match to the empirical data. History matching relies on the computational efficiency of a Bayesian representation of the simulator, known as an emulator. Emulators mimic the simulator's behaviour, but are often several orders of magnitude faster to evaluate. In the case study, we use a 22 input simulator, fitting its 18 outputs simultaneously. After 9 iterations of history matching, a non-implausible region of the simulator input space was identified that was (Formula presented.) times smaller than the original input space. Simulator evaluations made within this region were found to have a 65% probability of fitting all 18 outputs. History matching and emulation are useful additions to the toolbox of infectious disease modellers. Further research is required to explicitly address the stochastic nature of the simulator as well as to account for correlations between outputs.},
   author = {Ioannis Andrianakis and Ian R. Vernon and Nicky McCreesh and Trevelyan J. McKinley and Jeremy E. Oakley and Rebecca N. Nsubuga and Michael Goldstein and Richard G. White},
   doi = {10.1371/journal.pcbi.1003968},
   issn = {15537358},
   issue = {1},
   journal = {PLoS Computational Biology},
   month = {1},
   pmid = {25569850},
   publisher = {Public Library of Science},
   title = {Bayesian History Matching of Complex Infectious Disease Models Using Emulation: A Tutorial and a Case Study on HIV in Uganda},
   volume = {11},
   year = {2015},
}
@article{,
   title = {L7 steglich_et_al_2010},
}
@article{,
   title = {L7 hamill_gilbert_2011},
}
@report{Zachary1977,
   author = {Wayne W Zachary},
   issue = {4},
   journal = {Source: Journal of Anthropological Research},
   pages = {452-473},
   publisher = {Winter},
   title = {An Information Flow Model for Conflict and Fission in Small Groups},
   volume = {33},
   year = {1977},
}
@report{Miller1996,
   abstract = {A model of learning and adaptation is used to analyze the coevolution of strategies in the repeated Prisoner's Dilemma game under both perfect and imperfect reporting. Meta-players submit finite automata strategies and update their choices through an explicit evolutionary process modeled by a genetic algorithm. Using this framework, adaptive strategic choice and the emergence of cooperation are studied through 'computational experiments.' The results of the analyses indicate that information conditions lead to significant differences among the evolving strategies. Furthermore, they suggest that the general methodology may have much wider applicability to the analysis of adaptation in economic and social systems.},
   author = {John H Miller},
   journal = {Journal of Economic Behavior and Organization &},
   keywords = {C70,D80,Evolution,Genetic algorithms,JEL clussijicution: C63,Learning,Ll3 Keywords: Adaptation,Machine learning,Repeated prisoner's dilemma game},
   pages = {87-112},
   title = {The coevolution of automata in the repeated prisoner' s dilemma},
   volume = {29},
   year = {1996},
}
@article{Balke2014,
   abstract = {When designing an agent-based simulation, an important question to answer is how to model the decision making processes of the agents in the system. A large number of agent decision making models can be found in the literature, each inspired by different aims and research questions. In this paper we provide a review of 14 agent decision making architectures that have attracted interest. They range from production-rule systems to psychologically- and neurologically-inspired approaches. For each of the architectures we give an overview of its design, highlight research questions that have been answered with its help and outline the reasons for the choice of the decision making model provided by the originators. Our goal is to provide guidelines about what kind of agent decision making model, with which level of simplicity or complexity, to use for which kind of research question.},
   author = {Tina Balke and Nigel Gilbert},
   doi = {10.18564/jasss.2687},
   issn = {14607425},
   issue = {4},
   journal = {JASSS},
   keywords = {Agents,Decision Making,Survey},
   month = {10},
   pages = {1},
   publisher = {University of Surrey},
   title = {How do agents make decisions? A survey},
   volume = {17},
   year = {2014},
}
@article{,
   title = {L2 bersini_2012},
}
@report{,
   abstract = {Long House Valley in the Black Mesa area of northeastern Arizona (U.S.) was inhabited by the Kayenta Anasazi from about 1800 before Christ to about anno Domini 1300. These people were prehistoric ancestors of the modern Pueblo cultures of the Colo-rado Plateau. Paleoenvironmental research based on alluvial geo-morphology, palynology, and dendroclimatology permits accurate quantitative reconstruction of annual fluctuations in potential agricultural production (kg of maize per hectare). The archaeological record of Anasazi farming groups from anno Domini 200-1300 provides information on a millennium of sociocultural stasis, variability , change, and adaptation. We report on a multiagent computational model of this society that closely reproduces the main features of its actual history, including population ebb and flow, changing spatial settlement patterns, and eventual rapid decline. The agents in the model are monoagriculturalists, who decide both where to situate their fields as well as the location of their settlements. Nutritional needs constrain fertility. Agent heteroge-neity, difficult to model mathematically, is demonstrated to be crucial to the high fidelity of the model.},
   author = {Robert L Axtell and Joshua M Epstein and Jeffrey S Dean and George J Gumerman and Alan C Swedlund and Jason Harburger and Shubha Chakravarty and Ross Hammond and Jon Parker and Miles Parker},
   title = {Population growth and collapse in a multiagent model of the Kayenta Anasazi in Long House Valley},
   url = {www.brookings.edudynamicsmodels.},
}
@article{Collier2013,
   abstract = {In the last decade, agent-based modeling and simulation (ABMS) has been applied to a variety of domains, demonstrating the potential of this technique to advance science, engineering, and policy analysis. However, realizing the full potential of ABMS to find breakthrough research results requires far greater computing capability than is available through current ABMS tools. The Repast for High Performance Computing (Repast HPC) project addresses this need by developing a useful and useable next-generation ABMS system explicitly focusing on larger-scale distributed computing platforms. Repast HPC is intended to smooth the path from small-scale simulations to large-scale distributed simulations through the use of a Logo-like system. This article’s contribution is its detailed presentation of the implementation of Repast HPC as a useful and usable framework, a complete ABMS platform developed explicitly for larger-scale distributed computing systems that leverages modern C++ techniques and the ReLogo language. © 2012, The Society for Modeling and Simulation International. All rights reserved.},
   author = {Nicholson Collier and Michael North},
   doi = {10.1177/0037549712462620},
   issn = {17413133},
   issue = {10},
   journal = {SIMULATION},
   keywords = {agent-based modeling and simulation,high-performance computing,parallel and distributed computing,simulation framework},
   pages = {1215-1235},
   title = {Parallel agent-based simulation with Repast for High Performance Computing},
   volume = {89},
   year = {2013},
}
@report{,
   abstract = {Long House Valley in the Black Mesa area of northeastern Arizona (U.S.) was inhabited by the Kayenta Anasazi from about 1800 before Christ to about anno Domini 1300. These people were prehistoric ancestors of the modern Pueblo cultures of the Colo-rado Plateau. Paleoenvironmental research based on alluvial geo-morphology, palynology, and dendroclimatology permits accurate quantitative reconstruction of annual fluctuations in potential agricultural production (kg of maize per hectare). The archaeological record of Anasazi farming groups from anno Domini 200-1300 provides information on a millennium of sociocultural stasis, variability , change, and adaptation. We report on a multiagent computational model of this society that closely reproduces the main features of its actual history, including population ebb and flow, changing spatial settlement patterns, and eventual rapid decline. The agents in the model are monoagriculturalists, who decide both where to situate their fields as well as the location of their settlements. Nutritional needs constrain fertility. Agent heteroge-neity, difficult to model mathematically, is demonstrated to be crucial to the high fidelity of the model.},
   author = {Robert L Axtell and Joshua M Epstein and Jeffrey S Dean and George J Gumerman and Alan C Swedlund and Jason Harburger and Shubha Chakravarty and Ross Hammond and Jon Parker and Miles Parker},
   title = {Population growth and collapse in a multiagent model of the Kayenta Anasazi in Long House Valley},
   url = {www.brookings.edudynamicsmodels.},
}
@article{repasthpc,
   abstract = {<p>In the last decade, agent-based modeling and simulation (ABMS) has been applied to a variety of domains, demonstrating the potential of this technique to advance science, engineering, and policy analysis. However, realizing the full potential of ABMS to find breakthrough research results requires far greater computing capability than is available through current ABMS tools. The Repast for High Performance Computing (Repast HPC) project addresses this need by developing a useful and useable next-generation ABMS system explicitly focusing on larger-scale distributed computing platforms. Repast HPC is intended to smooth the path from small-scale simulations to large-scale distributed simulations through the use of a Logo-like system. This article’s contribution is its detailed presentation of the implementation of Repast HPC as a useful and usable framework, a complete ABMS platform developed explicitly for larger-scale distributed computing systems that leverages modern C + + techniques and the ReLogo language.</p>},
   author = {Nicholson Collier and Michael North},
   doi = {10.1177/0037549712462620},
   issn = {0037-5497},
   issue = {10},
   journal = {SIMULATION},
   month = {10},
   pages = {1215-1235},
   title = {Parallel agent-based simulation with Repast for High Performance Computing},
   volume = {89},
   year = {2013},
}
@article{mbssmCommentary,
   abstract = {Psychology endeavors to develop theories of human capacities and behaviors on the basis of a variety of methodologies and dependent measures. We argue that one of the most divisive factors in psychological science is whether researchers choose to use computational modeling of theories (over and above data) during the scientific-inference process. Modeling is undervalued yet holds promise for advancing psychological science. The inherent demands of computational modeling guide us toward better science by forcing us to conceptually analyze, specify, and formalize intuitions that otherwise remain unexamined—what we dub open theory. Constraining our inference process through modeling enables us to build explanatory and predictive theories. Here, we present scientific inference in psychology as a path function in which each step shapes the next. Computational modeling can constrain these steps, thus advancing scientific inference over and above the stewardship of experimental practice (e.g., preregistration). If psychology continues to eschew computational modeling, we predict more replicability crises and persistent failure at coherent theory building. This is because without formal modeling we lack open and transparent theorizing. We also explain how to formalize, specify, and implement a computational model, emphasizing that the advantages of modeling can be achieved by anyone with benefit to all.},
   author = {Olivia Guest and Andrea E. Martin},
   doi = {10.1177/1745691620970585},
   issn = {17456924},
   issue = {4},
   journal = {Perspectives on Psychological Science},
   keywords = {computational model,open science,scientific inference,theoretical psychology},
   month = {7},
   pages = {789-802},
   pmid = {33482070},
   publisher = {SAGE Publications Inc.},
   title = {How Computational Modeling Can Force Theory Building in Psychological Science},
   volume = {16},
   year = {2021},
}
@report{BoydphD,
   author = {Jennifer Eva Boyd and John Holmes},
   city = {Sheffield},
   institution = {University of Sheffield},
   month = {9},
   title = {Understanding the role of health inequality theory in creating and sustaining the Alcohol Harm Paradox},
   year = {2022},
}
@article{oddOrigin,
   abstract = {Simulation models that describe autonomous individual organisms (individual based models, IBM) or agents (agent-based models, ABM) have become a widely used tool, not only in ecology, but also in many other disciplines dealing with complex systems made up of autonomous entities. However, there is no standard protocol for describing such simulation models, which can make them difficult to understand and to duplicate. This paper presents a proposed standard protocol, ODD, for describing IBMs and ABMs, developed and tested by 28 modellers who cover a wide range of fields within ecology. This protocol consists of three blocks (Overview, Design concepts, and Details), which are subdivided into seven elements: Purpose, State variables and scales, Process overview and scheduling, Design concepts, Initialization, Input, and Submodels. We explain which aspects of a model should be described in each element, and we present an example to illustrate the protocol in use. In addition, 19 examples are available in an Online Appendix. We consider ODD as a first step for establishing a more detailed common format of the description of IBMs and ABMs. Once initiated, the protocol will hopefully evolve as it becomes used by a sufficiently large proportion of modellers. © 2006 Elsevier B.V. All rights reserved.},
   author = {Volker Grimm and Uta Berger and Finn Bastiansen and Sigrunn Eliassen and Vincent Ginot and Jarl Giske and John Goss-Custard and Tamara Grand and Simone K. Heinz and Geir Huse and Andreas Huth and Jane U. Jepsen and Christian Jørgensen and Wolf M. Mooij and Birgit Müller and Guy Pe'er and Cyril Piou and Steven F. Railsback and Andrew M. Robbins and Martha M. Robbins and Eva Rossmanith and Nadja Rüger and Espen Strand and Sami Souissi and Richard A. Stillman and Rune Vabø and Ute Visser and Donald L. DeAngelis},
   doi = {10.1016/j.ecolmodel.2006.04.023},
   issn = {03043800},
   issue = {1-2},
   journal = {Ecological Modelling},
   keywords = {Agent-based model,Individual-based model,Model description,Scientific communication,Standardization},
   month = {9},
   pages = {115-126},
   title = {A standard protocol for describing individual-based and agent-based models},
   volume = {198},
   year = {2006},
}
@report{,
   title = {Using Agent-based Modelling to understand the causes of the Alcohol Harm Paradox: A Scottish case study investigating Fundamental Cause Theory: ODD Protocol Overview},
}
@report{oddUpdate,
   author = {Volker Grimm and Steven F Railsback and Christian E Vincenot and Uta Berger and Cara Gallagher and Donald L DeAngelis and Bruce Edmonds and Jiaqi Ge and Jarl Giske and Jürgen Groeneveld and Alice S A Johnston and Alexander Milles and Jacob Nabe-Nielsen and J Gareth Polhill and Vik-toriia Radchuk and Marie-Sophie Rohwäder and Richard A Stillman and Jan C Thiele and Daniel Ayllón and Kyoto ooo-----},
   title = {The ODD Protocol for Describing Agent-Based and Other Simulation Models: A Second Update to Improve Clarity, Replication, and Structural Realism},
}
@report{abmGeneral,
   author = {M Salgado and M Gilbert},
   title = {AGENT BASED MODELLING},
   year = {2013},
}
@article{abmEpstein,
   abstract = {This article argues that the agent-based computational model permits a distinctive approach to social science for which the term “generative” is suitable. In defending this terminology, features distinguishing the approach from both “inductive” and “deductive” science are given. Then, the following specific contributions to social science are discussed: The agent-based computational model is a new tool for empirical research. It offers a natural environment for the study of connectionist phenomena in social science. Agent-based modeling provides a powerful way to address certain enduring—and especially interdisciplinary—questions. It allows one to subject certain core theories—such as neoclassical microeconomics—to important types of stress (e.g., the effect of evolving preferences). It permits one to study how rules of individual behavior give rise—or “map up”—to macroscopic regularities and organizations. In turn, one can employ laboratory behavioral research findings to select among competing agent-based (“bottom up”) models. The agent-based approach may well have the important effect of decoupling individual rationality from macroscopic equilibrium and of separating decision science from social science more generally. Agent-based modeling offers powerful new forms of hybrid theoretical-computational work; these are particularly relevant to the study of non-equilibrium systems. The agent-based approach invites the interpretation of society as a distributed computational device, and in turn the interpretation of social dynamics as a type of computation. This interpretation raises important foundational issues in social science—some related to intractability, and some to undecidability proper. Finally, since “emergence” figures prominently in this literature, I take up the connection between agent-based modeling and classical emergentism, criticizing the latter and arguing that the two are incompatible. © 1999 John Wiley & Sons, Inc.},
   author = {Joshua M. Epstein},
   doi = {10.1002/(SICI)1099-0526(199905/06)4:5<41::AID-CPLX9>3.0.CO;2-F},
   issn = {10990526},
   issue = {5},
   journal = {Complexity},
   keywords = {Agent-based models,Artificial societies,Philosophy of social science},
   pages = {41-60},
   title = {Agent-based computational models and generative social science},
   volume = {4},
   year = {1999},
}
@article{schelling,
   author = {Thomas C. Schelling},
   doi = {10.1080/0022250X.1971.9989794},
   issn = {0022-250X},
   issue = {2},
   journal = {The Journal of Mathematical Sociology},
   month = {7},
   pages = {143-186},
   title = {Dynamic models of segregation},
   volume = {1},
   year = {1971},
}
@report{collegeABMCLD,
   abstract = {Alcohol use is prevalent among college students in the US and is the leading cause of many alcohol-related consequences such as injury, driving under influence, and sexual assault. The problem of college drinking involves complex individual, social, and cultural factors. By viewing college drinking as a complex system problem, this paper describes two components necessary for the full development of a simulation-based dynamic agent model for alcohol use in college. The first component is a basic agent-based model that explores the dynamic of college drinking. The second component discusses the use of system dynamic modeling to explore the causal relationship between various personal/environmental factors and alcohol consumption. The paper also discusses important leverage points for intervention strategies, especially in the context of targeting both high-risk and low-to medium-risk drinkers in college.},
   author = {Edward H Ip and Mark Wolfson and Douglas Easterling and Erin Sutfin and Kimberly Wagoner and Jill Blocker and Kathleen Egan and Hazhir Rahmandad and Shyh-Huei Chen},
   title = {Agent-based modeling of College Drinking Behavior and mapping of system dynamics of alcohol reduction using both environmental and individual-based intervention strategies Agent-based modeling of College Drinking Behavior and mapping of feedback mechanism of alcohol reduction using both environmental and individual-based intervention strategies},
   year = {2012},
}
@article{abmVenueLock,
   abstract = {Objective: Many variations of venue lockout and last-drink policies have been introduced in attempts to reduce drinking-related harms. We estimate the public health gains and licensee costs of these policies using a computer simulated population of young adults engaging in heavy drinking. Method: Using an agent-based model we implemented 1 am/2 am/3 am venue lockouts in conjunction with last drinks zero/one/two hours later, or at current closing times. Outcomes included: the number of incidents of verbal aggression in public drinking venues, private venues or on the street; and changed revenue to public venues. Results: The most effective policy in reducing verbal aggression among agents was 1 am lockouts with current closing times. All policies produced substantial reductions in street-based incidents of verbal aggression among agents (33–81%) due to the smoothing of transport demand. Direct revenue losses were 1–9% for simulated licensees, with later lockout times and longer periods between lockout and last drinks producing smaller revenue losses. Conclusion: Simulation models are useful for exploring consequences of policy change. Our simulation suggests that additional hours between lockout and last drinks could reduce aggression by easing transport demand, while minimising revenue loss to venue owners. Implications for public health: Direct policies to reduce late-night transport-related disputes should be considered.},
   author = {Nick Scott and Michael Livingston and Iyanoosh Reporter and Paul Dietze},
   doi = {10.1111/1753-6405.12640},
   issn = {17536405},
   issue = {3},
   journal = {Australian and New Zealand Journal of Public Health},
   keywords = {agent-based model,alcohol,last drinks,venue lockouts,verbal aggression},
   month = {6},
   pages = {243-247},
   pmid = {28245536},
   publisher = {Wiley-Blackwell},
   title = {Using simulation modelling to examine the impact of venue lockout and last-drink policies on drinking-related harms and costs to licensees},
   volume = {41},
   year = {2017},
}
@report{abmRiskBehaviourDev,
   abstract = {Adolescents tend to adopt behaviors that are similar to those of their friends, and also tend to become friends with peers that have similar interests and behaviors. This tendency towards homogeneity applies not only to conventional behaviors such as working for school and participating in sports activities, but also to risk behaviors such as drug use, oppositional behavior or unsafe sex. The current study aims at building an agent model to answer the following related questions: How do friendship groups evolve and what is the role of behavioral similarity in friendship formation? How does homogeneity among peers emerge, with regard to conventional as well as risk behaviors? On the basis of the theoretical and empirical literature on friendship selection and influences on risk behavior during adolescence we first developed a conceptual framework, which was then translated into a mathematical model of a dynamic system and implemented as an agent-based computer simulation consisting of simple behavioral rules and principles. Each agent in the model holds distinct property matrices including an individual behavioral profile with a list of risky (i.e., alcohol use, aggressiveness, soft drugs) and conventional behaviors (i.e., school attendance, sports, work). The computer model simulates the development, during one school year, of a social network (i.e. formation of friendships and cliques), the (dyadic) interactions between pupils and their behavioral profiles. During the course of simulation, the agents' behavioral profiles change on the basis of their interactions resulting in individual developmental curves of conventional and risk behaviors. These profiles are used to calculate the (behavioral) similarity and differences between the various agents. Generally, the model output is analyzed by means of visual inspection (i.e., plotting developmental curves of behavior and social networks), systematic comparison and by calculating additional measures (i.e., using specific social analysis software packages). Simulation results conclusively indicate model validity. The model simulates qualitative properties currently found in research on adolescent development, namely the role of homophily, the appearance of friendship clusters, and the increase in behavioral homogeneity among friends. The model not only converges with empirical findings, but furthermore helps to explain social psychological phenomena (e.g. the emergence of homophily among adolescents).},
   author = {Nils Schuhmacher and Laura Ballato},
   issue = {3},
   journal = {Adolescence Journal of Artificial Societies and Social Simulation},
   keywords = {Behavioral Change,Dynamic Systems,Friendship Formation,Peer Homogeneity,Risk Behavior in Adolescence},
   pages = {1},
   title = {Using an Agent-Based Model to Simulate the Development of Risk Behaviors During},
   volume = {17},
   year = {2014},
}
@article{abmPublicTransport,
   abstract = {Background The late-night accessibility of entertainment precincts is a contributing factor to acute drinking-related harms. Using computer simulation we test the effects of improved public transport (PT) and venue lockouts on verbal aggression, consumption-related harms and transport-related harms among a population of young adults engaging in heavy drinking in Melbourne. Methods Using an agent-based model we implemented: a two-hour PT extension/24-hour PT; 1 am/3 am venue lockouts; and combinations of both. Outcomes determined for outer-urban (OU) and inner-city (IC) residents were: the number of incidents of verbal aggression inside public and private venues; the number of people ejected from public venues for being intoxicated; and the percentage of people experiencing verbal aggression, consumption-related harms and transport-related harms. Results All-night PT reduced verbal aggression in the model by 21% but displaced some incidents among OU residents from private to public settings. Comparatively, 1 am lockouts reduced verbal aggression in the model by 19% but led to IC residents spending more time in private rather than public venues where their consumption-related harms increased. Extending PT by 2 h had similar outcomes to 24-hour PT except with fewer incidents of verbal aggression displaced. Although 3 am lockouts were inferior to 1 am lockouts, when modelled in combination with any extension of PT both policies were similar. Conclusions A two-hour extension of PT is likely to be more effective in reducing verbal aggression and consumption-related harms than venue lockouts. Modelling a further extension of PT to 24 h had minimal additional benefits but the potential to displace incidents of verbal aggression among OU residents from private to public venues.},
   author = {Nick Scott and Aaron Hart and James Wilson and Michael Livingston and David Moore and Paul Dietze},
   doi = {10.1016/j.drugpo.2016.02.016},
   issn = {18734758},
   journal = {International Journal of Drug Policy},
   keywords = {Agent-based model,Alcohol,Drinking-related harms,Public transport,SimDrink,Venue lockouts,Verbal aggression},
   month = {6},
   pages = {44-49},
   pmid = {27140432},
   publisher = {Elsevier B.V.},
   title = {The effects of extended public transport operating hours and venue lockout policies on drinking-related harms in Melbourne, Australia: Results from SimDrink, an agent-based simulation model},
   volume = {32},
   year = {2016},
}
@article{abmYoungAus,
   abstract = {Background: Computer simulations provide a useful tool for bringing together diverse sources of information in order to increase understanding of the complex aetiology of drug use and related harm, and to inform the development of effective policies. In this paper, we describe SimAmph, an agent-based simulation model for exploring how individual perceptions, peer influences and subcultural settings shape the use of psychostimulants and related harm amongst young Australians. Methods: We present the conceptual architecture underpinning SimAmph, the assumptions we made in building it, the outcomes of sensitivity analysis of key model parameters and the results obtained when we modelled a baseline scenario. Results: SimAmph's core behavioural algorithm is able to produce social patterns of partying and recreational drug use that approximate those found in an Australian national population survey. We also discuss the limitations involved in running closed-system simulations and how the model could be refined to include the social, as well as health, consequences of drug use. Conclusion: SimAmph provides a useful tool for integrating diverse data and exploring drug policy scenarios. Its integrated approach goes some way towards overcoming the compartmentalisation that characterises existing data, and its structure, parameters and values can be modified as new data and understandings emerge. In a companion paper (Dray et al., 2011), we use the model outlined here to explore the possible consequences of two policy scenarios. © 2011 Elsevier B.V.},
   author = {Pascal Perez and Anne Dray and David Moore and Paul Dietze and Gabriele Bammer and Rebecca Jenkinson and Christine Siokou and Rachael Green and Susan L. Hudson and Lisa Maher},
   doi = {10.1016/j.drugpo.2011.05.017},
   issn = {09553959},
   issue = {1},
   journal = {International Journal of Drug Policy},
   keywords = {Agent-based modelling,Drug careers,Drug-related harm,Psychostimulants,Transdisciplinary approaches,Young people},
   month = {1},
   pages = {62-71},
   pmid = {21715152},
   title = {SimAmph: An agent-based simulation model for exploring the use of psychostimulants and related harm amongst young Australians},
   volume = {23},
   year = {2012},
}
@article{abmNetLogo,
   abstract = {Aggression and other acute harms experienced in the night-time economy are topics of significant public health concern. Although policies to minimise these harms are frequently proposed, there is often little evidence available to support their effectiveness. In particular, indirect and displacement effects are rarely measured. This paper describes a proof-of-concept agent-based model ‘SimDrink’, built in NetLogo, which simulates a population of 18-25 year old heavy alcohol drinkers on a night out in Melbourne to provide a means for conducting policy experiments to inform policy decisions. The model includes demographic, setting and situational-behavioural heterogeneity and is able to capture any unintended consequences of policy changes. It consists of individuals and their friendship groups moving between private, public-commercial (e.g. nightclub) and public-niche (e.g. bar, pub) venues while tracking their alcohol consumption, spending and whether or not they experience consumption-related harms (i.e. drink too much), are involved in verbal violence, or have difficulty getting home. When compared to available literature, the model can reproduce current estimates for the prevalence of verbal violence experienced by this population on a single night out, and produce realistic values for the prevalence of consumption-related and transportrelated harms. Outputs are robust to variations in underlying parameters. Further work with policy makers is required to identify several specific proposed harm reduction interventions that can be virtually implemented and compared. This will allow evidence based decisions to be made and will help to ensure any interventions have their intended effects.},
   author = {Nick Scott and Michael Livingston and Aaron Hart and James Wilson and David Moore and Paul Dietze},
   doi = {10.18564/jasss.2943},
   issn = {14607425},
   issue = {1},
   journal = {JASSS},
   keywords = {Agent-Based model,Alcohol,Heavy drinking,NetLogo,Night-Time economy,SimDrink},
   month = {1},
   publisher = {University of Surrey},
   title = {SimDrink: An agent-based netlogo model of young, heavy drinkers for conducting alcohol policy experiments},
   volume = {19},
   year = {2016},
}
@inproceedings{abmParameterEstimation,
   abstract = {This paper presents a new real-world application of evolutionary computation: identifying parameterisations of a theory-driven model that can reproduce alcohol consumption dynamics observed in a population over time. Population alcohol consumption is a complex system, with multiple interactions between economic and social factors and drinking behaviours, the nature and importance of which are not well-understood. Prediction of time trends in consumption is therefore difficult, but essential for robust estimation of future changes in health-related consequences of drinking and for appraising the impact of interventions aimed at changing alcohol use in society. The paper describes a microsimulation approach in which an attitude-behaviour model, Theory of Planned Behaviour, is used to describe the frequency of drinking by individuals. Consumption dynamics in the simulation are driven by changes in the social roles of individuals over time (parenthood, partnership, and paid labour). An evolutionary optimizer is used to identify parameterisations of the Theory that can describe the observed changes in drinking frequency. Niching is incorporated to enable multiple possible parameterisations to be identified, each of which can accurately recreate history but potentially encode quite different future trends. The approach is demonstrated using evidence from the 1979-1985 birth cohort in England between 2003 and 2010. © 2014 is held by the author/owner(s).},
   author = {Robin C. Purshouse and Abdallah K. Ally and Alan Brennan and Daniel Moyo and Paul Norman},
   doi = {10.1145/2576768.2598239},
   isbn = {9781450326629},
   journal = {GECCO 2014 - Proceedings of the 2014 Genetic and Evolutionary Computation Conference},
   keywords = {Genetic algorithms,Multiple solutions / niching,Simulation optimization,Social science},
   pages = {1159-1166},
   publisher = {Association for Computing Machinery},
   title = {Evolutionary parameter estimation for a theory of planned behaviour microsimulation of alcohol consumption dynamics in an English birth cohort 2003 to 2010},
   year = {2014},
}
@article{abmPolicyInterventions,
   abstract = {Increasing alcohol outlet density is well-documented to be associated with increased alcohol use and problems, leading to the policy recommendation that limiting outlet density will decrease alcohol problems. Yet few studies of decreasing problematic outlets and outlet density have been conducted. We estimated the association between closing alcohol outlets and alcohol use and alcohol-related violence, using an agent-based model of the adult population in New York City. The model was calibrated according to the empirical distribution of the parameters across the city's population, including the density of on-and off-premise alcohol outlets. Interventions capped the alcohol outlet distribution at the 90th to the 50th percentiles of the New York City density, and closed 5% to 25% of outlets with the highest levels of violence. Capping density led to a lower population of light drinkers (42.2% at baseline vs. 38.1% at the 50th percentile), while heavy drinking increased slightly (12.0% at baseline vs. 12.5% at the 50th percentile). Alcohol-related homicides and nonfatal violence remained unchanged. Closing the most violent outlets was not associated with changes in alcohol use or related problems. Results suggest that focusing solely on closing alcohol outlets might not be an effective strategy to reduce alcohol-related problems.},
   author = {Alvaro Castillo-Carniglia and Veronica A. Pear and Melissa Tracy and Katherine M. Keyes and Magdalena Cerdá},
   doi = {10.1093/aje/kwy289},
   issn = {14766256},
   issue = {4},
   journal = {American Journal of Epidemiology},
   keywords = {alcohol,alcohol outlets,public health,simulation,violence},
   month = {4},
   pages = {694-702},
   pmid = {30608509},
   publisher = {Oxford University Press},
   title = {Limiting Alcohol Outlet Density to Prevent Alcohol Use and Violence: Estimating Policy Interventions Through Agent-Based Modeling},
   volume = {188},
   year = {2019},
}
@article{abmCollegeConsumption,
   abstract = {An agent-based C++ program was developed to model student drinking. Student-agents interact with each other and are randomly subjected to good or bad drinking experiences, to stories of other students' experiences, and to peer pressure. The program outputs drinking rates as functions of time based on various input parameters. The intent of this project is to simulate alcohol use, eventually adding other drugs, and possibly creating a simulation game for use as an educational tool. © 2008 Wiley Periodicals, Inc.},
   author = {Laura A. Garrison and David S. Babcock},
   doi = {10.1002/cplx.20259},
   issn = {10990526},
   issue = {6},
   journal = {Complexity},
   keywords = {Agent-based modeling,Artificial societies,College drinking,Simulated societies},
   pages = {35-44},
   publisher = {John Wiley and Sons Inc.},
   title = {Alcohol consumption among college students: An agent-based computational simulation},
   volume = {14},
   year = {2009},
}
@article{abmCollegePolicyandMechanisms,
   abstract = {Purpose: To: (1) explore how multi-level factors impact the longitudinal prevalence of depression and alcohol misuse among urban older adults (≥ 65 years), and (2) simulate the impact of alcohol taxation policies and targeted interventions that increase social connectedness among excessive drinkers, socially isolated and depressed older adults; both alone and in combination. Methods: An agent-based model was developed to explore the temporal co-evolution of depression and alcohol misuse prevalence among older adults nested in a spatial network. The model was based on Los Angeles and calibrated longitudinally using data from the Multi-Ethnic Study of Atherosclerosis. Results: Interventions with a social component targeting depressed and socially isolated older adults appeared more effective in curbing depression prevalence than those focused on excessive drinkers. Targeting had similar impacts on alcohol misuse, though the effects were marginal compared to those on depression. Alcohol taxation alone had little impact on either depression or alcohol misuse trajectories. Conclusions: Interventions that improve social connectedness may reduce the prevalence of depression among older adults. Targeting considerations could play an important role in determining the success of such efforts.},
   author = {Ivana Stankov and Yong Yang and Brent A. Langellier and Jonathan Purtle and Katherine L. Nelson and Ana V. Diez Roux},
   doi = {10.1007/s00127-019-01701-1},
   issn = {14339285},
   issue = {10},
   journal = {Social Psychiatry and Psychiatric Epidemiology},
   keywords = {Agent-based model,Chronic disease,Complex systems,Health policy,Mental health},
   month = {10},
   pages = {1243-1253},
   pmid = {30918978},
   publisher = {Dr. Dietrich Steinkopff Verlag GmbH and Co. KG},
   title = {Depression and alcohol misuse among older adults: exploring mechanisms and policy impacts using agent-based modelling},
   volume = {54},
   year = {2019},
}
@inproceedings{abmAlcoholCrime,
   abstract = {The allocation of resources to challenge city centre violent crime traditionally relies on historical data to identify hot-spots. The usefulness of such data-driven approaches is limited when historical data is scarce or unavailable (e.g. planning of a new city) or insufficiently representative (e.g. does not account for novel events, such as Olympic Games). In some cities, crime data is not systematically accumulated at all. We present a graph-constrained agent based simulation model of alcohol-related violent crime that is capable of predicting areas of likely violent crime without requiring any historical data. The only inputs to our simulation are publicly available geographical data, which makes our method immediately applicable to a wide range of tasks, such as optimal city planning, police patrol optimisation, devising alcohol licensing policies. In experiments, we evaluate our model and demonstrate agreement of our model's predictions on where and when violence will occur with real-world violent crime data. Analyses indicate that our agent based model may be able to make a significant contribution to attempts to prevent violence through deterrence or by design.},
   author = {Joseph Redfern and Kirill Sidorov and Paul L. Rosin and Simon C. Moore and Padraig Corcoran and David Marshall},
   doi = {10.1109/AVSS.2017.8078513},
   isbn = {9781538629390},
   journal = {2017 14th IEEE International Conference on Advanced Video and Signal Based Surveillance, AVSS 2017},
   month = {10},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {An open-data, agent-based model of alcohol related crime},
   year = {2017},
}
@article{abmBinge,
   abstract = {In this paper, we analyse the recent rapid growth of 'binge' drinking in the UK. This means the rapid consumption of large amounts of alcohol, especially by young people, leading to serious anti-social and criminal behaviour in urban centres. British soccer fans have often exhibited this kind of behaviour abroad, but it has become widespread amongst young people within Britain itself. Vomiting, collapsing in the street, shouting and chanting loudly, intimidating passers-by and fighting are now regular night-time features of many British towns and cities. A particularly disturbing aspect is the huge rise in drunken and anti-social behaviour amongst young females. Increasingly, policy makers in the West are concerned about how not just to regulate but to alter social behaviour. Smoking and obesity are obvious examples, and in the UK 'binge' drinking has become a focus of acute policy concern. We show how a simple agent based model approach, combined with a limited amount of easily acquired information, can provide useful insights for policy makers in the context of behavioural regulation. We show that the hypothesis that the rise in binge drinking is a fashion-related phenomenon, with imitative behaviour spreading across social networks, is sufficient to account for the empirically observed patterns of binge drinking behaviour. The results show that a small world network, rather than a scale-free or random one, offers the best description of the data. © Fondazione Rosselli 2009.},
   author = {Paul Ormerod and Greg Wiltshire},
   doi = {10.1007/s11299-009-0058-1},
   issn = {15937879},
   issue = {2},
   journal = {Mind and Society},
   keywords = {Agent based model,Simulation methodology,Social network effect},
   month = {10},
   pages = {135-152},
   title = {'Binge' drinking in the UK: A social network phenomenon},
   volume = {8},
   year = {2009},
}
@article{abmAlcoholTaxation,
   abstract = {Aims: To use simulation to estimate the impact of alcohol taxation on drinking, non-fatal violent victimization and homicide in New York City (NYC). We simulate the heterogeneous effects of alcohol price elasticities by income, level of consumption and beverage preferences, and examine whether taxation can reduce income inequalities in alcohol-related violence. Design: Agent-based modeling simulation. Setting: NYC, USA. Participants: Adult population aged 18–64 years in the year 2000 in the 59 community districts of NYC. The population of 256 500 agents approximates a 5% sample of the NYC population. Measurements: Agents were parameterized through a series of rules that governed alcohol consumption and engagement in violence. Six taxation interventions were implemented based on extensive reviews and meta-analyses, increasing universal alcohol tax by 1, 5 and 10%, and beer tax by 1, 5 and 10%. Findings: Under no tax increase, approximately 12.2% [95% credible interval (prediction interval, PI) = 12.1–12.3%] were heavy drinkers. Taxation decreased the proportion of heavy drinkers; a 10% tax decreased heavy drinking to 9.6% (95% PI = 9.4–9.8). Beer taxes had the strongest effect on population consumption. Taxation influenced those in the lowest income groups more than the highest income groups. Alcohol-related homicide decreased from 3.22 per 100 000 (95% PI = 2.50–3.73) to 2.40 per 100 000 under a 10% universal tax (95% PI = 1.92–2.94). This translates into an anticipated benefit of ~1200 lives/year. Conclusion: Reductions in alcohol consumption in a large urban environment such as New York City can be sustained with modest increases in universal taxation. Alcohol tax increases also have a modest effect on alcohol-related violent victimization. Taxation policies reduce income inequalities in alcohol-related violence.},
   author = {Katherine M. Keyes and Aaron Shev and Melissa Tracy and Magdalena Cerdá},
   doi = {10.1111/add.14470},
   issn = {13600443},
   issue = {2},
   journal = {Addiction},
   keywords = {Agent-based modeling,alcohol,complex system,homicide,taxation,taxes,violence,violent victimization},
   month = {2},
   pages = {236-247},
   pmid = {30315599},
   publisher = {Blackwell Publishing Ltd},
   title = {Assessing the impact of alcohol taxation on rates of violent victimization in a large urban area: an agent-based modeling approach},
   volume = {114},
   year = {2019},
}
@report{abmSimARC,
   abstract = {Alcohol-related problems (assaults, accidents and/or crimes) and alcohol abuse are recurrent societal problems leading to high social costs. Finding adapted policies to tackle this issue isn't a trivial task due to the highly complex nature of alcohol consumption as many interrelated risk factors interact in a hardly predictable way. This paper describes an agent-based simulation model, called SimARC (Simulation of Alcohol-Related Consequences), aiming at exploring the complex interplay of these factors following a generative process whereby theory and model co-evolve within iterative loops. To explore the complexity of alcohol use and abuse, we need not only to include the aforementioned risk factors but also their evolution and highly dynamical interactions across scales. Therefore, our agent-based model aims to encapsulate several levels of reality. Considering an ontology as catalog of elements and relation amongst those elements, our ontology-driven behavioral model includes: neuro-biological responses to alcohol use (individual level), peer influence channeled through various social networks (meso-level) and societal responses to alcohol-related problems (meta-level). This ontological framework aims to establish a robust test-bed to analyze-in silico-the plausible consequences of various public policies related to alcohol abuse in public venues. After a brief review of the literature, we present SimARC's core structure and preliminary results.},
   author = {Francois Lamy and Pascal Perez and Alison Ritter and Michael Livingston},
   institution = {University of Wollongong},
   keywords = {alcohol,component; agent-based model,ontology,public health,social simulation},
   title = {An Agent-based Model of Alcohol Use and Abuse: SimARC},
   year = {2011},
}
@report{abmDutchAdults,
   abstract = {Binge drinking is a complex social problem linked to an array of detrimental health effects. While binge drinking in youth has been analyzed extensively using traditional methods (e.g., regressions analyses), the adult population has received less attention, and recent work has exemplified the potential for simulations to help scholars and practitioners better understand the problem. In this paper, we used agent-based social network models to test a number of hypotheses on important aspects of binge drinking in a sample representative of the adult Dutch population. In particular, we found that a combination of simple social rules (choosing peers who are similar, being prompted to drink if at least a fraction of them drinks, and incorporating the context) was sufficient to correctly predict the behaviour of half of the binge drinkers and 4 out of 5 non binge drinkers. Furthermore, we used factorial analyses to examine the contribution and combination of hypotheses in predicting the behaviour of individuals, with results indicating that who we interact with may not matter so much as how we interact. Finally, we evaluated the potential for interventions that mediate interactions between people in order to reduce the prevalence of binge drinking and found that the impact of such interventions was non linear: moderate interventions would yield benefits, but stronger interventions may only be of limited further benefit.},
   author = {P Giabbanelli and R Crutzen},
   keywords = {Conceptual Exploration,Drinking Motives,Social Influence},
   title = {An Agent-Based Social Network Model of Binge Drinking Among Dutch Adults Journal of Artificial Societies and Social Simulation},
   url = {http://jasss.soc.surrey.ac.uk/16/2/10.html},
   year = {2013},
}
@article{abmDrinkingBehaviour,
   abstract = {Objectives. We developed a preliminary agent-based simulation model designed to examine agent-environment interactions that support the development and maintenance of drinking behavior at the population level. Methods. The model was defined on a 1-dimensional lattice along which agents might move left or right in single steps at each iteration. Agents could exchange information about their drinking with each other. In the second generation of the model, a "bar" was added to the lattice to attract drinkers. Results. The model showed that changes in drinking status propagated through the agent population as a function of probabilities of conversion, rates of contact, and contact time. There was a critical speed of population mixing beyond which the conversion rate of susceptible nondrinkers was saturated, and the bar both enhanced and buffered the rate of propagation, changing the model dynamics. Conclusions. The models demonstrate that the basic dynamics underlying social influences on drinking behavior are shaped by contacts between drinkers and focused by characteristics of drinking environments.},
   author = {Dennis M. Gorman and Jadranka Mezic and Igor Mezic and Paul J. Gruenewald},
   doi = {10.2105/AJPH.2005.063289},
   issn = {00900036},
   issue = {11},
   journal = {American Journal of Public Health},
   month = {11},
   pages = {2055-2060},
   pmid = {17018835},
   title = {Agent-based modeling of drinking behavior: A preliminary model and potential applications to theory and practice},
   volume = {96},
   year = {2006},
}
@article{abmEcologicalNiche,
   abstract = {The present paper presents a preliminary approach to the modeling of dynamic properties of the spatial assortment of alcohol outlets using agent-based techniques. Individual drinkers and business establishments are the core agent types. Drinkers assort themselves by frequenting establishments due to spatial and social (niche) motivations. We examine a number of questions concerning the feedback relationships between establishments targeting a particular niche clientele and the individuals seeking more desirable places to obtain alcohol. ©Copyright JASSS.},
   author = {Ben Fitzpatrick and Jason Martinez},
   doi = {10.18564/jasss.1926},
   issn = {14607425},
   issue = {2},
   journal = {JASSS},
   keywords = {Alcohol outlets,Assortative drinking,Ecological niche theory},
   title = {Agent-based modeling of ecological niche theory and assortative drinking},
   volume = {15},
   year = {2012},
}
@article{abmTradingHours,
   abstract = {Background and aim: Evaluations of alcohol policy changes demonstrate that restriction of trading hours of both ‘on’- and ‘off’-licence venues can be an effective means of reducing rates of alcohol-related harm. Despite this, the effects of different trading hour policy options over time, accounting for different contexts and demographic characteristics, and the common co-occurrence of other harm reduction strategies in trading hour policy initiatives, are difficult to estimate. The aim of this study was to use dynamic simulation modelling to compare estimated impacts over time of a range of trading hour policy options on various indicators of acute alcohol-related harm. Methods: An agent-based model of alcohol consumption in New South Wales, Australia was developed using existing research evidence, analysis of available data and a structured approach to incorporating expert opinion. Five policy scenarios were simulated, including restrictions to trading hours of on-licence venues and extensions to trading hours of bottle shops. The impact of the scenarios on four measures of alcohol-related harm were considered: total acute harms, alcohol-related violence, emergency department (ED) presentations and hospitalizations. Results: Simulation of a 3 a.m. (rather than 5 a.m.) closing time resulted in an estimated 12.3 ± 2.4% reduction in total acute alcohol-related harms, a 7.9 ± 0.8% reduction in violence, an 11.9 ± 2.1% reduction in ED presentations and a 9.5 ± 1.8% reduction in hospitalizations. Further reductions were achieved simulating a 1 a.m. closing time, including a 17.5 ± 1.1% reduction in alcohol-related violence. Simulated extensions to bottle shop trading hours resulted in increases in rates of all four measures of harm, although most of the effects came from increasing operating hours from 10 p.m. to 11 p.m. Conclusions: An agent-based simulation model suggests that restricting trading hours of licensed venues reduces rates of alcohol-related harm and extending trading hours of bottle shops increases rates of alcohol-related harm. The model can estimate the effects of a range of policy options.},
   author = {Jo An Atkinson and Ante Prodan and Michael Livingston and Dylan Knowles and Eloise O'Donnell and Robin Room and Devon Indig and Andrew Page and Geoff McDonnell and John Wiggers},
   doi = {10.1111/add.14178},
   issn = {13600443},
   issue = {7},
   journal = {Addiction},
   keywords = {Agent-based modelling,alcohol-related harm,dynamic simulation modelling,evaluation,simulation,trading hour policy},
   month = {7},
   pages = {1244-1251},
   pmid = {29396879},
   publisher = {Blackwell Publishing Ltd},
   title = {Impacts of licensed premises trading hour policies on alcohol-related harms},
   volume = {113},
   year = {2018},
}
@web_page{liverDiseaseList,
   title = {Liver Disease: Types of Liver Problems, Causes, and More},
   url = {https://www.healthline.com/health/liver-diseases#common-problems},
}
@article{fctRetro,
   abstract = {Fundamental cause theory (FCT) was originally proposed to explain how socioeconomic inequalities in health emerged and persisted over time. The concept was that higher socioeconomic status helped some people to avoid risks and adopt protective strategies using flexible resources: knowledge, money, power, prestige, and beneficial social connections. As a sociological theory, FCT addressed this issue by calling on social stratification, stigma, and racism as they affected medical treatments and health outcomes. The last comprehensive review was completed a decade ago. Since then, FCT has been tested, and new applications have extended central features. The current review consolidates key foci in the literature in order to guide future research in the field. Notable themes emerged around types of resources and their usage, approaches used to test the theory, and novel extensions. We conclude that after 25 years of use, there remain crucial questions to be addressed.},
   author = {Sean A P Clouston and Bruce G Link},
   doi = {10.1146/annurev-soc-090320},
   keywords = {fundamental causes,medical sociology,racism,socioeconomic status,stigma},
   title = {Annual Review of Sociology A Retrospective on Fundamental Cause Theory: State of the Literature and Goals for the Future},
   url = {https://doi.org/10.1146/annurev-soc-090320-},
   year = {2021},
}
@report{Duncan2002,
   abstract = {Duncan et al. | Peer Reviewed | Research and Practice | 1151  RESEARCH AND PRACTICE  Objectives. In this study we examined the relationship between indicators of socioeconomic status (SES) and mortality for a representative sample of individuals. Methods. The sample included 3734 individuals aged 45 and older interviewed in 1984 in the Panel Study of Income Dynamics. In the current study, mortality was tracked between 1984 and 1994 and is related to SES indicators of education, occupation, income, and wealth. Results. Wealth and recent family income were the indicators that were most strongly associated with subsequent mortality. These associations persisted after we controlled for the other SES indicators and were stronger for women than for men and for nonel-derly than for elderly individuals. Conclusions. We found that the economic indicators of SES were usually as strongly associated with mortality as, if not more strongly associated with mortality than, the more conventional indicators of completed schooling and occupation. In general, indicators of SES are meant to provide information about an individual's access to social and economic resources. As such, they are markers of social relationships and command over resources and skills that vary over time. 23-24 Among the most frequently used socioeconomic indicators are education and occupation. Economic indicators such as household income and wealth are used less frequently but are potentially as important as or more important than education and occupation. We describe the benefits and drawbacks of each indicator below. Education is an important determinant of individuals' work and economic circumstances , 25 which are themselves linked to health through specific work conditions and levels of consumption. Education may also be associated with health through its connection to health behaviors. The higher one's level of education, the more likely one is to engage in a range of health-enhancing self-maintenance activities. 26,27 Years of completed schooling are reported with reasonable ease and reliability and are a meaningful indicator of SES for virtually all adults. Because education is typically completed early in adulthood, it serves as a marker of early life circumstances, 28 and no reverse-causation problems result from linking education with health outcomes at older ages. It is for these reasons that the National Center for Health Statistics (NCHS) selected education for inclusion in death certificates in 1989 7 and that the National Committee on Vital and Health Statistics has offered the preliminary assessment that education may be the most useful SES indicator for administrative databases. 11 However, education captures neither the differential on-the-job training and other career investments made by individuals with similar levels of formal schooling nor the volatility in economic status during adulthood that has recently been shown to adversely affect health. 16 Usual or most recent occupation has long been used as an SES indicator for persons in the workforce, and it can have direct and indirect effects on health. For example, occupation represents exposure to the psychosocial and physical dimensions of work arrangements 29,30 as well as a range of expected earnings and social capital in the form of relative standing or prestige. Indicators of occupational class are widely used in other industrialized countries and have been found to be robust in predicting variations in health status. 17 The National Institutes of Health (NIH) conference on Measuring Social Inequalities in Health called for including occupation as a core SES variable in the US health status re-Although numerous studies have documented the associations between socioeconomic status (SES) indicators and a variety of health outcomes, 1-6 comprehensive indicators of SES are not routinely collected in the United States. In addition, most SES data that are obtained are not reported. 7-9 This data deficiency was highlighted at a 1996 federally sponsored health conference on SES 1,10 and has been noted by the National Committee on Vital and Health Statistics. 11 In both cases, the recommendation was for regular collection of SES data and for the use of SES variables in studies of differential health outcomes. Despite growing awareness of the need for regular collection of SES indicators, however, there is little agreement on which indicators should be gathered. 12 One problem is that numerous indicators of SES, including occupation , 13 education, 14,3 and household income , 4,5,15,16 have been shown to affect health outcomes, but these indicators are not interchangeable. 12,17-19 Moreover, the impact on health of any particular SES indicator-such as one based on sex and age-varies across different population subgroups. 3,20-22 The fact that various SES indicators may capture different aspects of overall health risk suggests that a systematic examination of the explanatory power of a variety of SES indicators is required before an optimal set of indicators can be recommended. We contribute to this examination by analyzing the empirical relationship between a set of SES indicators (available from both administrative and survey data sources) and mortality for a nationally representative sample of individuals. We used a unique data set, the Panel Study of Income Dynamics (PSID), to evaluate the predictive power of a variety of SES indicators. Although it includes the traditional SES indicators of education and occupation, our analysis focuses on the relatively neglected economic indicators of SES.},
   author = {Greg J Duncan and Mary C Daly and Peggy Mcdonough and David R Williams},
   issue = {7},
   journal = {American Journal of Public Health},
   title = {Optimal Indicators of Socioeconomic Status for Health Research},
   volume = {92},
   year = {2002},
}
@report{fctNeighbourhoods,
   author = {Catherine E Ross and John Mirowsky},
   issue = {3},
   journal = {Journal of Health and Social Behavior},
   pages = {258-276},
   title = {Neighborhood Disadvantage, Disorder, and Health},
   volume = {42},
   year = {2001},
}
@article{Pampel2010,
   abstract = {The inverse relationships between socioeconomic status (SES) and unhealthy behaviors such as tobacco use, physical inactivity, and poor nutrition have been well demonstrated empirically but encompass diverse underlying causal mechanisms. These mechanisms have special theoretical importance because disparities in health behaviors, unlike disparities in many other components of health, involve something more than the ability to use income to purchase good health. Based on a review of broad literatures in sociology, economics, and public health, we classify explanations of higher smoking, lower exercise, poorer diet, and excess weight among low-SES persons into nine broad groups that specify related but conceptually distinct mechanisms. The lack of clear support for any one explanation suggests that the literature on SES disparities in health and health behaviors can do more to design studies that better test for the importance of the varied mechanisms. © 2010 by Annual Reviews. All rights reserved.},
   author = {Fred C. Pampel and Patrick M. Krueger and Justin T. Denney},
   doi = {10.1146/annurev.soc.012809.102529},
   issn = {03600572},
   journal = {Annual Review of Sociology},
   keywords = {diet,education,exercise,obesity,smoking,socioeconomic status},
   pages = {349-370},
   pmid = {21909182},
   title = {Socioeconomic disparities in health behaviors},
   volume = {36},
   year = {2010},
}
@article{fctStressors,
   abstract = {Forty decades of sociological stress research offer five major findings. First, when stressors (negative events, chronic strains, and traumas) are measured comprehensively, their damaging impacts on physical and mental health are substantial. Second, differential exposure to stressful experiences is a primary way that gender, racial-ethnic, marital status, and social class inequalities in physical and mental health are produced. Third, minority group members are additionally harmed by discrimination stress. Fourth, stressors proliferate over the life course and across generations, widening health gaps between advantaged and disadvantaged group members. Fifth, the impacts of stressors on health and well-being are reduced when persons have high levels of mastery, self-esteem, and/or social support. With respect to policy, to help individuals cope with adversity, tried and true coping and support interventions should be more widely disseminated and employed. To address health inequalities, the structural conditions that put people at risk of stressors should be a focus of programs and policies at macro and meso levels of intervention. Programs and policies also should target children who are at lifetime risk of ill health and distress due to exposure to poverty and stressful family circumstances.},
   author = {Peggy A. Thoits},
   doi = {10.1177/0022146510383499},
   issn = {21506000},
   issue = {1_suppl},
   journal = {Journal of Health and Social Behavior},
   keywords = {health policy,inequality,mental health,physical health,stress},
   month = {3},
   pages = {S41-S53},
   pmid = {20943582},
   publisher = {American Sociological Association},
   title = {Stress and Health: Major Findings and Policy Implications},
   volume = {51},
   year = {2010},
}
@article{Phelan2010,
   abstract = {Link and Phelan (1995) developed the theory of fundamental causes to explain why the association between socioeconomic status (SES) and mortality has persisted despite radical changes in the diseases and risk factors that are presumed to explain it. They proposed that the enduring association results because SES embodies an array of resources, such as money, knowledge, prestige, power, and beneficial social connections that protect health no matter what mechanisms are relevant at any given time. In this article, we explicate the theory, review key findings, discuss refinements and limits to the theory, and discuss implications for health policies that might reduce health inequalities. We advocate policies that encourage medical and other health-promoting advances while at the same time breaking or weakening the link between these advances and socioeconomic resources. This can be accomplished either by reducing disparities in socioeconomic resources themselves or by developing interventions that, by their nature, are more equally distributed across SES groups.},
   author = {Jo C. Phelan and Bruce G. Link and Parisa Tehranifar},
   doi = {10.1177/0022146510383498},
   issn = {21506000},
   issue = {1_suppl},
   journal = {Journal of Health and Social Behavior},
   keywords = {fundamental causes,health,health disparities,mortality,social stratification},
   month = {3},
   pages = {S28-S40},
   pmid = {20943581},
   publisher = {American Sociological Association},
   title = {Social Conditions as Fundamental Causes of Health Inequalities: Theory, Evidence, and Policy Implications},
   volume = {51},
   year = {2010},
}
@report{inverseCareLaw,
   abstract = {The availability of good medical care tends to vary inversely with the need for it in the population served. This inverse care law operates more completely where medical care is most exposed to market forces, and less so where such exposure is reduced. The market distribution of medical care is a primitive and historically outdated social form, and any return to it would further exaggerate the maldistribution of medical resources. Interpreting the Evidence THE existence of large social and geographical inequalities in mortality and morbidity in Britain is known, and not all of them are diminishing. Between 1934 and 1968, weighted mean standardised mortality from all causes in the Glamorgan and Monmouthshire valleys rose from 128% of England and Wales rates to 131 %. Their weighted mean infant mortality rose from 115% of England and Wales rates to 124% between 1921 and 1968.1 The Registrar General's last Decennial Supplement on Occupational Mortality for 1949-53 still showed combined social classes i and II (wholly non-manual) with a standardised mortality from all causes 18% below the mean, and combined social classes iv and v (wholly manual) 5% above it. Infant mortality was 37% below the mean for social class (professional) and 38% above it for social class v (unskilled manual). A just and rational distribution of the resources of medical care should show parallel social and geographical differences, or at least a uniform distribution. The common experience was described by Titmuss in 1968: " We have learnt from 15 years' experience of the Health Service that the higher income groups know how to make better use of the service; they tend to receive more specialist attention; occupy more of the beds in better equipped and staffed hospitals; receive more elective surgery; have better maternal care, and are more likely to get psychiatric help and psychotherapy than low-income groups-particularly the unskilled." 2 These generalisations are not easily proved statistically , because most of the statistics are either not available (for instance, outpatient waiting-lists by area and social class, age and cause specific hospital mortality rates by area and social class, the relation between ante-mortem and post-mortem diagnosis by area and social class, and hospital staff shortage by area) or else they are essentially use-rates. Use-rates may be interpreted either as evidence of high morbidity among high users, or of disproportionate benefit drawn by them from the National Health Service. By piling up the valid evidence that poor people in Britain have higher consultation and referral rates at all levels of the N.H.S., and by denying that these reflect actual differences in morbidity, Rein 3,4 has tried to show that Titmuss's opinion is incorrect, and that there are no significant gradients in the quality or accessibility of medical care in the N.H.S. between social classes.},
   author = {Julian Tudor Hart},
   city = {Port Talbot},
   title = {THE INVERSE CARE LAW},
   year = {1971},
}
@article{primateSocialDeterminants,
   abstract = {It is well established that health depends on socioeconomic circumstances, but the biology of this relation is not well described. Psychosocial factors operating throughout the life course, beginning in early life, influence a variety of biological variables. Research with non-human primates shows the effects of dominance hierarchy on biology, and similar metabolic differentials are evident in a hierarchy of white collar civil servants. The neuroendocrine “fight or flight” response produces physiological and metabolic alterations which parallel those observed with lower socioeconomic status. The biological effects of the psychosocial environment could explain health inequalities between relatively affluent groups.},
   author = {E. Brunner},
   doi = {10.1136/bmj.314.7092.1472},
   issn = {0959-8138},
   issue = {7092},
   journal = {BMJ},
   month = {5},
   pages = {1472-1472},
   title = {Socioeconomic determinants of health: Stress and the biology of inequality},
   volume = {314},
   year = {1997},
}
@report{ahpOrigin,
   author = {K Smith and J Foster},
   city = {London},
   institution = {Institute of Alcohol Studies},
   title = {Alcohol, Health Inequalities and the Harm Paradox: Why some groups face greater problems despite consuming less alcohol},
   year = {2014},
}
@article{Krieger1997,
   author = {N. Keieger and D. R. Williams and N. E. Moss},
   journal = {Annual Review of Public Health},
   pages = {341-378},
   title = {MEASURING SOCIAL CLASS IN US PUBLIC HEALTH RESEARCH- Concepts, Methodologies, and Guidelines},
   year = {1997},
}
@article{csHealthDisparities,
   abstract = {Complex systems approaches have received increasing attention in public health because reductionist approaches yield limited insights in the context of dynamic systems. Most discussions have been highly abstract. There is a need to consider the application of complex systems approaches to specific research questions. I review the features of population health problems for which complex systems approaches are most likely to yield new insights, and discuss possible applications of complex systems to health disparities research. I provide illustrative examples of how complex systems approaches may help address unanswered and persistent questions regarding genetic factors, life course processes, place effects, and the impact of upstream policies. The concepts and methods of complex systems may help researchers move beyond current impasse points in health disparities research.},
   author = {Ana V.Diez Roux},
   doi = {10.2105/AJPH.2011.300149},
   issn = {00900036},
   issue = {9},
   journal = {American Journal of Public Health},
   month = {9},
   pages = {1627-1634},
   pmid = {21778505},
   title = {Complex systems thinking and current impasses in health disparities research},
   volume = {101},
   year = {2011},
}
@report{sdhInterventions,
   abstract = {r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r r T here is considerable scientific and policy interest in reducing socioeconomic and racial/ethnic disparities in healthcare and health status. Currently, much of the policy focus around reducing health disparities has been geared toward improving access, coverage, quality, and the intensity of healthcare. However, health is more a function of lifestyles linked to living and working conditions than of healthcare. Accordingly, effective efforts to improve health and reduce gaps in health need to pay greater attention to addressing the social determinants of health within and outside of the healthcare system. This article highlights research evidence documenting that tackling the social determinants of health can lead to reductions in health disparities. It focuses both on interventions within the healthcare system that address some of the social determinants of health and on interventions in upstream factors such as housing, neighborhood conditions, and increased socioeconomic status that can lead to improvements in health. The studies reviewed highlight the importance of systematic evaluation of social and economic policies that might have health consequences and the need for policy makers, healthcare providers, and leaders across multiple sectors of society to apply currently available knowledge to improve the underlying conditions that impact the health of populations. KEY WORDS: healthcare, interventions, racial disparities, socioeconomic disparities In the past decade, there has been increased awareness of large racial and ethnic disparities in medical care and many new initiatives to address them. Despite these efforts, several recent studies indicate the persistence of large disparities in treatment by race and ethnicity. 1 Even more important than these healthcare disparities is the striking evidence of the persistence of disparities in health status by both socioeconomic status (SES) 2 and race/ethnicity. 3 Despite a broad range of societal efforts to improve the health practices and quality of life of socioeconomically disadvantaged populations , very little progress has been made in reducing social gaps in health. Moreover, there is growing evidence that these disparities are costly to society in multiple ways. For example, a recent economic analysis revealed that if all adult Americans experienced the level of illness and mortality of college graduates, the annual economic benefit would amount to at least 1 trillion dollars. 4 These substantial costs highlight the urgency of renewed efforts to find effective strategies to improve health and reduce social disparities. Currently, much of the policy focus around reducing social disparities in health status has been on initiatives seeking to improve access and coverage as well as). q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q},
   author = {David R Williams and Manuela V Costa and Adebola O Odunlami and Selina A Mohammed and Norman Professor},
   journal = {J Public Health Management Practice},
   pages = {8-17},
   title = {Moving Upstream: How Interventions That Address the Social Determinants of Health Can Improve Health and Reduce Disparities},
   year = {2008},
}
@article{SAPM,
   abstract = {This methodology paper sets out a mathematical description of the Sheffield Alcohol Policy Model version 2.0, a model to evaluate public health strategies for alcohol harm reduction in the UK. Policies that can be appraised include a minimum price per unit of alcohol, restrictions on price discounting, and broader public health measures. The model estimates the impact on consumers, health services, crime, employers, retailers and government tax revenues. The synthesis of public and commercial data sources to inform the model structure is described. A detailed algebraic description of the model is provided. This involves quantifying baseline levels of alcohol purchasing and consumption by age and gender subgroups, estimating the impact of policies on consumption, for example, using evidence on price elasticities of demand for alcohol, quantification of risk functions relating alcohol consumption to harms including 47 health conditions, crimes, absenteeism and unemployment, and finally monetary valuation of the consequences. The results framework, shown for a minimum price per unit of alcohol, has been used to provide policy appraisals for the UK government policy-makers. In discussion and online appendix, we explore issues around valuation and scope, limitations of evidence/data, how the framework can be adapted to other countries and decisions, and ongoing plans for further development.},
   author = {Alan Brennan and Petra Meier and Robin Purshouse and Rachid Rafia and Yang Meng and Daniel Hill-Macmanus and Colin Angus and John Holmes},
   doi = {10.1002/hec.3105},
   issn = {10991050},
   issue = {10},
   journal = {Health Economics (United Kingdom)},
   keywords = {alcohol,economic evaluation,modelling methodology,public health},
   month = {10},
   pages = {1368-1388},
   publisher = {John Wiley and Sons Ltd},
   title = {The Sheffield alcohol policy model - A mathematical description},
   volume = {24},
   year = {2015},
}
@article{ahpInterventions,
   author = {H Baldwin and G Bates and M Bellis},
   journal = {Alcohol Research UK},
   title = {Understanding the alcohol harm paradox in order to focus the development of interventions},
   url = {http://alcoholresearchuk.org/alcohol-insights/understanding-the-alcohol-harm-paradox-2/},
   year = {2015},
}
@article{covidABM,
   abstract = {SARS-CoV-2 has spread across the world, causing high mortality and unprecedented restrictions on social and economic activity. Policymakers are assessing how best to navigate through the ongoing epidemic, with computational models being used to predict the spread of infection and assess the impact of public health measures. Here, we present OpenABM-Covid19: An agent-based simulation of the epidemic including detailed age-stratification and realistic social networks. By default the model is parameterised to UK demographics and calibrated to the UK epidemic, however, it can easily be re-parameterised for other countries. OpenABM-Covid19 can evaluate non-pharmaceutical interventions, including both manual and digital contact tracing, and vaccination programmes. It can simulate a population of 1 million people in seconds per day, allowing parameter sweeps and formal statistical model-based inference. The code is open-source and has been developed by teams both inside and outside academia, with an emphasis on formal testing, documentation, modularity and transparency. A key feature of OpenABM-Covid19 are its Python and R interfaces, which has allowed scientists and policymakers to simulate dynamic packages of interventions and help compare options to suppress the COVID-19},
   author = {Robert Hinch and William J.M. Probert and Anel Nurtay and Michelle Kendall and Chris Wymant and Matthew Hall and Katrina Lythgoe and Ana Bulas Cruz and Lele Zhao and Andrea Stewart and Luca Ferretti and Daniel Montero and James Warren and Nicole Mather and Matthew Abueg and Neo Wu and Olivier Legat and Katie Bentley and Thomas Mead and Kelvin Van-Vuuren and Dylan Feldner-Busztin and Tommaso Ristori and Anthony Finkelstein and Dav G. Bonsall and Lucie Abeler-Dörner and Christophe Fraser},
   doi = {10.1371/journal.pcbi.1009146},
   issn = {15537358},
   issue = {7},
   journal = {PLoS Computational Biology},
   month = {7},
   pmid = {34252083},
   publisher = {Public Library of Science},
   title = {OpenABM-Covid19-An agent-based model for non-pharmaceutical interventions against COVID-19 including contact tracing},
   volume = {17},
   year = {2021},
}
@article{noDrinkvsSomeDrink,
   abstract = {Background: While current research is largely consistent as to the harms of heavy drinking in terms of both cancer incidence and mortality, there are disparate messages regarding the safety of light-moderate alcohol consumption, which may confuse public health messages. We aimed to evaluate the association between average lifetime alcohol intakes and risk of both cancer incidence and mortality. Methods and findings: We report a population-based cohort study using data from 99,654 adults (68.7% female), aged 55–74 years, participating in the U.S. Prostate, Lung, Colorectal, and Ovarian (PLCO) Cancer Screening Trial. Cox proportional hazards models assessed the risk of overall and cause-specific mortality, cancer incidence (excluding nonmelanoma skin cancer), and combined risk of cancer and death across categories of self-reported average lifetime alcohol intakes, with adjustment for potential confounders. During 836,740 person-years of follow-up (median 8.9 years), 9,599 deaths and 12,763 primary cancers occurred. Positive linear associations were observed between lifetime alcohol consumption and cancer-related mortality and total cancer incidence. J-shaped associations were observed between average lifetime alcohol consumption and overall mortality, cardiovascular-related mortality, and combined risk of death or cancer. In comparison to lifetime light alcohol drinkers (1–3 drinks per week), lifetime never or infrequent drinkers (<1 drink/week), as well as heavy (2–<3 drinks/day) and very heavy drinkers (3+ drinks/day) had increased overall mortality and combined risk of cancer or death. Corresponding hazard ratios (HRs) and 95% confidence intervals (CIs) for combined risk of cancer or death, respectively, were 1.09 (1.01–1.13) for never drinkers, 1.08 (1.03–1.13) for infrequent drinkers, 1.10 (1.02–1.18) for heavy drinkers, and 1.21 (1.13–1.30) for very heavy drinkers. This analysis is limited to older adults, and residual confounding by socioeconomic factors is possible. Conclusions: The study supports a J-shaped association between alcohol and mortality in older adults, which remains after adjustment for cancer risk. The results indicate that intakes below 1 drink per day were associated with the lowest risk of death. Trial registration: NCT00339495 (ClinicalTrials.gov).},
   author = {Andrew T. Kunzmann and Helen G. Coleman and Wen Yi Huang and Sonja I. Berndt},
   doi = {10.1371/journal.pmed.1002585},
   issn = {15491676},
   issue = {6},
   journal = {PLoS Medicine},
   month = {6},
   pmid = {29920516},
   publisher = {Public Library of Science},
   title = {The association of lifetime alcohol use with mortality and cancer risk in older adults: A cohort study},
   volume = {15},
   year = {2018},
}
@report{fullAlcoholHarms,
   abstract = {Alcohol consumption, particularly heavier drinking, is an important risk factor for many health problems and, thus, is a major contributor to the global burden of disease. In fact, alcohol is a necessary underlying cause for more than 30 conditions and a contributing factor to many more. The most common disease categories that are entirely or partly caused by alcohol consumption include infectious diseases, cancer, diabetes, neuropsychiatric diseases (including alcohol use disorders), cardiovascular disease, liver and pancreas disease, and unintentional and intentional injury. Knowledge of these disease risks has helped in the development of low­risk drinking guidelines. In addition to these disease risks that affect the drinker, alcohol consumption also can affect the health of others and cause social harm both to the drinker and to others, adding to the overall cost associated with alcohol consumption. These findings underscore the need to develop effective prevention efforts to reduce the pain and suffering, and the associated costs, resulting from excessive alcohol use. KEY WORDS: alcohol and other drug (AOD) use; alcohol use disorders; alcoholism; heavy drinking; AOD induced risk; AOD effects and consequences; health; disease cause; disease factor; disease risk and protective factors; burden of disease; health care costs; injury; social harm; drinking guidelines; prevention},
   author = {Jürgen Rehm},
   pages = {135-143},
   title = {Alcohol Research \& Health: Preventing Alcohol Abuse and Alcoholism Update},
   year = {2011},
}
@report{alcoholRiskThresholds,
   abstract = {Background Low-risk limits recommended for alcohol consumption vary substantially across different national guidelines. To define thresholds associated with lowest risk for all-cause mortality and cardiovascular disease, we studied individual-participant data from 599 912 current drinkers without previous cardiovascular disease.},
   author = {Angela M Wood and Stephen Kaptoge and Adam S Butterworth and Samantha Warnakula},
   journal = {www.thelancet.com},
   title = {Risk thresholds for alcohol consumption: combined analysis of individual-participant data for 599 912 current drinkers in 83 prospective studies},
   volume = {391},
   url = {www.thelancet.com},
   year = {2018},
}
@report{scotlandAlcohol2022,
   author = {V Ponce Hardy and L Giles},
   city = {Edinburgh},
   institution = {Public Health Scotland},
   title = {Monitoring and Evaluating Scotland’s Alcohol Strategy: Monitoring Report 2022},
   year = {2022},
}
@report{englandAlcohol2021,
   abstract = {This report presents newly published information on prescriptions items for drugs used to treat alcohol dependence and affordability of alcohol and expenditure on alcohol.},
   institution = {Office for National Statistics},
   month = {1},
   title = {Current Chapter Statistics on Alcohol, England 2021},
   year = {2022},
}
@report{scotlandMUP,
   abstract = {Acknowledgements We are grateful to the members of the MESAS Governance Board who provided advice on the overall MUP evaluation design described in this protocol and now oversee its implementation. We are also grateful to the members of the Evaluation Advisory Groups who provide advice to individual or groups of studies. We are grateful to the leads of the MESAS-funded studies, and to the leads of the separately funded studies, for their input to the MUP evaluation and this protocol.},
   author = {Wraw C Protocol and Clare Beeston},
   title = {Monitoring and Evaluating Scotland's Alcohol Strategy (MESAS) MUP evaluation Protocol for the evaluation of Minimum Unit Pricing for alcohol},
   year = {2021},
}
@report{WHOGlobalStatusReportFull,
   city = {Geneva},
   institution = {World Health Organisation},
   title = {WHO Global Status Report on Alcohol},
   year = {2018},
}
@article{lancetPopRisks,
   author = {Dana Bryazka and Marissa B Reitsma and Max G Griswold and Kalkidan Hassen Abate and Cristiana Abbafati and Mohsen Abbasi-Kangevari and Zeinab Abbasi-Kangevari and Amir Abdoli and Mohammad Abdollahi and Abu Yousuf Md Abdullah and E S Abhilash and Eman Abu-Gharbieh and Juan Manuel Acuna and Giovanni Addolorato and Oladimeji M Adebayo and Victor Adekanmbi and Kishor Adhikari and Sangeet Adhikari and Qorinah Estiningtyas Sakilah Adnani and Saira Afzal and Wubetu Yimam Agegnehu and Manik Aggarwal and Bright Opoku Ahinkorah and Araz Ramazan Ahmad and Sajjad Ahmad and Tauseef Ahmad and Ali Ahmadi and Sepideh Ahmadi and Haroon Ahmed and Tarik Ahmed Rashid and Chisom Joyqueenet Akunna and Hanadi Al Hamad and Md Zakiul Alam and Dejene Tsegaye Alem and Kefyalew Addis Alene and Yousef Alimohamadi and Atiyeh Alizadeh and Kasim Allel and Jordi Alonso and Saba Alvand and Nelson Alvis-Guzman and Firehiwot Amare and Edward Kwabena Ameyaw and Sohrab Amiri and Robert Ancuceanu and Jason A Anderson and Catalina Liliana Andrei and Tudorel Andrei and Jalal Arabloo and Muhammad Arshad and Anton A Artamonov and Zahra Aryan and Malke Asaad and Mulusew A Asemahagn and Thomas Astell-Burt and Seyyed Shamsadin Athari and Desta Debalkie Atnafu and Prince Atorkey and Alok Atreya and Floriane Ausloos and Marcel Ausloos and Getinet Ayano and Martin Amogre ayanore Ayanore and Olatunde O Ayinde and Jose L Ayuso-Mateos and Sina Azadnajafabad and Melkalem Mamuye Azanaw and Mohammadreza Azangou-Khyavy and Amirhossein Azari Jafari and Ahmed Y Azzam and Ashish D Badiye and Nasser Bagheri and Sara Bagherieh and Mohan Bairwa and Shankar M Bakkannavar and Ravleen Kaur Bakshi and Awraris Hailu Balchut/Bilchut and Till Winfried Bärnighausen and Fabio Barra and Amadou Barrow and Pritish Baskaran and Luis Belo and Derrick A Bennett and Isabela M Benseñor and Akshaya Srikanth Bhagavathula and Neeraj Bhala and Ashish Bhalla and Nikha Bhardwaj and Pankaj Bhardwaj and Sonu Bhaskar and Krittika Bhattacharyya and Vijayalakshmi S Bhojaraja and Bagas Suryo Bintoro and Elena A Elena Blokhina and Belay Boda Abule Bodicha and Archith Boloor and Cristina Bosetti and Dejana Braithwaite and Hermann Brenner and Nikolay Ivanovich Briko and Andre R Brunoni and Zahid A Butt and Chao Cao and Yin Cao and Rosario Cárdenas and Andre F Carvalho and Márcia Carvalho and Joao Mauricio Castaldelli-Maia and Giulio Castelpietra and Luis F S Castro-de-Araujo and Maria Sofia Cattaruzza and Promit Ananyo Chakraborty and Jaykaran Charan and Vijay Kumar Chattu and Akhilanand Chaurasia and Nicolas Cherbuin and Dinh-Toi Chu and Nandita Chudal and Sheng-Chia Chung and Chuchu Churko and Liliana G Ciobanu and Massimo Cirillo and Rafael M Claro and Simona Costanzo and Richard G Cowden and Michael H Criqui and Natália Cruz-Martins and Garland T Culbreth and Berihun Assefa Dachew and Omid Dadras and Xiaochen Dai and Giovanni Damiani and Lalit Dandona and Rakhi Dandona and Beniam Darge Daniel and Anna Danielewicz and Jiregna Darega Gela and Kairat Davletov and Jacyra Azevedo Paiva de Araujo and Antonio Reis de Sá-Junior and Sisay Abebe Debela and Azizallah Dehghan and Andreas K Demetriades and Meseret Derbew Molla and Rupak Desai and Abebaw Alemayehu Desta and Diana Dias da Silva and Daniel Diaz and Lankamo Ena Digesa and Mengistie Diress and Milad Dodangeh and Deepa Dongarwar and Fariba Dorostkar and Haneil Larson Dsouza and Bereket Duko and Bruce B Duncan and Kristina Edvardsson and Michael Ekholuenetale and Frank J Elgar and Muhammed Elhadi and Mohamed A Elmonem and Aman Yesuf Endries and Sharareh Eskandarieh and Azin Etemadimanesh and Adeniyi Francis Fagbamigbe and Ildar Ravisovich Fakhradiyev and Fatemeh Farahmand and Carla Sofia e Sá Farinha and Andre Faro and Farshad Farzadfar and Ali Fatehizadeh and Nelsensius Klau Fauk and Valery L Feigin and Rachel Feldman and Xiaoqi Feng and Zinabu Fentaw and Simone Ferrero and Lorenzo Ferro Desideri and Irina Filip and Florian Fischer and Joel Msafiri Francis and Richard Charles Franklin and Peter Andras Gaal and Mohamed M Gad and Silvano Gallus and Fabio Galvano and Balasankar Ganesan and Tushar Garg and Mesfin Gebrehiwot Damtew Gebrehiwot and Teferi Gebru Gebremeskel and Mathewos Alemu Gebremichael and Tadele Regasa Gemechu and Lemma Getacher and Motuma Erena Getachew and Abera Getachew Obsa and Asmare Getie and Amir Ghaderi and Mansour Ghafourifard and Alireza Ghajar and Seyyed-Hadi Ghamari and Lilian A Ghandour and Mohammad Ghasemi Nour and Ahmad Ghashghaee and Sherief Ghozy and Franklin N Glozah and Ekaterina Vladimirovna Glushkova and Justyna Godos and Amit Goel and Salime Goharinezhad and Mahaveer Golechha and Pouya Goleij and Mohamad Golitaleb and Felix Greaves and Michal Grivna and Giuseppe Grosso and Temesgen Worku Gudayu and Bhawna Gupta and Rajeev Gupta and Sapna Gupta and Veer Bala Gupta and Vivek Kumar Gupta and Nima Hafezi-Nejad and Arvin Haj-Mirzaian and Brian J Hall and Rabih Halwani and Tiilahun Beyene Handiso and Graeme J Hankey and Sanam Hariri and Josep Maria Haro and Ahmed I Hasaballah and Hossein Hassanian-Moghaddam and Simon I Hay and Khezar Hayat and Golnaz Heidari and Mohammad Heidari and Delia Hendrie and Claudiu Herteliu and Demisu Zenbaba Heyi and Kamal Hezam and Mbuzeleni Mbuzeleni Hlongwa and Ramesh Holla and Md Mahbub Hossain and Sahadat Hossain and Seyed Kianoosh Hosseini and Mehdi hosseinzadeh and Mihaela Hostiuc and Sorin Hostiuc and Guoqing Hu and Junjie Huang and Salman Hussain and Segun Emmanuel Ibitoye and Irena M Ilic and Milena D Ilic and Mustapha Immurana and Lalu Muhammad Irham and M Mofizul Islam and Rakibul M Islam and Sheikh Mohammed Shariful Islam and Hiroyasu Iso and Ramaiah Itumalla and Masao Iwagami and Roxana Jabbarinejad and Louis Jacob and Mihajlo Jakovljevic and Zahra Jamalpoor and Elham Jamshidi and Sathish Kumar Jayapal and Umesh Umesh Jayarajah and Ranil Jayawardena and Rime Jebai and Seyed Ali Jeddi and Alelign Tasew Jema and Ravi Prakash Jha and Har Ashish Jindal and Jost B Jonas and Tamas Joo and Nitin Joseph and Farahnaz Joukar and Jacek Jerzy Jozwiak and Mikk Jürisson and Ali Kabir and Robel Hussen Kabthymer and Bhushan Dattatray Kamble and Himal Kandel and Girum Gebremeskel Kanno and Neeti Kapoor and Ibraheem M Karaye and Salah Eddin Karimi and Bekalu Getnet Kassa and Rimple Jeet Kaur and Gbenga A Kayode and Mohammad Keykhaei and Himanshu Khajuria and Rovshan Khalilov and Imteyaz A Khan and Moien AB Khan and Hanna Kim and Jihee Kim and Min Seo Kim and Ruth W Kimokoti and Mika Kivimäki and Vitalii Klymchuk and Ann Kristin Skrindo Knudsen and Ali-Asghar Kolahi and Vladimir Andreevich Korshunov and Ai Koyanagi and Kewal Krishan and Yuvaraj Krishnamoorthy and G Anil Kumar and Narinder Kumar and Nithin Kumar and Ben Lacey and Tea Lallukka and Savita Lasrado and Jerrald Lau and Sang-woong Lee and Wei-Chen Lee and Yo Han Lee and Lee-Ling Lim and Stephen S Lim and Stany W Lobo and Platon D Lopukhov and Stefan Lorkowski and Rafael Lozano and Giancarlo Lucchetti and Farzan Madadizadeh and Áurea M Madureira-Carvalho and Soleiman Mahjoub and Ata Mahmoodpoor and Rashidul Alam Mahumud and Alaa Makki and Mohammad-Reza Malekpour and Narayana Manjunatha and Borhan Mansouri and Mohammad Ali Mansournia and Jose Martinez-Raga and Francisco A Martinez-Villa and Richard Matzopoulos and Pallab K Maulik and Mahsa Mayeli and John J McGrath and Jitendra Kumar Meena and Entezar Mehrabi Nasab and Ritesh G Menezes and Gert B M Mensink and Alexios-Fotios A Mentis and Atte Meretoja and Bedasa Taye Merga and Tomislav Mestrovic and Junmei Miao Jonasson and Bartosz Miazgowski and Ana Carolina Micheletti Gomide Nogueira de Sá and Ted R Miller and GK Mini and Andreea Mirica and Antonio Mirijello and Seyyedmohammadsadeq Mirmoeeni and Erkin M Mirrakhimov and Sanjeev Misra and Babak Moazen and Maryam Mobarakabadi and Marcello Moccia and Yousef Mohammad and Esmaeil Mohammadi and Abdollah Mohammadian-Hafshejani and Teroj Abdulrahman Mohammed and Nagabhishek Moka and Ali H Mokdad and Sara Momtazmanesh and Yousef Moradi and Ebrahim Mostafavi and Sumaira Mubarik and Erin C Mullany and Beemnet Tekabe Mulugeta and Efrén Murillo-Zamora and Christopher J L Murray and Julius C Mwita and Mohsen Naghavi and Mukhammad David Naimzada and Vinay Nangia and Biswa Prakash Nayak and Ionut Negoi and Ruxandra Irina Negoi and Seyed Aria Nejadghaderi and Samata Nepal and Sudan Prasad Prasad Neupane and Sandhya Neupane Kandel and Yeshambel T Nigatu and Ali Nowroozi and Khan M Nuruzzaman and Chimezie Igwegbe Nzoputam and Kehinde O Obamiro and Felix Akpojene Ogbo and Ayodipupo Sikiru Oguntade and Hassan Okati-Aliabad and Babayemi Oluwaseun Olakunde and Gláucia Maria Moraes Oliveira and Ahmed Omar Bali and Emad Omer and Doris V Ortega-Altamirano and Adrian Otoiu and Stanislav S Otstavnov and Bilcha Oumer and Mahesh P A and Alicia Padron-Monedero and Raffaele Palladino and Adrian Pana and Songhomitra Panda-Jonas and Anamika Pandey and Ashok Pandey and Shahina Pardhan and Tarang Parekh and Eun-Kee Park and Charles D H Parry and Fatemeh Pashazadeh Kan and Jay Patel and Siddhartha Pati and George C Patton and Uttam Paudel and Shrikant Pawar and Amy E Peden and Ionela-Roxana Petcu and Michael R Phillips and Marina Pinheiro and Evgenii Plotnikov and Pranil Man Singh Pradhan and Akila Prashant and Jianchao Quan and Amir Radfar and Alireza Rafiei and Pankaja Raghav Raghav and Vafa Rahimi-Movaghar and Azizur Rahman and Md Mosfequr Rahman and Mosiur Rahman and Amir Masoud Rahmani and Shayan Rahmani and Chhabi Lal Ranabhat and Priyanga Ranasinghe and Chythra R Rao and Drona Prakash Rasali and Mohammad-Mahdi Rashidi and Zubair Ahmed Ratan and David Laith Rawaf and Salman Rawaf and Lal Rawal and Andre M N Renzaho and Negar Rezaei and Saeid Rezaei and Mohsen Rezaeian and Seyed Mohammad Riahi and Esperanza Romero-Rodríguez and Gregory A Roth and Godfrey M Rwegerera and Basema Saddik and Erfan Sadeghi and Reihaneh Sadeghian and Umar Saeed and Farhad Saeedi and Rajesh Sagar and Amirhossein Sahebkar and Harihar Sahoo and Mohammad Ali Sahraian and KM Saif-Ur-Rahman and Sarvenaz Salahi and Hamideh Salimzadeh and Abdallah M Samy and Francesco Sanmarchi and Milena M Santric-Milicevic and Yaser Sarikhani and Brijesh Sathian and Ganesh Kumar Saya and Mehdi Sayyah and Maria Inês Schmidt and Aletta Elisabeth Schutte and Michaël Schwarzinger and David C Schwebel and Abdul-Aziz Seidu and Nachimuthu Senthil Kumar and SeyedAhmad SeyedAlinaghi and Allen Seylani and Feng Sha and Sarvenaz Shahin and Fariba Shahraki-Sanavi and Shayan Shahrokhi and Masood Ali Shaikh and Elaheh Shaker and Murad Ziyaudinovich Shakhmardanov and Mehran Shams-Beyranvand and Sara Sheikhbahaei and Rahim Ali Sheikhi and Adithi Shetty and Jeevan K Shetty and Damtew Solomon Shiferaw and Mika Shigematsu and Rahman Shiri and Reza Shirkoohi and K M Shivakumar and Velizar Shivarov and Parnian Shobeiri and Roman Shrestha and Negussie Boti Sidemo and Inga Dora Sigfusdottir and Diego Augusto Santos Silva and Natacha Torres da Silva and Jasvinder A Singh and Surjit Singh and Valentin Yurievich Skryabin and Anna Aleksandrovna Skryabina and David A Sleet and Marco Solmi and YONATAN SOLOMON and Suhang Song and Yimeng Song and Reed J D Sorensen and Sergey Soshnikov and Ireneous N Soyiri and Dan J Stein and Sonu Hangma Subba and Miklós Szócska and Rafael Tabarés-Seisdedos and Takahiro Tabuchi and Majid Taheri and Ker-Kan Tan and Minale Tareke and Elvis Enowbeyang Tarkang and Gebremaryam Temesgen and Worku Animaw Temesgen and Mohamad-Hani Temsah and Kavumpurathu Raman Thankappan and Rekha Thapar and Nikhil Kenny Thomas and Chalachew Tiruneh and Jovana Todorovic and Marco Torrado and Mathilde Touvier and Marcos Roberto Tovani-Palone and Mai Thi Ngoc Tran and Sergi Trias-Llimós and Jaya Prasad Tripathy and Alireza Vakilian and Rohollah Valizadeh and Mehdi Varmaghani and Shoban Babu Varthya and Tommi Juhani Vasankari and Theo Vos and Birhanu Wagaye and Yasir Waheed and Mandaras Tariku Walde and Cong Wang and Yanzhong Wang and Yuan-Pang Wang and Ronny Westerman and Nuwan Darshana Wickramasinghe and Abate Dargie Wubetu and Suowen Xu and Kazumasa Yamagishi and Lin Yang and Gesila Endashaw E Yesera and Arzu Yigit and Vahit Yiğit and Ayenew Engida Ayenew Engida Yimaw and Dong Keon Yon and Naohiro Yonemoto and Chuanhua Yu and Siddhesh Zadey and Mazyar Zahir and Iman Zare and Mikhail Sergeevich Zastrozhin and Anasthasia Zastrozhina and Zhi-Jiang Zhang and Chenwen Zhong and Mohammad Zmaili and Yves Miel H Zuniga and Emmanuela Gakidou},
   doi = {10.1016/S0140-6736(22)00847-9},
   issn = {01406736},
   issue = {10347},
   journal = {The Lancet},
   month = {7},
   pages = {185-235},
   title = {Population-level risks of alcohol consumption by amount, geography, age, sex, and year: a systematic analysis for the Global Burden of Disease Study 2020},
   volume = {400},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0140673622008479},
   year = {2022},
}
@article{Manthey2022,
   author = {Jakob Manthey and Kevin Shield and Jürgen Rehm},
   doi = {10.1016/S0140-6736(22)02123-7},
   issn = {01406736},
   issue = {10365},
   journal = {The Lancet},
   month = {11},
   pages = {1764-1765},
   title = {Alcohol and health},
   volume = {400},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0140673622021237},
   year = {2022},
}
@article{fctLungCancerUS,
   abstract = {This study examines how associations between socioeconomic status (SES) and lung and pancreatic cancer mortality have changed over time in the U.S. The fundamental cause hypothesis predicts as diseases become more preventable due to innovation in medical knowledge or technology, individuals with greater access to resources will disproportionately benefit, triggering the formation or worsening of health disparities along social cleavages. We examine socioeconomic disparities in mortality due to lung cancer, a disease that became increasingly preventable with the development and dissemination of knowledge of the causal link between smoking cigarettes and lung cancer, and compare it to that of pancreatic cancer, a disease for which there have been no major prevention or treatment innovations. County-level disease-specific mortality rates for those ≥45 years, adjusted for sex, race, and age during 1968-2009 are derived from death certificate and population data from the National Center for Health Statistics. SES is measured using five county-level variables from four decennial censuses, interpolating values for intercensal years. Negative binomial regression was used to model mortality. Results suggest the impact of SES on lung cancer mortality increases 0.5% per year during this period. Although lung cancer mortality rates are initially higher in higher SES counties, by 1980 persons in lower SES counties are at greater risk and by 2009 the difference in mortality between counties with SES one SD above compared to one SD below average was 33 people per 100,000. In contrast, we find a small but significant reverse SES gradient in pancreatic cancer mortality that does not change over time. These data support the fundamental cause hypothesis: social conditions influencing access to resources more greatly impact mortality when preventative knowledge exists. Public health interventions and policies should facilitate more equitable distribution of new health-enhancing knowledge and faster uptake and utilization among lower SES groups. © 2013 Elsevier Ltd.},
   author = {Marcie S. Rubin and Sean Clouston and Bruce G. Link},
   doi = {10.1016/J.SOCSCIMED.2013.10.026},
   issn = {0277-9536},
   journal = {Social Science & Medicine},
   keywords = {Fundamental causes,Lung cancer mortality,Pancreatic cancer mortality,Socioeconomic status,United States},
   month = {1},
   pages = {54-61},
   pmid = {24444839},
   publisher = {Pergamon},
   title = {A fundamental cause approach to the study of disparities in lung cancer and pancreatic cancer mortality in the United States},
   volume = {100},
   year = {2014},
}
@generic{sipherIntro,
   abstract = {The conditions in which we are born, grow, live, work and age are key drivers of health and inequalities in life chances. To maximise health and wellbeing across the whole population, we need well-coordinated action across government sectors, in areas including economic, education, welfare, labour market and housing policy. Current research struggles to offer effective decision support on the cross-sector strategic alignment of policies, and to generate evidence that gives budget holders the confidencto change the way major investment decisions are made. This open letter introduces a new research initiative in this space. The SIPHER (Systems Science in Public Health and Health Economics Research) Consortium brings together a multi-disciplinary group of scientists from across six universities, three government partners at local, regional and national level, and ten practice partner organisations. The Consortium’s vision is a shift from health policy to healthy public policy, where the wellbeing impacts of policies are a core consideration across government sectors. Researchers policies are a core consideration across government sectors. Researchers and policy makers will jointly tackle fundamental questions about: a) the complex causal relationships between upstream policies and wellbeing, economic and equality outcomes; b) the multi-sectoral appraisal of costs and benefits of alternative investment options; c) public values and preferences for different outcomes, and how necessary trade-offs can be negotiated; and d) creating the conditions for intelligence-led adaptive policy design that maximises progress against economic, social and health goals. Whilst our methods will be adaptable across policy topics and jurisdictions, we will initially focus on four policy areas: Inclusive Economic Growth, Adverse Childhood Experiences, Mental Wellbeing and Housing.},
   author = {Petra Meier and Robin Purshouse and Marion Bain and Clare Bambra and Richard Bentall and Mark Birkin and John Brazier and Alan Brennan and Mark Bryan and Julian Cox and Greg Fell and Elizabeth Goyder and Alison Heppenstall and John Holmes and Ceri Hughes and Asif Ishaq and Visakan Kadirkamanathan and Nik Lomax and Ruth Lupton and Suzy Paisley and Katherine Smith and Ellen Stewart and Mark Strong and Elizabeth Such and Aki Tsuchiya and Craig Watkins},
   doi = {10.12688/wellcomeopenres.15534.1},
   issn = {2398502X},
   journal = {Wellcome Open Research},
   keywords = {Adverse Childhood Experiences,Complex systems,Economic evaluation,Health in All Policies,Housing,Inclusive Growth,Inequalities,Non-Communicable Disease,Prevention,Public Mental Health,Public Policy,Wellbeing},
   publisher = {F1000 Research Ltd},
   title = {The SIPHER consortium: Introducing the new UK hub for systems science in public health and health economic research},
   volume = {4},
   year = {2019},
}
@report{MBSSM,
   author = {Tuong Manh Vu and Charlotte Probst and Alexandra Nielsen and Hao Bai and Charlotte Buckley and Petra S Meier and Mark Strong and Alan Brennan and Robin C Purshouse},
   city = {Sheffield},
   institution = {University of Sheffield},
   journal = {Journal of Artificial Societies and Social Simulation},
   month = {6},
   title = {A Software Architecture for Mechanism-Based Social Systems Modelling in Agent-Based Simulation Models},
   year = {2020},
}
@article{,
   author = {Hamill L and N Gilbert},
   journal = {Journal of Artificial Societies and Social Simulation},
   title = {Simulating Large Social Networks In Agent-Based Models: A Social Circle Model},
   volume = {12},
   year = {2010},
}
@report{whoAlcohol,
   city = {Geneva},
   institution = {World Health Organisation},
   title = {Global status report on alcohol and health 2018},
   url = {http://apps.who.int/bookorders.},
   year = {2018},
}
@report{Probst2020,
   abstract = {Background Individuals with low socioeconomic status (SES) experience disproportionately greater alcohol-attributable health harm than individuals with high SES from similar or lower amounts of alcohol consumption. Our aim was to provide an update of the current evidence for the role of alcohol use and drinking patterns in socioeconomic inequalities in mortality, as well as the effect modification or interaction effects between SES and alcohol use, as two potential explanations of this so-called alcohol-harm paradox.},
   author = {Charlotte Probst and Carolin Kilian and Sherald Sanchez and Shannon Lange and Jürgen Rehm},
   title = {Articles The role of alcohol use and drinking patterns in socioeconomic inequalities in mortality: a systematic review},
   url = {www.thelancet.com/},
   year = {2020},
}
@report{Angus2015,
   author = {Colin Angus and Abdallah Ally and Tony Stone and Yang Meng and John Holmes and Daniel Hill-Mcmanus and Alan Brennan and Petra Meier},
   title = {Comparing the effects of different alcohol taxation and price policies on health inequalities: Technical Appendix},
   year = {2015},
}
@article{Yuan2007,
   abstract = {We study the organization and dynamics of growing directed networks. These networks are built by adding nodes successively in such a way that each new node has K directed links to the existing ones. The organization of a growing directed network is analyzed in terms of the number of 'descendants' of each node in the network. We show that the distribution P(S) of the size, S, of the descendant cluster is described genetically by a power-law, P(S) ∼ S -n, where the exponent η depends on the value of K as well as the strength of preferential attachment. We determine that, in the case of growing random directed networks without any preferential attachment, η is given by 1 +1/K. We also show that the Boolean dynamics of these networks is stable for any value of K. However, with a small fraction of reversal in the direction of the links, the dynamics of growing directed networks appears to operate on 'the edge of chaos' with a power-law distribution of the cycle lengths. We suggest that the growing directed network may serve as another paradigm for the emergence of the scale-free features in network organization and dynamics. © IOP Publishing Ltd and Deutsche Physikalische Gesellschaft.},
   author = {Baosheng Yuan and Bing Hong Wang},
   doi = {10.1088/1367-2630/9/8/282},
   issn = {13672630},
   journal = {New Journal of Physics},
   month = {8},
   title = {Growing directed networks: Organization and dynamics},
   volume = {9},
   year = {2007},
}
@article{Mackenbach2003,
   abstract = {Objectives. During the past decades a widening of the relative gap in death rates between upper and lower socioeconomic groups has been reported for several European countries. Although differential mortality decline for cardiovascular diseases has been suggested as an important contributory factor, it is not known what its quantitative contribution was, and to what extent other causes of death have contributed to the widening gap in total mortality. Methods. We collected data on mortality by educational level and occupational class among men and women from national longitudinal studies in Finland, Sweden, Norway, Denmark, England/Wales, and Italy (Turin), and analysed age-standardized death rates in two recent time periods (1981-1985 and 1991-1995), both total mortality and by cause of death. For simplicity, we report on inequalities in mortality between two broad socioeconomic groups (high and low educational level, non-manual and manual occupations). Results. Relative inequalities in total mortality have increased in all six countries, but absolute differences in total mortality were fairly stable, with the exception of Finland where an increase occurred. In most countries, mortality from cardiovascular diseases declined proportionally faster in the upper socioeconomic groups. The exception is Italy (Turin) where the reverse occurred. In all countries with the exception of Italy (Turin), changes in cardiovascular disease mortality contributed about half of the widening relative gap for total mortality. Other causes also made important contributions to the widening gap in total mortality. For these causes, widening inequalities were sometimes due to increasing mortality rates in the lower socioeconomic groups. We found rising rates of mortality from lung cancer, breast cancer, respiratory disease, gastrointestinal disease, and injuries among men and/or women in lower socioeconomic groups in several countries. Conclusions. Reducing socioeconomic inequalities in mortality in Western Europe critically depends upon speeding up mortality declines from cardiovascular diseases in lower socioeconomic groups, and countering mortality increases from several other causes of death in lower socioeconomic groups.},
   author = {Johan P. Mackenbach and Vivian Bos and Otto Andersen and Mario Cardano and Giuseppe Costa and Seeromanie Harding and Alison Reid and Örjan Hemström and Tapani Valkonen and Anton E. Kunst},
   doi = {10.1093/ije/dyg209},
   issn = {03005771},
   issue = {5},
   journal = {International Journal of Epidemiology},
   month = {10},
   pages = {830-837},
   pmid = {14559760},
   title = {Widening socioeconomic inequalities in mortality in six Western European countries},
   volume = {32},
   year = {2003},
}
@article{,
   abstract = {This paper investigates the feasibility of using Approximate Bayesian Computation (ABC) to calibrate and evaluate complex individual-based models (IBMs). As ABC evolves, various versions are emerging, but here we only explore the most accessible version, rejection-ABC. Rejection-ABC involves running models a large number of times, with parameters drawn randomly from their prior distributions, and then retaining the simulations closest to the observations. Although well-established in some fields, whether ABC will work with ecological IBMs is still uncertain.Rejection-ABC was applied to an existing 14-parameter earthworm energy budget IBM for which the available data consist of body mass growth and cocoon production in four experiments. ABC was able to narrow the posterior distributions of seven parameters, estimating credible intervals for each. ABC's accepted values produced slightly better fits than literature values do. The accuracy of the analysis was assessed using cross-validation and coverage, currently the best-available tests. Of the seven unnarrowed parameters, ABC revealed that three were correlated with other parameters, while the remaining four were found to be not estimable given the data available.It is often desirable to compare models to see whether all component modules are necessary. Here, we used ABC model selection to compare the full model with a simplified version which removed the earthworm's movement and much of the energy budget. We are able to show that inclusion of the energy budget is necessary for a good fit to the data. We show how our methodology can inform future modelling cycles, and briefly discuss how more advanced versions of ABC may be applicable to IBMs. We conclude that ABC has the potential to represent uncertainty in model structure, parameters and predictions, and to embed the often complex process of optimising an IBM's structure and parameters within an established statistical framework, thereby making the process more transparent and objective.},
   author = {Elske van der Vaart and Mark A. Beaumont and Alice S.A. Johnston and Richard M. Sibly},
   doi = {10.1016/j.ecolmodel.2015.05.020},
   issn = {03043800},
   journal = {Ecological Modelling},
   keywords = {Approximate Bayesian Computation,Individual-based models,Model selection,Parameter estimation,Population dynamics},
   month = {9},
   pages = {182-190},
   publisher = {Elsevier},
   title = {Calibration and evaluation of individual-based models using Approximate Bayesian Computation},
   volume = {312},
   year = {2015},
}
@article{unravellingAHP,
   abstract = {Background: There is consistent evidence that individuals in higher socioeconomic status groups are more likely to report exceeding recommended drinking limits, but those in lower socioeconomic status groups experience more alcohol-related harm. This has been called the 'alcohol harm paradox'. Such studies typically use standard cut-offs to define heavy drinking, which are exceeded by a large proportion of adults. Our study pools data from six years (2008-2013) of the population-based Health Survey for England to test whether the socioeconomic distribution of more extreme levels of drinking could help explain the paradox. Methods: The study included 51,498 adults from a representative sample of the adult population of England for a cross-sectional analysis of associations between socioeconomic status and self-reported drinking. Heavy weekly drinking was measured at four thresholds, ranging from 112 g+/168 g + (alcohol for women/men, or 14/21 UK standard units) to 680 g+/880 g + (or 85/110 UK standard units) per week. Heavy episodic drinking was also measured at four thresholds, from 48 g+/64 g + (or 6/8 UK standard units) to 192 g+/256 g + (or 24/32 UK standard units) in one day. Socioeconomic status indicators were equivalised household income, education, occupation and neighbourhood deprivation. Results: Lower socioeconomic status was associated with lower likelihoods of exceeding recommended limits for weekly and episodic drinking, and higher likelihoods of exceeding more extreme thresholds. For example, participants in routine or manual occupations had 0.65 (95 % CI 0.57-0.74) times the odds of exceeding the recommended weekly limit compared to those in 'higher managerial' occupations, and 2.15 (95 % CI 1.06-4.36) times the odds of exceeding the highest threshold. Similarly, participants in the lowest income quintile had 0.60 (95 % CI 0.52-0.69) times the odds of exceeding the recommended weekly limit when compared to the highest quintile, and 2.30 (95 % CI 1.28-4.13) times the odds of exceeding the highest threshold. Conclusions: Low socioeconomic status groups are more likely to drink at extreme levels, which may partially explain the alcohol harm paradox. Policies that address alcohol-related health inequalities need to consider extreme drinking levels in some sub-groups that may be associated with multiple markers of deprivation. This will require a more disaggregated understanding of drinking practices.},
   author = {Dan Lewer and Petra Meier and Emma Beard and Sadie Boniface and Eileen Kaner},
   doi = {10.1186/s12889-016-3265-9},
   issn = {14712458},
   issue = {1},
   journal = {BMC Public Health},
   month = {7},
   pmid = {27430342},
   publisher = {BioMed Central Ltd.},
   title = {Unravelling the alcohol harm paradox: A population-based study of social gradients across very heavy drinking thresholds},
   volume = {16},
   year = {2016},
}
@report{ahpWhatNext,
   author = {Berke Em and Tanski Se and Demidenko E and Alford-Teaster J and Shi X and Sargent Jd},
   journal = {Am J Public Health},
   pages = {1967-71},
   title = {Comment Alcohol retail density and demographic predictors of health disparities: a geographic analysis},
   volume = {100},
   url = {www.thelancet.com/public-health},
   year = {2020},
}
@report{Flor2020,
   abstract = {A commitment to timely monitoring of alcohol use and to a package of policies that address, among others, taxation, access to alcohol, and the availability of treatment of alcohol use disorders, are likely to achieve success in reducing the health and negative social effects of alcohol use.},
   author = {Luisa Socio Flor and Emmanuela Gakidou},
   journal = {www.thelancet.com/public-health},
   title = {Comment},
   volume = {5},
   url = {www.thelancet.com/public-health},
   year = {2020},
}
@article{,
   author = {Emil Øversveen and Håvard T Rydland and Clare Bambra and Terje A Eikemo},
   doi = {10.2307/48615019},
   issue = {2},
   journal = {Journal of Public Health},
   pages = {103-112},
   title = {Rethinking the relationship between socio-economic status and health},
   volume = {45},
   year = {2017},
}
@article{Katikireddi2017,
   abstract = {Background Alcohol-related mortality and morbidity are high in socioeconomically disadvantaged populations compared with individuals from advantaged areas. It is unclear if this increased harm reflects differences in alcohol consumption between these socioeconomic groups, reverse causation (ie, downward social selection for high-risk drinkers), or a greater risk of harm in individuals of low socioeconomic status compared with those of higher status after similar consumption. We aimed to investigate whether the harmful effects of alcohol differ by socioeconomic status, accounting for alcohol consumption and other health-related factors. Methods The Scottish Health Surveys are record-linked cross-sectional surveys representative of the adult population of Scotland. We obtained baseline demographics and data for alcohol consumption (units per week and binge drinking) from Scottish Health Surveys done in 1995, 1998, 2003, 2008, 2009, 2010, 2011, and 2012. We matched these data to records for deaths, admissions, and prescriptions. The primary outcome was alcohol-attributable admission or death. The relation between alcohol-attributable harm and socioeconomic status was investigated for four measures (education level, social class, household income, and area-based deprivation) using Cox proportional hazards models. The potential for alcohol consumption and other risk factors (including smoking and body-mass index [BMI]) mediating social patterning was explored in separate regression models. Reverse causation was tested by comparing change in area deprivation over time. Findings 50 236 participants (21 777 men and 28 459 women) were included in the analytical sample, with 429 986 person-years of follow-up. Low socioeconomic status was associated consistently with strikingly raised alcohol-attributable harms, including after adjustment for weekly consumption, binge drinking, BMI, and smoking. Evidence was noted of effect modification; for example, relative to light drinkers living in advantaged areas, the risk of alcohol-attributable admission or death for excessive drinkers was increased (hazard ratio 6·12, 95% CI 4·45–8·41 in advantaged areas; and 10·22, 7·73–13·53 in deprived areas). We found little support for reverse causation. Interpretation Disadvantaged social groups have greater alcohol-attributable harms compared with individuals from advantaged areas for given levels of alcohol consumption, even after accounting for different drinking patterns, obesity, and smoking status at the individual level. Funding Medical Research Council, NHS Research Scotland, Scottish Government Chief Scientist Office.},
   author = {Srinivasa Vittal Katikireddi and Elise Whitley and Jim Lewsey and Linsay Gray and Alastair H. Leyland},
   doi = {10.1016/S2468-2667(17)30078-6},
   issn = {24682667},
   issue = {6},
   journal = {The Lancet Public Health},
   month = {6},
   pages = {e267-e276},
   pmid = {28626829},
   publisher = {Elsevier Ltd},
   title = {Socioeconomic status as an effect modifier of alcohol consumption and harm: analysis of linked cohort data},
   volume = {2},
   year = {2017},
}
@article{Vu2020,
   abstract = {The generative approach to social science, in which agent-based simulations (or other complex systems models) are executed to reproduce a known social phenomenon, is an important tool for realist explanation. However, a generative model, when suitably calibrated and validated using empirical data, represents just one viable candidate set of entities and mechanisms. The model only partially addresses the needs of an abductive reasoning process - specifically it does not provide insight into other viable sets of entities or mechanisms nor suggests which of these are fundamentally constitutive for the phenomenon to exist. In this paper, we propose a new model discovery framework that more fully captures the needs of realist explanation. The framework exploits the implicit ontology of an existing human-built generative model to propose and test a plurality of new candidate model structures. Genetic programming is used to automate this search process. A multiobjective approach is used, which enables multiple perspectives on the value of any particular generative model - such as goodness of fit, parsimony, and interpretability - to be represented simultaneously. We demonstrate this new framework using a complex systems modeling case study of change and stasis in societal alcohol use patterns in the US over the period 1980-2010. The framework is successful in identifying three competing explanations of these alcohol use patterns, using novel integrations of social role theory not previously considered by the human modeler. Practitioners in complex systems modeling should use model discovery to improve the explanatory utility of the generative approach to realist social science.},
   author = {Tuong M. Vu and Charlotte Buckley and Hao Bai and Alexandra Nielsen and Charlotte Probst and Alan Brennan and Paul Shuper and Mark Strong and Robin C. Purshouse and Murari Andrea},
   doi = {10.1155/2020/8923197},
   issn = {10990526},
   journal = {Complexity},
   publisher = {Hindawi Limited},
   title = {Multiobjective Genetic Programming Can Improve the Explanatory Capabilities of Mechanism-Based Models of Social Systems},
   volume = {2020},
   year = {2020},
}
@article{Manzo2007,
   abstract = {The article proposes a critical analysis of the concept of « model » and of the place that it should hold in sociology. The first part discusses the possibility of, and the interest in conceiving theoretical modelling as a systematic construction of « generating models ». The second part deals with three various types of « models » ( « statistical models », « computational models », and « mathematical models » ), and assesses their respective capacities to implement a « generating model ». In this section, the article devotes a detailed attention to a recent form of « computational modelling » (the multi-agents systems) : the idea put forward is that these « models » constitute the most suitable tool for formalizing and studying « generating models ». By way of conclusion, the article outlines a possible way of integrating the four meanings of the concept of « model » mobilized in order to propose a coherent research framework able to support a « modelling-based sociology ».},
   author = {Gianluca Manzo},
   doi = {10.3917/anso.071.0013},
   issn = {00662399},
   issue = {1},
   journal = {Annee Sociologique},
   month = {3},
   pages = {13-61},
   title = {Progrès et « urgence » de la modélisation en sociologie. du concept de « modèle générateur » et de sa mise en œuvre},
   volume = {57},
   year = {2007},
}
@report{,
   author = {Daphné Giorgi and Sarah Kaakai and Vincent Lemaire},
   title = {Human population with swap},
}
@article{Vandenbroucke2016,
   abstract = {Causal inference based on a restricted version of the potential outcomes approach reasoning is assuming an increasingly prominent place in the teaching and practice of epidemiology. The proposed concepts and methods are useful for particular problems, but it would be of concern if the theory and practice of the complete field of epidemiology were to become restricted to this single approach to causal inference. Our concerns are that this theory restricts the questions that epidemiologists may ask and the study designs that they may consider. It also restricts the evidence thatmay be considered acceptable to assess causality, and thereby the evidence that may be considered acceptable for scientific and public health decision making. These restrictions are based on a particular conceptual framework for thinking about causality. In Section 1, we describe the characteristics of the restricted potential outcomes approach (RPOA) and show that there is a methodological movement which advocates these principles, not just for solving particular problems, but as ideals for which epidemiology as a whole should strive. In Section 2, we seek to show that the limitation of epidemiology to one particular view of the nature of causality is problematic. In Section 3, we argue that the RPOA is also problematic with regard to the assessment of causality. We argue that it threatens to restrict study design choice, to wrongly discredit the results of types of observational studies that have been very useful in the past and to damage the teaching of epidemiological reasoning. Finally, in Section 4 we set out what we regard as a more reasonable 'working hypothesis' as to the nature of causality and its assessment: pragmatic pluralism.},
   author = {Jan P. Vandenbroucke and Alex Broadbent and Neil Pearce},
   doi = {10.1093/ije/dyv341},
   issn = {14643685},
   issue = {6},
   journal = {International Journal of Epidemiology},
   month = {12},
   pages = {1776-1786},
   pmid = {26800751},
   publisher = {Oxford University Press},
   title = {Causality and causal inference in epidemiology: The need for a pluralistic approach},
   volume = {45},
   year = {2016},
}
@report{FCTorigin,
   abstract = {Journal of Health and Social Behavior 1995, (Extra Issue):80-94 Over the last several decades, epidemiological studies have been enormously successful in identifying risk factors for major diseases. However, most of this research has focused attention on risk factors that are relatively proximal causes of disease such as diet, cholesterol level, exercise and the like. We question the emphasis on such individually-based risk factors and argue that greater attention must be paid to basic social conditions if health reform is to have its maximum effect in the time ahead. There are two reasons for this claim. First we argue that individually-based risk factors must be contextualized, by examining what puts people at risk of risks, if we are to craft effective interventions and improve the nation's health. Second, we argue that social factors such as socioeconomic status and social support are likely 'fundamental causes" of disease that, because they embody access to important resources, affect multiple disease outcomes through multiple mechanisms, and consequently maintain an association with disease even when intervening mechanisms change. Without careful attention to these possibilities, we run the risk of imposing individually-based intervention strategies that are ineffective and of missing opportunities to adopt broad-based societal interventions that could produce substantial health benefits for our citizens. Epidemiology has been enormously successful in heightening public awareness of risk factors for disease. Research findings are frequently and prominently publicized in the mass media and in rapidly proliferating university-based health newsletters. Moreover, there is evidence that the message has been received and that many people have at least attempted to quit smoking, include more exercise in their daily routine, and implement a healthier diet. With few exceptions, however, the new findings generated within the field of epidemiology have focused on risk factors that are relatively proximate "causes" of disease, such as diet, cholesterol, hypertension, electromagnetic fields, lack of exercise, and so on. Social factors, which tend to be more distal causes of disease, have received far less attention. ' This focus on more proximate links in the causal chain may be viewed by many, not as a limitation or bias, but as the rightful progression of science from identifying correlations to understanding causal relationships (e.g., Potter 1992). In fact, some in the so-called "modem" school of epidemiology (e.g., Rothman 1986) have explicitly argued that social conditions such as socioeconomic status are mere proxies for true causes lying closer to disease in the causal chain. This focus on proximate risk factors, potentially controllable at the individual level, resonates with the value and belief systems of Western culture that emphasize both the ability of the individual to control his or her personal fate and the importance of doing so (Becker * We thank Bernice Pescosolido, Sharon Schwartz, and Sarah Rosenfield for helpful comments. This work was supported in part by NIMH grants MH46101 and MH13043.},
   author = {Bruce G Link and J O Phelan},
   institution = {Columbia University},
   title = {Social Conditions as Fundamental Causes of Disease},
   year = {1995},
}
@article{ahp2016,
   abstract = {Background: Internationally, studies show that similar levels of alcohol consumption in deprived communities (vs. more affluent) result in higher levels of alcohol-related ill health. Hypotheses to explain this alcohol harm paradox include deprived drinkers: Suffering greater combined health challenges (e.g. smoking, obesity) which exacerbate effects of alcohol harms; exhibiting more harmful consumption patterns (e.g. bingeing); having a history of more harmful consumption; and disproportionately under-reporting consumption. We use a bespoke national survey to assess each of these hypotheses. Methods: A national telephone survey designed to test this alcohol harm paradox was undertaken (May 2013 to April 2014) with English adults (n = 6015). Deprivation was assigned by area of residence. Questions examined factors including: Current and historic drinking patterns; combined health challenges (smoking, diet, exercise and body mass); and under-reported consumption (enhanced questioning on atypical/special occasion drinking). For each factor, analyses examined differences between deprived and more affluent individuals controlled for total alcohol consumption. Results: Independent of total consumption, deprived drinkers were more likely to smoke, be overweight and report poor diet and exercise. Consequently, deprived increased risk drinkers (male >168-400 g, female >112-280 g alcohol/week) were >10 times more likely than non-deprived counterparts to drink in a behavioural syndrome combining smoking, excess weight and poor diet/exercise. Differences by deprivation were significant but less marked in higher risk drinkers (male >400 g, female >280 g alcohol/week). Current binge drinking was associated with deprivation independently of total consumption and a history of bingeing was also associated with deprivation in lower and increased risk drinkers. Conclusions: Deprived increased/higher drinkers are more likely than affluent counterparts to consume alcohol as part of a suite of health challenging behaviours including smoking, excess weight and poor diet/exercise. Together these can have multiplicative effects on risks of wholly (e.g. alcoholic liver disease) and partly (e.g. cancers) alcohol-related conditions. More binge drinking in deprived individuals will also increase risks of injury and heart disease despite total alcohol consumption not differing from affluent counterparts. Public health messages on how smoking, poor diet/exercise and bingeing escalate health risks associated with alcohol are needed, especially in deprived communities, as their absence will contribute to health inequalities.},
   author = {Mark A. Bellis and Karen Hughes and James Nicholls and Nick Sheron and Ian Gilmore and Lisa Jones},
   doi = {10.1186/s12889-016-2766-x},
   issn = {14712458},
   issue = {1},
   journal = {BMC Public Health},
   keywords = {Alcohol,Binge,Deprivation,Disease,Inequalities,Injury},
   month = {2},
   pmid = {26888538},
   publisher = {BioMed Central Ltd.},
   title = {The alcohol harm paradox: Using a national survey to explore how alcohol may disproportionately impact health in deprived individuals},
   volume = {16},
   year = {2016},
}
@article{lancetAlcoholEditorial,
   journal = {The Lancet Public Health},
   month = {6},
   pages = {e297},
   title = {Failing to address the burden of alcohol},
   volume = {5},
   year = {2020},
}
@book{,
   abstract = {The Global Status Report on Alcohol 2004 is a new edition of the Report published by WHO in 1999. This edition provides an update of the role of alcohol in global health and contains data not found in the earlier edition. The Report seeks to document what is known about alcohol consumption and drinking patterns among various population groups, alcohol's impact on health worldwide and what is needed on a global basis to prevent and reduce alcohol-related injury and disease. For this new edition, more emphasis has been placed on enhancing the comparability of data by setting clear and comprehens. Preliminaries -- Foreword -- Acknowledgements -- Contents -- INTRODUCTION -- ALCOHOL POLICY BACKGROUND AND DEFINITION -- WHO GLOBAL ALCOHOL DATABASE -- DATA SOURCES AND METHODS -- REGIONAL OVERVIEWS OF DATA AVAILABILITY -- AREAS OF ALCOHOL POLICY -- 1 Definition of an alcoholic beverage -- 2 Restrictions on the availability of alcoholic beverages -- 3 Drink driving legislation -- 4 Price and taxation -- 5 Advertising and sponsorship -- 6 Alcohol free environments -- DISCUSSION -- COUNTRY PROFILES -- REFERENCES -- ANNEX 1 GLOBAL QUESTIONNAIRE ALCOHOL CONTROL POLICIES. ANNEX 2 LIST OF FOCAL POINTS FOR THE ALCOHOL POLICY QUESTIONNAIRE.},
   author = {World Health Organization. Substance Abuse Department.},
   isbn = {9241580356},
   pages = {209},
   publisher = {World Health Organization},
   title = {Global status report : alcohol policy},
   year = {2004},
}
@report{,
   author = {Storgatz S},
   city = {New York},
   institution = {Cornell University},
   month = {3},
   pages = {268-276},
   title = {Exploring Complex Networks},
   url = {www.nature.com},
   year = {2001},
}
@inproceedings{Purshouse2014,
   abstract = {This paper presents a new real-world application of evolutionary computation: identifying parameterisations of a theory-driven model that can reproduce alcohol consumption dynamics observed in a population over time. Population alcohol consumption is a complex system, with multiple interactions between economic and social factors and drinking behaviours, the nature and importance of which are not well-understood. Prediction of time trends in consumption is therefore difficult, but essential for robust estimation of future changes in health-related consequences of drinking and for appraising the impact of interventions aimed at changing alcohol use in society. The paper describes a microsimulation approach in which an attitude-behaviour model, Theory of Planned Behaviour, is used to describe the frequency of drinking by individuals. Consumption dynamics in the simulation are driven by changes in the social roles of individuals over time (parenthood, partnership, and paid labour). An evolutionary optimizer is used to identify parameterisations of the Theory that can describe the observed changes in drinking frequency. Niching is incorporated to enable multiple possible parameterisations to be identified, each of which can accurately recreate history but potentially encode quite different future trends. The approach is demonstrated using evidence from the 1979-1985 birth cohort in England between 2003 and 2010. © 2014 is held by the author/owner(s).},
   author = {Robin C. Purshouse and Abdallah K. Ally and Alan Brennan and Daniel Moyo and Paul Norman},
   doi = {10.1145/2576768.2598239},
   isbn = {9781450326629},
   journal = {GECCO 2014 - Proceedings of the 2014 Genetic and Evolutionary Computation Conference},
   keywords = {Genetic algorithms,Multiple solutions / niching,Simulation optimization,Social science},
   pages = {1159-1166},
   publisher = {Association for Computing Machinery},
   title = {Evolutionary parameter estimation for a theory of planned behaviour microsimulation of alcohol consumption dynamics in an English birth cohort 2003 to 2010},
   year = {2014},
}
@report{,
   abstract = {'Multiple causation' is the canon of contemporary epidemiology, and its metaphor and model is the 'web of causation.},
   author = {Nancy Krieger},
   isbn = {02779536(93},
   issue = {7},
   pages = {1994},
   title = {EPIDEMIOLOGY AND THE WEB OF CAUSATION: HAS ANYONE SEEN THE SPIDER?},
   volume = {39},
}
@generic{Boyd2022,
   abstract = {Background and Aims: The alcohol harm paradox (AHP) posits that disadvantaged groups suffer from higher rates of alcohol-related harm compared with advantaged groups, despite reporting similar or lower levels of consumption on average. The causes of this relationship remain unclear. This study aimed to identify explanations proposed for the AHP. Secondary aims were to review the existing evidence for those explanations and investigate whether authors linked explanations to one another. Methods: This was a systematic review. We searched MEDLINE (1946–January 2021), EMBASE (1974–January 2021) and PsycINFO (1967–January 2021), supplemented with manual searching of grey literature. Included papers either explored the causes of the AHP or investigated the relationship between alcohol consumption, alcohol-related harm and socio-economic position. Papers were set in Organization for Economic Cooperation and Development high-income countries. Explanations extracted for analysis could be evidenced in the empirical results or suggested by researchers in their narrative. Inductive thematic analysis was applied to group explanations. Results: Seventy-nine papers met the inclusion criteria and initial coding revealed that these papers contained 41 distinct explanations for the AHP. Following inductive thematic analysis, these explanations were grouped into 16 themes within six broad domains: individual, life-style, contextual, disadvantage, upstream and artefactual. Explanations related to risk behaviours, which fitted within the life-style domain, were the most frequently proposed (n = 51) and analysed (n = 21). Conclusions: While there are many potential explanations for the alcohol harm paradox, most research focuses on risk behaviours while other explanations lack empirical testing.},
   author = {Jennifer Boyd and Olivia Sexton and Colin Angus and Petra Meier and Robin C. Purshouse and John Holmes},
   doi = {10.1111/add.15567},
   issn = {13600443},
   issue = {1},
   journal = {Addiction},
   keywords = {Alcohol consumption,alcohol-related harm,causal mechanisms,disadvantage,health inequalities,morbidity,mortality,socio-economic position},
   month = {1},
   pages = {33-56},
   pmid = {33999487},
   publisher = {John Wiley and Sons Inc},
   title = {Causal mechanisms proposed for the alcohol harm paradox—a systematic review},
   volume = {117},
   year = {2022},
}
@article{Vichitkunakorn2019,
   abstract = {Background: To estimate and compare the socio-economic inequities in alcohol-related harms among households in Thailand between 2007 and 2017 adjusted for socioeconomic status with the proportions of current and binge drinkers in each household. Methods: A secondary data analysis of the 2007 and 2017 National Cigarette and Alcohol Consumption Survey was conducted. The unit of analysis was household-level. Concentration index (CI) was used to measure household income-based inequalities in alcohol-related harms (i.e., workplace, domestic, non-domestic, financial, and drinking-and-driving) in the previous 12 months. Results: Based on data from two waves of survey (n = 66,776 in 2007 and 39,630 in 2017), the prevalence of households that had at least one member who had an alcohol-related harm event was 21.8% and 26.2% in 2007 and 2017, respectively. The highest prevalence was the drinking-and-driving domain (about 20%). The prevalence increased between 2007 and 2017 with an annual rate of change ranged from 1.2 to 4.4%. All of the CI values were negative for both survey waves, except the drink-and-driving domain in 2007. The CI values for all domains in 2017 had a larger magnitude than in 2007, except the domestic domain. For any alcohol-related harm, the CI value was not significant at +0.002 (Standard error [SE] 0.004) in 2007, but significant at -0.014 (SE 0.004) in 2017. So, the index changed around -0.016. Conclusions: The poor households had a slightly greater tendency to incur harms from alcohol and there existed more inequality in the prevalence of harms in 2017 compared with 2007.},
   author = {Polathep Vichitkunakorn and Sawitri Assanangkornchai},
   doi = {10.1016/j.drugalcdep.2019.107577},
   issn = {18790046},
   journal = {Drug and Alcohol Dependence},
   keywords = {Alcohol-related harm,Household,Inequity,Thailand},
   month = {11},
   pmid = {31568937},
   publisher = {Elsevier Ireland Ltd},
   title = {Trends in inequalities of alcohol-related harms among Thai households: 2007-2017},
   volume = {204},
   year = {2019},
}
@article{Atkinson2018,
   abstract = {Objectives Alcohol misuse is a complex systemic problem. The aim of this study was to explore the feasibility of using a transparent and participatory agent-based modelling approach to develop a robust decision support tool to test alcohol policy scenarios before they are implemented in the real world. Methods A consortium of Australia's leading alcohol experts was engaged to collaboratively develop an agent-based model of alcohol consumption behaviour and related harms. As a case study, four policy scenarios were examined. Results A 19.5 ± 2.5% reduction in acute alcohol-related harms was estimated with the implementation of a 3 a.m. licensed venue closing time plus 1 a.m. lockout; and a 9 ± 2.6% reduction in incidence was estimated with expansion of treatment services to reach 20% of heavy drinkers. Combining the two scenarios produced a 33.3 ± 2.7% reduction in the incidence of acute alcohol-related harms, suggesting a synergistic effect. Conclusions This study demonstrates the feasibility of participatory development of a contextually relevant computer simulation model of alcohol-related harms and highlights the value of the approach in identifying potential policy responses that best leverage limited resources. The alcohol modelling consortium members are listed in Acknowledgements.},
   author = {Jo-An Atkinson and Dylan Knowles and John Wiggers and Michael Livingston and Robin Room and Ante Prodan and Geoff McDonnell and Jo-An Atkinson Jo-AnAtkinson},
   doi = {10.1007/s00038-017-1041-y},
   journal = {International Journal of Public Health},
   keywords = {Agent-based modelling,Alcohol-related harm,Evidence synthesis,Prevention policy},
   pages = {537-546},
   title = {Harnessing advances in computer simulation to inform policy and planning to reduce alcohol-related harms},
   volume = {63},
   url = {https://doi.org/10.1007/s00038-017-1041-y},
   year = {2018},
}
@article{Atkinson2017,
   abstract = {Development of effective policy responses to address complex public health problems can be challenged by a lack of clarity about the interaction of risk factors driving the problem, differing views of stakeholders on the most appropriate and effective intervention approaches, a lack of evidence to support commonly implemented and acceptable intervention approaches, and a lack of acceptance of effective interventions. Consequently, political considerations, community advocacy and industry lobbying can contribute to a hotly contested debate about the most appropriate course of action; this can hinder consensus and give rise to policy resistance. The problem of alcohol misuse and its associated harms in New South Wales (NSW), Australia, provides a relevant example of such challenges. Dynamic simulation modelling is increasingly being valued by the health sector as a robust tool to support decision making to address complex problems. It allows policy makers to ask 'what-if' questions and test the potential impacts of different policy scenarios over time, before solutions are implemented in the real world. Participatory approaches to modelling enable researchers, policy makers, program planners, practitioners and consumer representatives to collaborate with expert modellers to ensure that models are transparent, incorporate diverse evidence and perspectives, are better aligned to the decision-support needs of policy makers, and can facilitate consensus building for action. This paper outlines a procedure for embedding stakeholder engagement and consensus building in the development of dynamic simulation models that can guide the development of effective, coordinated and acceptable policy responses to complex public health problems, such as alcohol-related harms in NSW. Key points • Effective policy responses to complex public health problems are challenged by uncertainty around the most effective intervention combinations, political considerations, community advocacy, and lack of consensus on a course of action • Simulation models are 'what-if' tools for testing the impacts of alternative policy scenarios before implementing solutions in the real world • This paper outlines a procedure for embedding stakeholder engagement and consensus building in the development of simulation models 2},
   author = {Jo-An Atkinson and Eloise O'donnell and John Wiggers and Geoff Mcdonnell and Jo Mitchell and Louise Freebairn and Devon Indig and Lucie Rychetnik},
   doi = {10.17061/phrp2711707},
   issue = {1},
   keywords = {Biotext},
   pages = {2711707},
   title = {Dynamic simulation modelling of policy responses to reduce alcohol-related harms: rationale and procedure for a participatory approach},
   volume = {27},
   year = {2017},
}
@article{Nygaard2009,
   author = {Peter Nygaard},
   doi = {10.3109/16066350109141751},
   title = {Intervention in Social Networks: A New Method in the Prevention of Alcohol-Related Problems},
   url = {https://doi.org/10.3109/16066350109141751},
   year = {2009},
}
@article{Nygaard2009,
   author = {Peter Nygaard},
   doi = {10.3109/16066350109141751},
   title = {Intervention in Social Networks: A New Method in the Prevention of Alcohol-Related Problems},
   url = {https://doi.org/10.3109/16066350109141751},
   year = {2009},
}
@article{Knai2018,
   abstract = {The extent to which government should partner with business interests such as the alcohol, food, and other industries in order to improve public health is a subject of ongoing debate. A common approach involves developing voluntary agreements with industry or allowing them to self-regulate. In England, the most recent example of this was the Public Health Responsibility Deal (RD), a public-private partnership launched in 2011 under the then Conservative-led coalition government. The RD was organised around a series of voluntary agreements that aim to bring together government, academic experts, and commercial, public sector and voluntary organisations to commit to pledges to undertake actions of public health benefit. This paper brings together the main findings and implications of the evaluation of the RD using a systems approach. We analysed the functioning of the RD exploring the causal pathways involved and how they helped or hindered the RD; the structures and processes; feedback loops and how they might have constrained or potentiated the effects of the RD; and how resilient the wider systems were to change (i.e., the alcohol, food, and other systems interacted with). Both the production and uptake of pledges by RD partners were largely driven by the interests of partners themselves, enabling these wider systems to resist change. This analysis demonstrates how and why the RD did not meet its objectives. The findings have lessons for the development of effective alcohol, food and other policies, for defining the role of unhealthy commodity industries, and for understanding the limits of industry self-regulation as a public health measure.},
   author = {Cécile Knai and Mark Petticrew and Nick Douglas and Mary Alison Durand and Elizabeth Eastmure and Ellen Nolte and Nicholas Mays},
   doi = {10.3390/ijerph15122895},
   keywords = {alcohol,food,physical activity,public-private partnership,systems approach,systems thinking,workplace health},
   title = {The Public Health Responsibility Deal: Using a Systems-Level Analysis to Understand the Lack of Impact on Alcohol, Food, Physical Activity, and Workplace Health Sub-Systems},
   url = {www.mdpi.com/journal/ijerph},
   year = {2018},
}
@article{UMLABM,
   abstract = {Although the majority of researchers interested in ABM increasingly agree that the most natural way to program their models is to adopt OO practices, UML diagrams are still largely absent from their publications. In the last 15 years, the use of UML has risen constantly, to the point where UML has become the de facto standard for graphical visualization of software development. UML and its 13diagrams has many universally accepted virtues. Most importantly, UML provides a level of abstraction higher than that offered by OO programming languages (Java, C++, Python, .Net ...). This abstraction layer encourages researchers to spend more time on modelling rather than on programming. This paper initially presents the four most common UML diagrams - class, sequence, state and activity diagrams (based on my personal experience, these are the most useful diagrams for ABM development). The most important features of these diagrams are discussed, and explanations based on conceptual pieces often found in ABM models are given of how best to use the diagrams. Subsequently, some very well known and classical ABM models such as the Schelling segregation model, the spatial evolutionary game, and a continuous double action free market are subjected to more detailed UML analysis},
   author = {Bersini H},
   journal = {Journal of Artificial Societies and Social Simulation},
   month = {1},
   title = {UML FOR ABM},
   url = {http://jasss.soc.surrey.ac.uk/15/1/9.html},
   year = {2012},
}
@generic{scopingReview,
   abstract = {Background and Aims: A complex systems perspective has been advocated to explore multi-faceted factors influencing public health issues, including alcohol consumption and associated harms. This scoping review aimed to identify studies that applied a complex systems perspective to alcohol consumption and the prevention of alcohol-related harms in order to summarize their characteristics and identify evidence gaps. Methods: Studies published between January 2000 and September 2020 in English were located by searching for terms synonymous with ‘complex systems’ and ‘alcohol’ in the Scopus, MEDLINE, Web of Science and Embase databases, and through handsearching and reference screening of included studies. Data were extracted on each study's aim, country, population, alcohol topic, system levels, funding, theory, methods, data sources, time-frames, system modifications and type of findings produced. Results: Eighty-seven individual studies and three systematic reviews were identified, the majority of which were conducted in the United States or Australia in the general population, university students or adolescents. Studies explored types and patterns of consumption behaviour and the local environments in which alcohol is consumed. Most studies focused on individual and local interactions and influences, with fewer examples exploring the relationships between these and regional, national and international subsystems. The body of literature is methodologically diverse and includes theory-led approaches, dynamic simulation models and social network analyses. The systematic reviews focused on primary network studies. Conclusions: The use of a complex systems perspective has provided a variety of ways of conceptualizing and analyzing alcohol use and harm prevention efforts, but its focus ultimately has remained on predominantly individual- and/or local-level systems. A complex systems perspective represents an opportunity to address this gap by also considering the vertical dimensions that constrain, shape and influence alcohol consumption and related harms, but the literature to date has not fully captured this potential.},
   author = {Elizabeth McGill and Mark Petticrew and Dalya Marks and Michael McGrath and Chiara Rinaldi and Matt Egan},
   doi = {10.1111/add.15341},
   issn = {13600443},
   issue = {9},
   journal = {Addiction},
   keywords = {Alcohol consumption,alcohol harms,complex systems,dynamic simulation modelling,prevention,scoping review,social network analyses},
   month = {9},
   pages = {2260-2288},
   pmid = {33220118},
   publisher = {John Wiley and Sons Inc},
   title = {Applying a complex systems perspective to alcohol consumption and the prevention of alcohol-related harms in the 21st century: a scoping review},
   volume = {116},
   year = {2021},
}
@article{GaelaS,
   abstract = {Identifying biological and behavioural causes of diseases has been one of the central concerns of epidemiology for the past half century. This has led to the development of increasingly sophisticated conceptual and analytical approaches focused on the isolation of single causes of disease states. However, the growing recognition that (i) factors at multiple levels, including biological, behavioural and group levels may influence health and disease, and (ii) that the interrelation among these factors often includes dynamic feedback and changes over time challenges this dominant epidemiological paradigm. Using obesity as an example, we discuss how the adoption of complex systems dynamic models allows us to take into account the causes of disease at multiple levels, reciprocal relations and interrelation between causes that characterize the causation of obesity. We also discuss some of the key difficulties that the discipline faces in incorporating these methods into non-infectious disease epidemiology. We conclude with a discussion of a potential way forward. The hunt for causes in epidemiology Epidemiology is a practical discipline, ultimately concerned with identifying modifiable causes of disease and dysfunction. As such, it cannot escape issues related to how one conceptualizes causality and finds causes. The understanding of causality is, of course, not simple, and we should be mindful that whether it is in the social or biological sciences, there may be no real 'gold standard' for doing so. 1 As epidemiology has developed as a discipline in the last quarter century, the quest for isolating 'causes' has emerged as one of the central foci in the field. 2 The sufficient-component causal model 3 served as an early organizing heuristic, and more recently, the counterfactual paradigm 4 has allowed epidemiologists to cast other concerns that are central to the field, particularly issues of confounding, within a causal framework. Indeed, important epidemiological developments , both conceptual and methodological, have grown directly out of this emerging clarity about the nature of causes, including, for example, directed acyclic graphs 5 and marginal structural models. 6 These developments have provided support for the focus on isolating factors, primarily behavioural or biological, that we may term causes of a particular disease state. In many respects, this has been a successful enterprise. For example, epidemiological studies demonstrating strong causal connections between smoking and lung cancer and asbestos exposure and mesothelioma have strengthened our resolve that we can discover causes of disease states. A greater sophistication in our ability to isolate these independent factors, and to identify which ones of them do indeed cause disease, increases the usefulness of epidemiology that ultimately has always conceived of itself as a 'pragmatic science'. 2 In this article we discuss both some of the conceptual challenges that are inherent in our current},
   author = {Sandro Galea and Matthew Riddle and George A Kaplan},
   doi = {10.1093/ije/dyp296},
   journal = {International Journal of Epidemiology},
   keywords = {Agent-based modelling,dynamic systems modelling,epidemiology,regression},
   pages = {97-106},
   title = {Causal thinking and complex system approaches in epidemiology},
   volume = {39},
   url = {https://academic.oup.com/ije/article/39/1/97/712819},
   year = {2010},
}
@report{,
   title = {Using Agent-based Modelling to understand the causes of the Alcohol Harm Paradox: A Scottish case study investigating Fundamental Cause Theory: ODD Protocol Overview},
}
@generic{Boyd2021,
   abstract = {There are large socioeconomic inequalities in alcohol-related harm. The alcohol harm paradox (AHP) is the consistent finding that lower socioeconomic groups consume the same or less as higher socioeconomic groups yet experience greater rates of harm. To date, alcohol researchers have predominantly taken an individualised behavioural approach to understand the AHP. This paper calls for a new approach which draws on theories of health inequality, specifically the social determinants of health, fundamental cause theory, political economy of health and eco-social models. These theories consist of several interwoven causal mechanisms, including genetic inheritance, the role of social networks, the unequal availability of wealth and other resources, the psychosocial experience of lower socioeconomic position, and the accumulation of these experiences over time. To date, research exploring the causes of the AHP has often lacked clear theoretical underpinning. Drawing on these theoretical approaches in alcohol research would not only address this gap but would also result in a structured effort to identify the causes of the AHP. Given the present lack of clear evidence in favour of any specific theory, it is difficult to conclude whether one theory should take primacy in future research efforts. However, drawing on any of these theories would shift how we think about the causes of the paradox, from health behaviour in isolation to the wider context of complex interacting mechanisms between individuals and their environment. Meanwhile, computer simulations have the potential to test the competing theoretical perspectives, both in the abstract and empirically via synthesis of the disparate existing evidence base. Overall, making greater use of existing theoretical frameworks in alcohol epidemiology would offer novel insights into the AHP and generate knowledge of how to intervene to mitigate inequalities in alcohol-related harm.},
   author = {Jennifer Boyd and Clare Bambra and Robin C. Purshouse and John Holmes},
   doi = {10.3390/ijerph18116025},
   issn = {16604601},
   issue = {11},
   journal = {International Journal of Environmental Research and Public Health},
   keywords = {Alcohol,Alcohol-related harm,Health inequality,Social determinants,Socioeconomic position},
   month = {6},
   pmid = {34205125},
   publisher = {MDPI},
   title = {Beyond behaviour: How health inequality theory can enhance our understanding of the ‘alcohol-harm paradox’},
   volume = {18},
   year = {2021},
}
@article{predpreyFluctuations,
   abstract = {We present a dynamical model for the price evolution of financial assets. The model is based on a two-level approach: In the first stage, one finds an agent-based model that describes the current state of investors’ beliefs, perspectives or strategies. The dynamics is inspired by a model for describing predator–prey population evolution: Agents change their mind through self- or mutual interaction, and the decision is adopted on a random basis, with no direct influence of the price itself. One of the most appealing properties of such a system is the presence of large oscillations in the number of agents sharing the same perspective, what may be linked with the existence of bullish and bearish periods in financial markets. In the second stage, one has the pricing mechanism, which will be driven by the relative population in the different groups of investors. The price equation will depend on the specific nature of the species, and thus, it may change from one market to the other: We will present a simple model of excess demand in the first place and then consider a more elaborate liquidity model. The outcomes of both models are analyzed and compared.},
   author = {Miquel Montero},
   doi = {10.1007/s11403-020-00284-4},
   issn = {18607128},
   issue = {1},
   journal = {Journal of Economic Interaction and Coordination},
   keywords = {Interacting agent models,Models of financial markets,Stochastic processes},
   month = {1},
   pages = {29-57},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Predator–prey model for stock market fluctuations},
   volume = {16},
   year = {2021},
}
@article{predpreyTrade,
   abstract = {We examine the implications of trade in an economy with two interrelated natural resources, focusing on the case of a simple predator–prey relationship. We derive a three-sector general equilibrium model where production functions are linked via the ecological dynamics of the natural system. Under autarky, this economy exhibits a steady-state equilibrium that overexploits the prey stock, reducing the linked predator population and overall welfare in the absence of harvesting controls. When two economies engage in trade, differences in the dynamics of the two resource systems can become the basis for comparative advantage. In this case, the predator–prey relationship leads to a source of comparative advantage in harvesting prey for a country with a lower autarky steady-state proportion of predators to prey. This feature has not been noticed in the literature and leads to a counterintuitive implication: free trade can help conserve predator and prey stocks in the country with the higher autarkic steady-state proportion of predators to prey. To illustrate the relevance of our analytic findings, we present the stylized empirical example of the effect of Chinook salmon imports on killer whale populations.},
   author = {Eric C. Edwards and Dong Hun Go and Reza Oladi},
   doi = {10.1016/j.reseneeco.2020.101174},
   issn = {09287655},
   journal = {Resource and Energy Economics},
   keywords = {Bi-resource economy,Conservation,General equilibrium,Predator–prey,Trade},
   month = {8},
   publisher = {Elsevier B.V.},
   title = {Predator–prey dynamics in general equilibrium and the role of trade},
   volume = {61},
   year = {2020},
}
@web_page{githubRepo,
   author = {A Clarke},
   month = {12},
   title = {rsaihe/foodchain: A predator-prey simulation programmed with the p5.js library.},
   url = {https://github.com/rsaihe/foodchain},
   year = {2020},
}
@article{originalVolterra,
   author = {V Volterra},
   doi = {118558a0},
   journal = {Nature},
   month = {9},
   title = {Fluctuations in the Abundance of a Species considered Mathematically},
   year = {1926},
}
@article{Lotka1920,
   author = {Alfred J. Lotka},
   doi = {10.1073/pnas.6.7.410},
   issn = {0027-8424},
   issue = {7},
   journal = {Proceedings of the National Academy of Sciences},
   month = {7},
   pages = {410-415},
   title = {Analytical Note on Certain Rhythmic Relations in Organic Systems},
   volume = {6},
   year = {1920},
}
@book{economicsNaturalResource,
   author = {John M. Hartwick and Nancy D. Olewiler},
   edition = {2},
   month = {1},
   publisher = {Pearson College Div},
   title = {The Economics of Natural Resource Use},
   year = {1997},
}
@article{Callahan2022,
   abstract = {<p>Quantifying which nations are culpable for the economic impacts of anthropogenic warming is central to informing climate litigation and restitution claims for climate damages. However, for countries seeking legal redress, the magnitude of economic losses from warming attributable to individual emitters is not known, undermining their standing for climate liability claims. Uncertainties compound at each step from emissions to global greenhouse gas (GHG) concentrations, GHG concentrations to global temperature changes, global temperature changes to country-level temperature changes, and country-level temperature changes to economic losses, providing emitters with plausible deniability for damage claims. Here we lift that veil of deniability, combining historical data with climate models of varying complexity in an integrated framework to quantify each nation’s culpability for historical temperature-driven income changes in every other country. We find that the top five emitters (the United States, China, Russia, Brazil, and India) have collectively caused US$6 trillion in income losses from warming since 1990, comparable to 14% of annual global gross domestic product; many other countries are responsible for billions in losses. Yet the distribution of warming impacts from emitters is highly unequal: high-income, high-emitting countries have benefited themselves while harming low-income, low-emitting countries, emphasizing the inequities embedded in the causes and consequences of historical warming. By linking individual emitters to country-level income losses from warming, our results provide critical insight into climate liability and national accountability for climate policy.</p>},
   author = {Christopher W. Callahan and Justin S. Mankin},
   doi = {10.1007/s10584-022-03387-y},
   issn = {0165-0009},
   issue = {3-4},
   journal = {Climatic Change},
   month = {6},
   pages = {40},
   title = {National attribution of historical climate damages},
   volume = {172},
   url = {https://link.springer.com/10.1007/s10584-022-03387-y},
   year = {2022},
}
@article{Koh2016,
   abstract = {Purpose: Drawing on the systems theory and the natural resource-based view, the purpose of this paper is to advance an integrated resource efficiency view (IREV) and derive a composite “integrated resource efficiency index” (IRE-index) for assessing the environmental, economic, and social resource efficiencies of production economies. Design/methodology/approach: Using sub-national input-output data, the IRE-index builds on the human development index (HDI) and the OECD green growth indicators by including functions for environmental resource efficiency, energy, and material productivity. The study uses multiple regressions to examine and compare the IRE-index of 40 countries, including 34 OECD nations. The study further compares the IRE-index to similar composite indicators such as the human sustainable development index (HSDI) and the ecological footprint. Findings: The IRE-index reveals a discrepancy between social development and resource efficiency in many of the world’s wealthiest production economies. Findings also show that material productivity has been the key driver for observed improvements in IRE over time. The index is a robust macro-level methodology for assessing resource efficiency and sustainability, with implications for production operations in global supply chains. Originality/value: The IREV and IRE-index both contribute towards advancing green supply chain management and sustainability, and country-level resource efficiency accounting and reporting. The IRE-index is a useful composite for capturing aggregate environmental, economic, and social resource efficiencies of production economies. The paper clearly outlines the managerial, academic, and policy implications of the IREV and resulting index.},
   author = {S. C.L. Koh and Jonathan Morris and Seyed Mohammad Ebrahimi and Raymond Obayi},
   doi = {10.1108/IJOPM-05-2015-0266},
   issn = {17586593},
   issue = {11},
   journal = {International Journal of Operations and Production Management},
   keywords = {Performance measurement,Supply chain management,Sustainability},
   pages = {1576-1600},
   publisher = {Emerald Group Publishing Ltd.},
   title = {Integrated resource efficiency: measurement and management},
   volume = {36},
   year = {2016},
}
@report{Miller1996,
   abstract = {A model of learning and adaptation is used to analyze the coevolution of strategies in the repeated Prisoner's Dilemma game under both perfect and imperfect reporting. Meta-players submit finite automata strategies and update their choices through an explicit evolutionary process modeled by a genetic algorithm. Using this framework, adaptive strategic choice and the emergence of cooperation are studied through 'computational experiments.' The results of the analyses indicate that information conditions lead to significant differences among the evolving strategies. Furthermore, they suggest that the general methodology may have much wider applicability to the analysis of adaptation in economic and social systems.},
   author = {John H Miller},
   journal = {Journal of Economic Behavior and Organization &},
   keywords = {C70,D80,Evolution,Genetic algorithms,JEL clussijicution: C63,Learning,Ll3 Keywords: Adaptation,Machine learning,Repeated prisoner's dilemma game},
   pages = {87-112},
   title = {The coevolution of automata in the repeated prisoner' s dilemma},
   volume = {29},
   year = {1996},
}
@report{Miller1996,
   abstract = {A model of learning and adaptation is used to analyze the coevolution of strategies in the repeated Prisoner's Dilemma game under both perfect and imperfect reporting. Meta-players submit finite automata strategies and update their choices through an explicit evolutionary process modeled by a genetic algorithm. Using this framework, adaptive strategic choice and the emergence of cooperation are studied through 'computational experiments.' The results of the analyses indicate that information conditions lead to significant differences among the evolving strategies. Furthermore, they suggest that the general methodology may have much wider applicability to the analysis of adaptation in economic and social systems.},
   author = {John H Miller},
   journal = {Journal of Economic Behavior and Organization &},
   keywords = {C70,D80,Evolution,Genetic algorithms,JEL clussijicution: C63,Learning,Ll3 Keywords: Adaptation,Machine learning,Repeated prisoner's dilemma game},
   pages = {87-112},
   title = {The coevolution of automata in the repeated prisoner' s dilemma},
   volume = {29},
   year = {1996},
}
@article{Ghisellini2016,
   abstract = {In the last few years Circular Economy (CE) is receiving increasing attention worldwide as a way to overcome the current production and consumption model based on continuous growth and increasing resource throughput. By promoting the adoption of closing-the-loop production patterns within an economic system CE aims to increase the efficiency of resource use, with special focus on urban and industrial waste, to achieve a better balance and harmony between economy, environment and society. This study provides an extensive review of the literature of last two decades, with the purpose of grasping the main CE features and perspectives: origins, basic principles, advantages and disadvantages, modelling and implementation of CE at the different levels (micro, meso and macro) worldwide. Results evidence that CE origins are mainly rooted in ecological and environmental economics and industrial ecology. In China CE is promoted as a top-down national political objective while in other areas and countries as European Union, Japan and USA it is a tool to design bottom-up environmental and waste management policies. The ultimate goal of promoting CE is the decoupling of environmental pressure from economic growth. The implementation of CE worldwide still seems in the early stages, mainly focused on recycle rather than reuse. Important results have been achieved in some activity sectors (e.g. in waste management, where large waste recycling rates are achieved in selected developed countries). CE implies the adoption of cleaner production patterns at company level, an increase of producers and consumers responsibility and awareness, the use of renewable technologies and materials (wherever possible) as well as the adoption of suitable, clear and stable policies and tools. The lesson learned from successful experiences is that the transition towards CE comes from the involvement of all actors of the society and their capacity to link and create suitable collaboration and exchange patterns. Success stories also point out the need for an economic return on investment, in order to provide suitable motivation to companies and investors. In summary, the CE transition has just started. Moreover, the interdisciplinary framework underpinning CE offers good prospects for gradual improvement of the present production and consumption models, no longer adequate because of their environmental load and social inequity, a clear indicator of resource use inefficiency.},
   author = {Patrizia Ghisellini and Catia Cialani and Sergio Ulgiati},
   doi = {10.1016/j.jclepro.2015.09.007},
   issn = {09596526},
   journal = {Journal of Cleaner Production},
   keywords = {Circular economy,Recycling,Resource efficiency,Reuse,Sustainability,Zero waste},
   month = {2},
   pages = {11-32},
   publisher = {Elsevier Ltd},
   title = {A review on circular economy: The expected transition to a balanced interplay of environmental and economic systems},
   volume = {114},
   year = {2016},
}
@article{Reveliotis2017,
   abstract = {The problem addressed in this document concerns the coordinated allocation of a finite set of reUSAble resources to a set of concurrently running processes. These processes execute in a staged manner, and each stage requires a different subset of the system resources for its support. Furthermore, processes will hold upon the resources currently allocated to them until they will secure the necessary resources for their next processing stage. Such resource allocation dynamics currently arise in the context of many flexibly automated operations: from the workflow that takes place in various production shop floors and certain internet-supported platforms that seek to automate various service operations; to the traffic coordination in guidepath-based transport systems like industrial monorail and urban railway systems; to the resource allocation that takes place in the context of the contemporary multi-core computer architectures. From a theoretical standpoint, the resource allocation problems that are abstracted from the aforementioned applications, correspond to the problem of scheduling a stochastic network with blocking and deadlocking effects. This is an area of the modern scheduling theory with very limited results. To a large extent, this lack of results is due to the intricacies that arise from the blocking, and especially the deadlocking effects that take place in these networks, and prevents a tractable analysis of these problems through the classical modeling frameworks. Hence, the departing thesis of the work that is presented in this document, is the decomposition of the aforementioned scheduling problems to (i) a supervisory control problem that will seek to prevent the deadlock formation in the underlying resource allocation dynamics, and (ii) a scheduling problem that will be formulated on the admissible subspace to be defined by the adopted supervisory control policy. Each of these two subproblems can be further structured and addressed using some formal modeling frameworks borrowed, respectively, from the qualitative and the quantitative theory of Discrete Event Systems. At the same time, the above two subproblems possess considerable special structure that can be leveraged towards their effective and efficient solution. The presented material provides a comprehensive tutorial exposition of the current achievements of the corresponding research community with respect to the first of the two subproblems mentioned above. As it will be revealed by this exposition, the corresponding results are pretty rich in their theoretical developments and practically potent. At the same time, it is expected and hoped that the resulting awareness regarding the aforementioned results will also set the stage for undertaking a more orchestrated effort on the second of the two subproblems mentioned above.},
   author = {Spyros Reveliotis},
   doi = {10.1561/2600000010},
   issn = {23256826},
   issue = {1-2},
   journal = {Foundations and Trends in Systems and Control},
   pages = {1-223},
   publisher = {Now Publishers Inc},
   title = {Logical control of complex resource allocation systems},
   volume = {4},
   year = {2017},
}
@article{Recio2022,
   abstract = {<p>The modern economy is both a complex self-organizing system and an innovative, evolving one. Contemporary theory, however, treats it essentially as a static equilibrium system. Here we propose a formal framework to capture its complex, evolving nature. We develop an agent-based model of an economic system in which firms interact with each other and with consumers through market transactions. Production functions are represented by a pair of von Neumann technology matrices, and firms implement production plans taking into account current price levels for their inputs and output. Prices are determined by the relation between aggregate demand and supply. In the absence of exogenous perturbations the system fluctuates around its equilibrium state. New firms are introduced when profits are above normal, and are ultimately eliminated when losses persist. The varying number of firms represents a recurrent perturbation. The system thus exhibits dynamics at two levels: the dynamics of prices and output, and the dynamics of system size. The model aims to be realistic in its fundamental structure, but is kept simple in order to be computationally efficient. The ultimate aim is to use it as a platform for modeling the structural evolution of an economic system. Currently the model includes one form of structural evolution, the ability to generate new technologies and new products.</p>},
   author = {Gustavo Recio and Wolfgang Banzhaf and Roger White},
   doi = {10.1162/artl_a_00365},
   issn = {1064-5462},
   issue = {1},
   journal = {Artificial Life},
   month = {6},
   pages = {58-95},
   title = {From Dynamics to Novelty: An Agent-Based Model of the Economic System},
   volume = {28},
   year = {2022},
}
@report{Lindgren1991,
   author = {Kristian Lindgren},
   title = {Evolutionary Phenomena in Simple Dynamics ODYCCEUS-Opinion Dynamics and Cultural Conflict in European Spaces View project},
   url = {https://www.researchgate.net/publication/258883366},
   year = {1991},
}
@report{,
   title = {QUARTERLY JOURNAL OF},
}
@article{,
   title = {An evolutionary theory of economic change},
}
@report{Durlauf1997,
   author = {Steven N Durlauf},
   title = {The Economy as an Evolving Complex System II},
   url = {https://www.researchgate.net/publication/237357182},
   year = {1997},
}
@article{Edwards2020,
   abstract = {We examine the implications of trade in an economy with two interrelated natural resources, focusing on the case of a simple predator–prey relationship. We derive a three-sector general equilibrium model where production functions are linked via the ecological dynamics of the natural system. Under autarky, this economy exhibits a steady-state equilibrium that overexploits the prey stock, reducing the linked predator population and overall welfare in the absence of harvesting controls. When two economies engage in trade, differences in the dynamics of the two resource systems can become the basis for comparative advantage. In this case, the predator–prey relationship leads to a source of comparative advantage in harvesting prey for a country with a lower autarky steady-state proportion of predators to prey. This feature has not been noticed in the literature and leads to a counterintuitive implication: free trade can help conserve predator and prey stocks in the country with the higher autarkic steady-state proportion of predators to prey. To illustrate the relevance of our analytic findings, we present the stylized empirical example of the effect of Chinook salmon imports on killer whale populations.},
   author = {Eric C. Edwards and Dong Hun Go and Reza Oladi},
   doi = {10.1016/j.reseneeco.2020.101174},
   issn = {09287655},
   journal = {Resource and Energy Economics},
   keywords = {Bi-resource economy,Conservation,General equilibrium,Predator–prey,Trade},
   month = {8},
   publisher = {Elsevier B.V.},
   title = {Predator–prey dynamics in general equilibrium and the role of trade},
   volume = {61},
   year = {2020},
}
@report{Rosser1992,
   abstract = {The economic and ecologic theories of evolution have developed by means of a mutual dialogue and interaction from the early nineteenth century to the present, despite certain fundamental differences between them. This'mutual influence has involved the rise of the equilibrium concept as well as the appearance of cyclical and chaotic dynamic models. Both theories have faced a conflict between gradualistic and punctuationist (saltationalist) approaches. Both have emphasized the emergence of higher-order self-reproducing structures, a process of hypercyclic morphogenesis uhimately encouraged by the dialogue itself.},
   author = {J Barkiey Rosser},
   journal = {Journal of Economic Behavior and Organization},
   pages = {195-215},
   title = {The dialogue between the economic and the ecologic theories of evolution},
   volume = {17},
   year = {1992},
}
@book{Rosser2000,
   abstract = {From Catastrophe to Chaos: A General Theory of Economic Discontinuities presents and unusual perspective on economics and economic analysis. Current economic theory largely depends upon assuming that the world is fundamentally continuous. However, an increasing amount of economic research has been done using approaches that allow for discontinuities such as catastrophe theory, chaos theory, synergetics, and fractal geometry. The spread of such approaches across a variety of disciplines of thought has constituted a virtual intellectual revolution in recent years. This book reviews the applications of these approaches in various subdisciplines of economics and draws upon past economic thinkers to develop an integrated view of economics as a whole from the perspective of inherent discontinuity.},
   author = {J. Barkley Rosser},
   doi = {10.1007/978-94-017-1613-0},
   journal = {From Catastrophe to Chaos: A General Theory of Economic Discontinuities},
   publisher = {Springer Netherlands},
   title = {From Catastrophe to Chaos: A General Theory of Economic Discontinuities},
   year = {2000},
}
@report{,
   author = {John B Robinson},
   title = {Modelling The Interactions Between Human and Natural Systems},
   url = {https://www.researchgate.net/publication/235948482},
}
@report{,
   abstract = {/ During the last 20 years our understanding of the development of complex systems has changed significantly. Two major advancements are catastrophe theory and nonequilibrium thermodynamics with its associated theory of self-organization. These theories indicate that complex system development is nonlinear, discontinuous (catas-trophes), not predictable (bifurcations), and multivalued (mul-tiple developmental pathways). Ecosystem development should be expected to exhibit these characteristics. Traditional ecological theory has attempted to describe ecosystem stress response using some simple notions such as stability and resiliency, In fact, stress-response must be characterized by a richer set of concepts. The ability of the system to maintain its current operating point in the face of the stress, must be ascertained. If the system changes operating points, there are several questions to be considered: Is the change along the original developmental pathway or a new one? Is the change organizing or disorganizing? Will the system return to its original state? Will the system flip to some new state in a catastrophic way? Is the change acceptable to humans? The integrity of an ecosystem does not reflect a single characteristic of an ecosystem. The concept of integrity must be seen as multidimensional and encompassing a rich set of ecosystem behaviors. A framework of concepts for discussing integrity is presented in this articte.},
   author = {lAMES J Kay},
   title = {PROFILE A Nonequilibrium Thermodynamic Discussing Ecosystem Integrity Framework for},
}
@report{Holland1991,
   author = {John H Holland and John H Miller},
   issue = {2},
   pages = {365-370},
   publisher = {American Economic Association},
   title = {Artificial Adaptive Agents in Economic Theory},
   volume = {81},
   year = {1991},
}
@report{Rastetter1992,
   author = {Edward B Rastetter and Anthony W King and Bernard J Cosby and George M Hornberger and Robert V O'neill and John E Hobbie},
   issue = {1},
   journal = {Source: Ecological Applications},
   pages = {55-70},
   title = {Aggregating Fine-Scale Ecological Knowledge to Model Coarser-Scale Attributes of},
   volume = {2},
   year = {1992},
}
@report{Wroblewski1989,
   abstract = {Al>stract-Within the past decade there have been several multidisciplinary research programs designed to study interactions among physical, biological and chemical processes in U.S. continental shelf waters. The data sets resulting from these oceanographic studies have provided a basis for the formulation of interdisciplinary models. This overview focuses on those models that investigate the transport of biogenic material between estuarine, nearshore, shelf, slope and oceanic regimes. We summarize the physical mechanisms controlling fluxes of nutrients and plankton between the continental margin and the open ocean. Finally, we discuss how physical-biological modeling of coastal-offshore exchange processes can contribute to global biogeochemical studies ',~t are planned for the coming decade. CONTENTS},
   author = {J S Wroblewski and Eileen E Hofmann},
   pages = {65-99},
   title = {U. S. interdisciplinary modeling studies of coastal-offshore exchange processes: Past and future},
   volume = {23},
   year = {1989},
}
@report{,
   abstract = {The book concludes with three appendices which present some special aspects of similarity transformations for matrices with multiple eigenvalues, systemization of transformations to canonical forms, and the use of truncated polynomials in state-space analysis. The referencing for these appendices, like that for the text proper, is superb. Overall, the translation of the book is quite good, although slightly awkward in some places. The latter, however, is not a deterrent to the text's readability.},
   author = {James S Meditch and Colin W Clark John Wiley},
   isbn = {129.49.23.145},
   title = {The Optimal Management of Renewable Resources. By},
   url = {http://www.siam.org/journals/ojsa.php},
}
@report{Costanza1990,
   author = {Robert Costanza and Fred H Sklar and Mary L White},
   issue = {2},
   pages = {91-107},
   title = {Modeling Coastal Landscape Dynamics},
   volume = {40},
   year = {1990},
}
@article{,
   abstract = {Technological evolution has been compared to biological evolution by many authors over the last two centuries. As a parallel experiment of innovation involving economic, historical, and social components, artifacts define a universe of evolving properties that displays episodes of diversification and extinction. Here, we critically review previous work comparing the two types of evolution. Like biological evolution, technological evolution is driven by descent with variation and selection, and includes tinkering, convergence, and contingency. At the same time, there are essential differences that make the two types of evolution quite distinct. Major distinctions are illustrated by current specific examples, including the evolution of cornets and the historical dynamics of information technologies. Due to their fast and rich development, the later provide a unique opportunity to study technological evolution at all scales with unprecedented resolution. Despite the presence of patterns suggesting convergent trends between man-made systems end biological ones, they provide examples of planned design that have no equivalent with natural evolution. Copyright © 2013 Wiley Periodicals, Inc.},
   author = {Ricard V. Solée and Sergi Valverde and Marti Rosas Casals and Stuart A. Kauffman and Doyne Farmer and Niles Eldredge},
   doi = {10.1002/cplx.21436},
   issn = {10990526},
   issue = {4},
   journal = {Complexity},
   keywords = {Convergence,Culturomics,Evolution,Information technology,Technology,Tinkering},
   pages = {15-27},
   publisher = {John Wiley and Sons Inc.},
   title = {The evolutionary ecology of technological innovations},
   volume = {18},
   year = {2013},
}
@report{,
   author = {Franz Barachini and Christian Stary},
   title = {From Digital Twins to Digital Selves and Beyond Engineering and Social Models for a Trans-humanist World},
}
@article{Farmer2015,
   abstract = {Modelling the economics of climate change is daunting. Many existing methodologies from social and physical sciences need to be deployed, and new modelling techniques and ideas still need to be developed. Existing bread-and-butter micro- and macroeconomic tools, such as the expected utility framework, market equilibrium concepts and representative agent assumptions, are far from adequate. Four key issues—along with several others—remain inadequately addressed by economic models of climate change, namely: (1) uncertainty, (2) aggregation, heterogeneity and distributional implications (3) technological change, and most of all, (4) realistic damage functions for the economic impact of the physical consequences of climate change. This paper assesses the main shortcomings of two generations of climate-energy-economic models and proposes that a new wave of models need to be developed to tackle these four challenges. This paper then examines two potential candidate approaches—dynamic stochastic general equilibrium (DSGE) models and agent-based models (ABM). The successful use of agent-based models in other areas, such as in modelling the financial system, housing markets and technological progress suggests its potential applicability to better modelling the economics of climate change.},
   author = {J. Doyne Farmer and Cameron Hepburn and Penny Mealy and Alexander Teytelboym},
   doi = {10.1007/s10640-015-9965-2},
   issn = {15731502},
   issue = {2},
   journal = {Environmental and Resource Economics},
   keywords = {Agent based models,Climate change,DSGE models,Damage function,Heterogeneity,Integrated assessment models,Technological innovation,Uncertainty},
   month = {10},
   pages = {329-357},
   publisher = {Kluwer Academic Publishers},
   title = {A Third Wave in the Economics of Climate Change},
   volume = {62},
   year = {2015},
}
@report{,
   title = {How to Improve the Financial Architecture and Its Resilience Financial Resilience Survey: selected answers},
   url = {http://ssrn.com/abstract=2449874},
}
@report{Bagley1989,
   abstract = {During the evolution of many systems found in nature, both the system composition and the interactions between components will vary. Equating the dimension with the number of different components, a system which adds or deletes components belongs to a class of dynamical systems with a finite dimensional phase space of variable dimension. We present two models of biochemical systems with a variable phase space, a model of autocatalytic reaction networks in the prebiotic soup and a model of the idiotypie network of the immune system. Each model contains characteristic recta-dynamical rules for constructing equations of motion from component properties. The simulation of each model occurs on two levels. On one level, the equations of motion are integrated to determine the state of each component. On a second level, algorithms which approximate physical processes in the real system are employed to change the equations of motion. Models with meta-dynamical rules possess several advantages for the study of evolving systems. First, there are no explicit fitness functions to determine how the components of the model rank in terms of survivability. The success of any component is a function of its relationship to the res~ of the system. A second advantage is that since the phase space representation of the system is always :~inite but continually changing, we can explore a potentially infinite phase space which would otherwise be inaccessible with finite computer resources. Third, the enlarged capacity of systems with meta-dynamics for variation allows us to conduct true evolution experiments. The modeling methods presented here can be applied to many real biological systems. In the two studies we present, we are investigating two apparent properties of adaptive networks. With the simulation oi~ the prebiotic soup, we are most interested in how a chemical reaction network might emerge from an initial state of rehtive disorder. With the study of the immune system, we study the self-regulation of the network including its ability to distinguish between species which are part of the network and those which are not.},
   author = {R J Bagley and J D Farmer and S A Kauffman and N H Packard and A S Perelson and I M Stadnyk},
   journal = {BioSystems},
   keywords = {Adaptation,Evolution,Kswo~s: Complex sy,,~tems,Modeling,Networks,Self-organization},
   pages = {113-138},
   title = {Modeling adaptive biological systems},
   volume = {23},
   year = {1989},
}
@report{Farmer2000,
   abstract = {Markets have internal dynamics leading to excess volatility and other phenomena that are difficult to explain using rational expectations models. This paper studies these using a nonequilibrium price formation rule, developed in the context of trading with market orders. Because this is so much simpler than a standard inter-temporal equilibrium model, it is possible to study multi-period markets analytically. There price dynamics have second order oscillatory terms. Value investing does not necessarily cause prices to track values. Trend following causes short term trends in prices, but also causes longer-term oscillations. When value investing and trend following are combined, even though there is little linear structure, there can be boom-bust cycles, excess and temporally correlated volatility, and fat tails in price fluctuations. The long term evolution of markets can be studied in terms of flows of money. Profits can be decomposed in terms of aggregate pairwise correlations. Under reinvestment of profits this leads to a capital allocation model that is equivalent to a standard model in population biology. An investigation of market efficiency shows that patterns created by trend followers are more resistant to efficiency than those created by value investors, and that profit maximizing behavior slows the progression to efficiency. Order of magnitude estimates suggest that the timescale for efficiency is years to decades 2 .},
   author = {J Doyne Farmer},
   title = {Market force, ecology, and evolution},
   year = {2000},
}
@article{,
   abstract = {Technological improvement is the most important cause of long-term economic growth. In standard growth models, technology is treated in the aggregate, but an economy can also be viewed as a network in which producers buy goods, convert them to new goods, and sell the production to households or other producers. We develop predictions for how this network amplifies the effects of technological improvements as they propagate along chains of production, showing that longer production chains for an industry bias it toward faster price reduction and that longer production chains for a country bias it toward faster growth. These predictions are in good agreement with data from the World Input Output Database and improve with the passage of time. The results show that production chains play a major role in shaping the long-term evolution of prices, output growth, and structural change. production networks | technology change | economic growth | price evolution},
   author = {James Mcnerney and Charles Savoie and Francesco Caravelli and Vasco M Carvalho and J Doyne Farmer},
   doi = {10.1073/pnas.2106031118/-/DCSupplemental},
   title = {How production networks amplify economic growth},
   url = {https://doi.org/10.1073/pnas.2106031118},
}
@article{,
   abstract = {The potential impact of automation on the labour market is a topic that has generated significant interest and concern amongst scholars, policymakers and the broader public. A number of studies have estimated occupation-specific risk profiles by examining how suitable associated skills and tasks are for automation. However, little work has sought to take a more holistic view on the process of labour reallocation and how employment prospects are impacted as displaced workers transition into new jobs. In this article, we develop a data-driven model to analyse how workers move through an empirically derived occupational mobility network in response to automation scenarios. At a macro level, our model reproduces the Beveridge curve, a key stylized fact in the labour market. At a micro level, our model provides occupation-specific estimates of changes in short and long-term unemployment corresponding to specific automation shocks. We find that the network structure plays an important role in determining unemployment levels, with occupations in particular areas of the network having few job transition opportunities. In an automation scenario where low wage occupations are more likely to be automated than high wage occupations, the network effects are also more likely to increase the long-term unemployment of low-wage occupations.},
   author = {R. Maria Del Rio-Chanona and Penny Mealy and Mariano Beguerisse-Diáz and François Lafond and J. Doyne Farmer},
   doi = {10.1098/rsif.2020.0898},
   issn = {17425662},
   issue = {174},
   journal = {Journal of the Royal Society Interface},
   keywords = {agent-based model,automation,labour market,networks,unemployment},
   month = {1},
   pmid = {33468022},
   publisher = {Royal Society Publishing},
   title = {Occupational mobility and automation: A data-driven network model: Occupational mobility and automation: A data-driven network model},
   volume = {18},
   year = {2021},
}
@report{originalHolling,
   abstract = {Ecosystem change has usefully been seen as controlled by two functions: exploitation, where rapid colonization of recently disturbed land is emphasized, and conservation where slow accumulation and storage of energy and material is emphasized. Analysis of a series of ecosystems-managed and unmanaged-indicates there are two additional functions. One is that of creative destruction where the tightly bound accumulation of biomass and nutrients is suddenly released by agents such as forest fires, insect pests, or intense grazing. The second function is one of renewal where released material is mobilized to become available for the next exploitive phase. That pattern is discontinuous and is dependent on the existence of multistable states. Resilience and recovery is emphasized during the release and renewal sequence and stability and productivity during the exploitation and conservation sequence. Such studies of resilience are beginning to combine with hierarchy theory and with the theory of dissipative structures to deepen our understanding of change and how to manage change.},
   author = {C S Holling},
   journal = {European Journal of Operational Research},
   keywords = {Control,agriculture and food,differential equations,ecology,simulation,systems},
   pages = {139-146},
   title = {Simplifying the complex: The paradigms of ecological function and structure},
   volume = {30},
   year = {1987},
}
@article{modellingEco,
   abstract = {great review of almost all problems of modeling complex systems, and need to link economic and ecological systems when modeling},
   author = {Robert Costanza and Lisa Wainger and Carl Folke},
   doi = {10.2307/1311949},
   issn = {00063568},
   issue = {8},
   journal = {BioScience},
   month = {9},
   pages = {545-555},
   publisher = {Oxford University Press (OUP)},
   title = {Modeling Complex Ecological Economic Systems},
   volume = {43},
   year = {1993},
}
@report{Bicket2021,
   author = {Martha Bicket and Brian Castellani and Corrina Elsenbroich and Nigel Gilbert and Dione Hills and Mark Gurney Frances Rowe and Helen Wilkinson},
   title = {CECAN (2021) The Complexity Evaluation Toolkit},
   url = {www.cecan.ac.uk},
   year = {2021},
}
@book_section{Vitousek2008,
   abstract = {Human alteration of Earth is substantial and growing. Between one-third and one-half of the land surface has been transformed by human action; the carbon dioxide concentration in the atmosphere has increased by nearly 30 percent since the beginning of the Industrial Revolution; more atmospheric nitrogen is fixed by humanity than by all natural terrestrial sources combined; more than half of all accessible surface fresh water is put to use by humanity; and about one-quarter of the bird species on Earth have been driven to extinction. By these and other standards, it is clear that we live on a human-dominated planet. © 2008 Springer Science+Business Media, LLC.},
   author = {Peter M. Vitousek and Harold A. Mooney and Jane Lubchenco and Jerry M. Melillo},
   doi = {10.1007/978-0-387-73412-5_1},
   isbn = {9780387734118},
   journal = {Urban Ecology: An International Perspective on the Interaction Between Humans and Nature},
   keywords = {carbon cycle,extinction,global change,human domination,land cover change,nitrogen cycle},
   pages = {3-13},
   publisher = {Springer US},
   title = {Human domination of Earth's ecosystems},
   year = {2008},
}
@article{Hickel2020,
   abstract = {When the Human Development Index (HDI) was introduced in the 1990s, it was an important step toward a more sensible measure of progress, one defined less by GDP growth and more by social goals. But the limitations of HDI have become clear in the 21st century, given a growing crisis of climate change and ecological breakdown. HDI pays no attention to ecology, and retains an emphasis on high levels of income that – given strong correlations between income and ecological impact – violates sustainability principles. The countries that score highest on the HDI also contribute most, in per capita terms, to climate change and other forms of ecological breakdown. In this sense, HDI promotes a model of development that is empirically incompatible with ecological stability, and impossible to universalize. In this paper I propose an alternative index that corrects for these problems: the Sustainable Development Index (SDI). The SDI retains the base formula of the HDI but places a sufficiency threshold on per capita income, and divides by two key indicators of ecological impact: CO2 emissions and material footprint, both calculated in per capita consumption-based terms and rendered vis-à-vis planetary boundaries. The SDI is an indicator of strong sustainability that measures nations’ ecological efficiency in delivering human development.},
   author = {Jason Hickel},
   doi = {10.1016/j.ecolecon.2019.05.011},
   issn = {09218009},
   journal = {Ecological Economics},
   month = {1},
   publisher = {Elsevier B.V.},
   title = {The sustainable development index: Measuring the ecological efficiency of human development in the anthropocene},
   volume = {167},
   year = {2020},
}
@report{,
   author = {D Rapport and J Turner},
   doi = {10.1126/science.195.4276.367},
   issue = {9},
   journal = {J. Fac. Sci. Hok-kaido Univ},
   pages = {38-42},
   publisher = {Dover},
   title = {Economic Models in Ecology},
   volume = {2},
   url = {https://www.science.org},
}
@article{Crocker1992,
   abstract = {This paper incorporates an ecosystem model into a model of a simple economy. The decisionmaking agents in the ecosystem are individual organisms aggregated to the species level. A species may provide utility directly to humans, or it may provide utility indirectly because it is used either as a raw material in goods fabrication or as sustenance for other species. We describe a comparative static equilibrium of the ecosystem where species' demands for other species are equal to the supplies of those other species, and energy is conserved. The ecosystem is then embedded in the economy so that the effects of human intervention can be traced through both the ecosystem and the economy. Human intervention creates ecosystem externalities such that ecosystem equilibria are shifted and the new equilibria affect the utility or the production processes of other humans. This framework allows us to describe in principle which ecosystem services can be efficiently usurped by humans, which waste flows can be efficiently allowed into ecosystems, and which ecosystem organisms and physical attributes can be efficiently maintained. © 1992 Kluwer Academic Publishers.},
   author = {Thomas D. Crocker and John Tschirhart},
   doi = {10.1007/BF00330283},
   issn = {09246460},
   issue = {6},
   journal = {Environmental & Resource Economics},
   keywords = {Ecosystems,economies,externalities,sustainable development},
   month = {11},
   pages = {551-567},
   publisher = {Kluwer Academic Publishers},
   title = {Ecosystems, externalities, and economies},
   volume = {2},
   year = {1992},
}
@report{Bockstael1995,
   abstract = {We are attempting to integrate ecological and economic modeling and analysis in order to improve our understanding of regional systems, assess potential future impacts of various land-use, development, and agricultural policy options, and to better assess the value of ecological systems. Starting with an existing spatially articulated ecosystem model of the Patuxent River drainage basin in Maryland, we are adding modules to endogenize the agricultural components of the system (especially the impacts of agricultural practices and crop choice) and the process of land-use decision making. The integrated model will allow us to evaluate the indirect effects over long time horizons of current policy options. These effects are almost always ignored in partial analyses, although they may be very significant and may reverse many long-held assumptions and policy predictions. This paper is a progress report on this modeling effort, indicating our motivations, ideas, and plans for completion.},
   author = {N Bockstael and R Costanza and I Strand and W Boynton and K Bell and L Wainger},
   journal = {Ecological Economics},
   keywords = {Ecological-economic modeling,Ecosystem models,Land use,Patuxent River basin,Spatial modeling},
   pages = {143-159},
   title = {ECOLOGICAL ECONOMICS Analysis Ecological economic modeling and valuation of ecosystems},
   volume = {14},
   year = {1995},
}
@article{Eichner2005,
   abstract = {A dynamic general equilibrium model is developed for analyzing economy-ecosystem interactions. It includes stock-flow relations of natural resources, species populations and pollution, and it provides a microfoundation of the growth of populations. Humans and (other) species compete for natural resources and (prey) biomass. Resource stocks, pollution and populations determine public ecosystem services which in turn affect all agents in both the ecosystem and the economy. We establish a benchmark market system encompassing Lindahl markets for ecosystem services and emission markets. We also analyze a system where those two types of markets break down (laissez-faire) and propose efficiency restoring policies. © Springer-Verlag 2005.},
   author = {Thomas Eichner and Rüdiger Pethig},
   doi = {10.1007/s00712-005-0135-9},
   issn = {09318658},
   issue = {3},
   journal = {Journal of Economics/ Zeitschrift fur Nationalokonomie},
   keywords = {Biomass,Ecosystem,Natural resources,Pollution,Species},
   month = {9},
   pages = {213-249},
   title = {Ecosystem and economy: An integrated dynamic general equilibrium approach},
   volume = {85},
   year = {2005},
}
@article{Hardt2017,
   abstract = {Our society faces a dilemma. While continued economic growth is ecologically unsustainable, low or negative rates of economic growth are accompanied by adverse social impacts. Hence there is a need for macroeconomic tools that can help identify socially sustainable post-growth pathways. The emerging field of ecological macroeconomics aims to address this need and features a number of new macroeconomic modelling approaches. This article provides (1) a review of modelling developments in ecological macroeconomics, based on the literature and interviews with researchers, and (2) an analysis of how the different models incorporate policy themes from the post-growth literature. Twenty-two ecological macroeconomic models were analysed and compared to eight policy themes. It was found that environmental interactions and the monetary system were treated most comprehensively. Themes of income inequality, work patterns, indicators of well-being, and disaggregated production were addressed with less detail, while alternative business models and cross-scale interactions were hardly addressed. Overall, the combination of input-output analysis with stock-flow consistent modelling was identified as a promising avenue for developing macroeconomic models for a post-growth economy. However, due to the wide interpretation of what “the economy” entails, future research will benefit from employing a range of approaches.},
   author = {Lukas Hardt and Daniel W. O'Neill},
   doi = {10.1016/j.ecolecon.2016.12.027},
   issn = {09218009},
   journal = {Ecological Economics},
   keywords = {Degrowth,Ecological macroeconomics,Models,Policies,Post-growth},
   month = {4},
   pages = {198-211},
   publisher = {Elsevier B.V.},
   title = {Ecological Macroeconomic Models: Assessing Current Developments},
   volume = {134},
   year = {2017},
}
@generic{Cosme2017,
   abstract = {Debates on ecological and social limits to economic growth, and new ways to deal with resource scarcity without compromising human well-being, have re-emerged in the last few years. Central to many of these is a call for a degrowth approach. In this paper, a framework is developed to support a systematic analysis of degrowth in the academic literature. This article attempts to present a clearer notion of what the academic degrowth literature explores by identifying, organising, and analysing a set of proposals for action retrieved from a selection of articles. The framework is applied to classify proposals according to their alignment to ecological economics policy objectives (sustainable scale, fair distribution, and efficient allocation), type of approach (top-down versus bottom-up), and geographical focus (local, national, or international). A total of 128 peer-reviewed articles focused on degrowth were reviewed, and 54 that include proposals for action were analysed. The proposals identified align with three broad goals: (1) Reduce the environmental impact of human activities; (2) Redistribute income and wealth both within and between countries; and (3) Promote the transition from a materialistic to a convivial and participatory society. The findings indicate that the majority of degrowth proposals are national top-down approaches, focusing on government as a major driver of change, rather than local bottom-up approaches, as advocated by many degrowth proponents. The most emphasised aspects in the degrowth literature are related to social equity, closely followed by environmental sustainability. Topics such as population growth and the implications of degrowth for developing nations are largely neglected, and represent an important area for future research. Moreover, there is a need for a deeper analysis of how degrowth proposals would act in combination.},
   author = {Inês Cosme and Rui Santos and Daniel W. O'Neill},
   doi = {10.1016/j.jclepro.2017.02.016},
   issn = {09596526},
   journal = {Journal of Cleaner Production},
   keywords = {Bottom-up,Degrowth,Fair distribution,Policy,Sustainable scale,Top-down},
   month = {4},
   pages = {321-334},
   publisher = {Elsevier Ltd},
   title = {Assessing the degrowth discourse: A review and analysis of academic degrowth policy proposals},
   volume = {149},
   year = {2017},
}
@article{Rammel2007,
   abstract = {The overexploitation of natural resources and the increasing number of social conflicts following from their unsustainable use point to a wide gap between the objectives of sustainability and current resource management practices. One of the reasons for the difficulties to close this gap is that for evolving complex systems like natural and socio-economic systems, sustainability cannot be a static objective. Instead sustainable development is an open evolutionary process of improving the management of social-ecological systems, through better understanding and knowledge. Therefore, natural resource management systems need to be able to deal with different temporal, spatial and social scales, nested hierarchies, irreducible uncertainty, multidimensional interactions and emergent properties. The co-evolutionary perspective outlined in this paper serves as heuristic device to map the interactions settled in the networks between the resource base, social institutions and the behaviour of individual actors. For this purpose we draw on ideas from complex adaptive systems theory, evolutionary theory and evolutionary economics. Finally, we outline a research agenda for a co-evolutionary approach for natural resource management systems. © 2007.},
   author = {Christian Rammel and Sigrid Stagl and Harald Wilfing},
   doi = {10.1016/j.ecolecon.2006.12.014},
   issn = {09218009},
   issue = {1},
   journal = {Ecological Economics},
   keywords = {Co-evolution,Complex adaptive systems,Evolutionary theory,Natural resource management,Social institutions,Sustainable development},
   month = {6},
   pages = {9-21},
   title = {Managing complex adaptive systems - A co-evolutionary perspective on natural resource management},
   volume = {63},
   year = {2007},
}
@article{Costanza1987,
   abstract = {Integrating ecology and economics is increasingly important as humanity's impact on the natural world increases. Current paradigms in both fields are too narrow (and seem to be getting narrower). This paper introduces and summarizes this special issue of Ecological Modeling devoted to ecological economics. There are eleven papers (including this one) that cover most of the important theoretical issues involved (applied papers are left for a future volume). These issues are: (1) sustainability; (2) inter- and intra-species distribution of wealth; (3) discounting and intergenerational justice; and (4) dealing with non-monetized values, imprecision, and incertainty. This collection is seen as a hopeful first step toward a true synthesis of ecology and economics that could lead to better management of renewable and non-renewable natural resources and a sustainable future. © 1987.},
   author = {Robert Costanza and Herman E. Daly},
   doi = {10.1016/0304-3800(87)90041-X},
   issn = {03043800},
   issue = {1-2},
   journal = {Ecological Modelling},
   pages = {1-7},
   title = {Toward an ecological economics},
   volume = {38},
   year = {1987},
}
@book_section{Nikolaou2022,
   abstract = {Circular economy (CE) has recently become an innovative and popular scientific topic in the fields of engineering/natural-based and management/economic-based studies. It is known that two components constitute the concept of CE, i.e., a technical (circular) and an economic component (economy). These components are mainly studied by scholars on a separate basis either through engineering and natural-based sciences with various techniques and technologies or through management/economic-based sciences by utilizing managerial and economic techniques, methods, and tools. This chapter aims to conduct a short review of existing literature on CE by utilizing the classical threefold context (micro-, meso-, and macro-level) and through the engineering/nature-based science and the management/economic-based science. Furthermore, this book chapter will identify different and common research areas of engineering and management sciences in order to create a new framework to examine topics of CE.},
   author = {Ioannis E. Nikolaou and Alexandros I. Stefanakis},
   doi = {10.1016/b978-0-12-819817-9.00001-6},
   journal = {Circular Economy and Sustainability},
   pages = {1-19},
   publisher = {Elsevier},
   title = {A review of circular economy literature through a threefold level framework and engineering-management approach},
   year = {2022},
}
@book_section{,
   doi = {10.4018/978-1-7998-4504-1.ch001},
   month = {7},
   pages = {1-31},
   title = {An Introduction to Soft Systems Methodology},
   year = {2020},
}
@article{Mannan2022,
   abstract = {This chapter gives an overview of the opportunities and benefits of implementing life cycle assessment (LCA) in circular economy (CE) evaluation. The CE has been designed to create value for both economic and social life by redefining the concept of end-of-life for products through reduce, reuse, and recycle programs. As this strategy is becoming more widespread, it is necessary to scientifically assess how the products or processes effectively correspond to CE and what improvements may arise in terms of the environment and economy. Robust environmental assessment tools, such as LCA, can be beneficial for assessing the success of CE strategies by companies, product designers, and sustainability teams, as well as consumers, since a closed loop system is not always the preferred method from an environmental viewpoint. The combination of LCA and CE can result in more in-depth analysis and better understanding of economic, social, and environmental sustainability.},
   author = {Mehzabeen Mannan and Sami G. Al-Ghamdi},
   doi = {10.1016/B978-0-12-819817-9.00032-6},
   isbn = {9780128198179},
   journal = {Circular Economy and Sustainability: Volume 1: Management and Policy},
   keywords = {Circular economy,Closed loops,Environmental assessment,Life cycle assessment,Sustainability},
   month = {1},
   pages = {145-160},
   publisher = {Elsevier},
   title = {Complementing circular economy with life cycle assessment: Deeper understanding of economic, social, and environmental sustainability},
   year = {2022},
}
@article{Tschirhart2009,
   abstract = {Scientific evidence suggests that economic activity is threatening global biodiversity in ways that could severely degrade nature's flow of ecosystem services. Yet, there is relatively little work in economics that addresses biodiversity loss. Some economists have called for better integration of economic and ecological models to address biodiversity and the attendant ecosystem services. Current integrated approaches in economics are discussed, and they take in ecosystem services, ecosystem externalities, and substantial ecological modeling. Much of the modeling uses Lotka-Volterra equations, which are standard in ecology, although there is concern that the equations lack the microfoundations of plant and animal behavior. An alternative approach is to admit microbehavior using economic optimization techniques that build adaptive ecological systems. However, much more effort is needed to assess whether admitting more ecological detail into economic models will be fruitful.},
   author = {John Tschirhart},
   doi = {10.1146/annurev.resource.050708.144113},
   issn = {1941-1340},
   issue = {1},
   journal = {Annual Review of Resource Economics},
   month = {10},
   pages = {381-407},
   publisher = {Annual Reviews},
   title = {Integrated Ecological-Economic Models},
   volume = {1},
   year = {2009},
}
@report{,
   abstract = {Using market mechanisms for resource allocation in distributed systems is not a new idea, nor is it one that has caught on in practice or with a large body of computer science research. Yet, projects that use markets for distributed resource allocation recur every few years [1, 2, 3], and a new generation of research is exploring market-based resource allocation mechanisms [4, 5, 6, 7, 8] for distributed environments such as Planet-lab, Netbed, and computational grids. This paper has three goals. The first goal is to explore why markets can be appropriate to use for allocation , when simpler allocation mechanisms exist. The second goal is to demonstrate why a new look at markets for allocation could be timely, and not a rehash of previous research. The third goal is to point out some of the thorny problems inherent in market deployment and to suggest action items both for market designers and for the greater research community. We are optimistic about the power of market design, but we also believe that key challenges exist for a markets/systems integration that must be overcome for market-based computer resource allocation systems to succeed.},
   author = {Jeffrey Shneidman and Chaki Ng and David C Parkes and Alvin Auyoung and Alex C Snoeren and Amin Vahdat and Brent Chun},
   title = {Why Markets Could (But Don't Currently) Solve Resource Allocation Problems in Systems},
}
@article{Yamasaki2021,
   abstract = {To transition to a sustainable society, every local government needs to properly recognize, measure, and publish the current situation in their administrative division. This study conducted eco-efficiency assessment of activities carried out at the local level around the world based on the economic value and environmental load indicators in one year (2015). The assessment's target areas were the administrative divisions of 42 countries that were mainly OECD members. The economic value was the Gross Regional Product (GRP), and the environmental load was calculated based on LIME3, an endpoint-type and global-scale Life Cycle Impact Assessment (LCIA) method that can calculate the assessment results in monetary units (the “eco-index USD”). The indicator for eco-efficiency (dimensionless) was quantitatively defined by dividing the monetary units of the eco-index USD by the GRP in each area. The assessment results show that the average eco-efficiency value in all areas of the 42 countries was 30.2. Of these countries, the major cities (administrative division unit) with the top-three values include Paris (213.1), London (154.3), and Dublin (141.8). The results for each division were visualized on graphs and maps in order to understand their positions from environmental and economic perspectives. No such assessment has ever been conducted at the local level and under the same conditions globally. This study provides insights that contributes to the establishment of a standard methodology for local governments’ environmental accounting.},
   author = {Junya Yamasaki and Norihiro Itsubo and Akito Murayama and Ryoichi Nitanai},
   doi = {10.1016/j.cacint.2021.100061},
   issn = {25902520},
   journal = {City and Environment Interactions},
   keywords = {Eco-efficiency,Environmental accounting,Life cycle impact assessment,Local government,Sustainable city},
   month = {4},
   publisher = {Elsevier B.V.},
   title = {Eco-efficiency assessment of 42 countries’ administrative divisions based on environmental impact and gross regional product},
   volume = {10},
   year = {2021},
}
@article{Oppon2018,
   abstract = {As environmental impacts continue to rise, the need to identify and quantify the underlying causes of these impacts has prompted important research questions. This is heightened by the fact that the production of goods and services is becoming increasingly global with countries relying on each other through trade. As such, it is important to have a mechanism in place to understand the environmental burden shifts from one country to another. To this end, this paper exploits a paradox in global environmental analysis, which stems from a false decoupling between economic and production systems as observed in most developed nations, which results in improved territorial emissions of these developed countries at the expense of developing countries. Ecological unequal exchange is one such contemporary ecological economic concept that is used to highlight such asymmetric transfer of embodied natural resources and biophysical indicators between countries. Attempts at environmental impacts reduction efforts has largely focused on carbon emissions but given the complex supply chain created through globalisation and international trade, it is important to consider other important metrics such as land and water use alongside carbon emissions to drive environmental policies that will holistically address ecologically unequal exchanges. For developing countries in Africa where the dependence on land use and water use for agricultural activities are crucial to the development of national economies and in combating poverty, an assessment of these metrics has become even more paramount. Against this backdrop, the current work draws upon the theoretical constructs of multi-regional input-output (MRIO) framework to trace country specific sectorial-level flows of the aforementioned metrics between a representative developed nation, UK, and 27 African regions in order to fully examine their ecological exchanges. Key findings in the study suggest that for water consumption and land use, there is a net externalisation of these impacts for all the 27 African regions by the UK. It was also determined that the extent of the imbalance between the UK and the African region is exceedingly far greater for water consumption. It is recommended that in formulating a robust multi-national environmental policy where so many factors are at play, country specific and industry targeted approach to ecological unequal exchange between nations provides better and improved insight into addressing ensuing environmental issues.},
   author = {Eunice Oppon and Adolf Acquaye and Taofeeq Ibn-Mohammed and Lenny Koh},
   doi = {10.1016/J.ECOLECON.2018.01.030},
   issn = {0921-8009},
   journal = {Ecological Economics},
   keywords = {Africa,Carbon,Ecological unequal exchange,Environmental policies,Multi-regional input-output,United Kingdom,Water and land use},
   month = {5},
   pages = {422-435},
   publisher = {Elsevier},
   title = {Modelling Multi-regional Ecological Exchanges: The Case of UK and Africa},
   volume = {147},
   year = {2018},
}
@article{Bai2012,
   abstract = {Purpose: The purpose of this paper is to introduce a methodology to help evaluate, select, and monitor sustainable supply chain performance measurement that can be integrated into a performance management system (PMS). Design/methodology/approach: Grey-based neighborhood rough set theory is used to help arrive at a core set of important business and environmental performance measures for sustainable supply chains. The supply chain operations reference (SCOR) model is used to develop both business and environmental measures for supply chain sourcing. Findings: A case illustration shows the applicability of the methodology. A sensitivity analysis shows that variations in outcome considerations may greatly influence the set of key performance measures for a sustainable supply chain PMS. Research limitations/implications: The methodology and presentation is conceptual, yet the tool can provide very useful interpretations for both researchers and practitioners. Practical implications: The tool can be valuable for companies that are trying to identify key environmental and business performance measures for their supply chains. It helps save resources by not requiring the management of a burdensome and complex set of performance measures. Originality/value: This is one of the few approaches that helps to clearly identify and narrow the set of performance measures for sustainable supply chains. It attempts to do so with minimal information loss. It is also the first time that grey techniques have been integrated with neighborhood rough set methodology. © Emerald Group Publishing Limited.},
   author = {Chunguang Bai and Joseph Sarkis and Xiaopeng Wei and Lenny Koh},
   doi = {10.1108/13598541211212221},
   issn = {13598546},
   issue = {1},
   journal = {Supply Chain Management},
   keywords = {Ecological sustainable performance measures,Supply chain management,Supply chain operations reference model; Neighborhood rough set,Sustainable development},
   month = {1},
   pages = {78-92},
   title = {Evaluating ecological sustainable performance measures for supply chain management},
   volume = {17},
   year = {2012},
}
@article{Gong2018,
   abstract = {There has been an increasing interest in the use of decision-making models to achieve sustainability goal in recent decades. However, a systematic review of performance metrics, which are an important element of decision-making models to evaluate the outcomes regarding firm's economic, environmental and social performance, is lacking. This study provides critical reflections on the current state of literature and industry development regarding sustainable performance metrics and offers concrete suggestions to guide future research. This study contributes to existing studies by (1) exploring the interrelationship between sustainable triple-bottom performance in the decision making process; (2) integrating corporate governance mechanism into decision making process for sustainable consideration; and (3) conducting a comparison between academic theory and industry practice regarding the performance metrics proposed and employed.},
   author = {Mengfeng Gong and Andrew Simpson and Lenny Koh and Kim Hua Tan},
   doi = {10.1016/J.RESCONREC.2016.11.001},
   issn = {0921-3449},
   journal = {Resources, Conservation and Recycling},
   keywords = {Business decision making,Corporate governance mechanism,Performance metrics,Sustainable supply chain management},
   month = {1},
   pages = {155-166},
   publisher = {Elsevier},
   title = {Inside out: The interrelationships of sustainable performance metrics and its effect on business decision making: Theory and practice},
   volume = {128},
   year = {2018},
}
@article{Vachon2008,
   abstract = {Several global corporations have been severely criticized by different lobbying groups for the impact of their operations on the natural environment and on the local communities. Because corporate operations cannot be studied in isolation but rather as a part of a large network often referred to as a supply chain, this paper investigates the potential link between supply chain characteristics and sustainable development at the country level. In particular, the linkage between supply chain strength, generally defined as the number and quality of the suppliers and customers in a country, and the three dimensions of sustainable development namely environmental performance, corporate environmental practices, and social sustainability is assessed. Using archival data from The Global Competitiveness Report (2004-2005) and the 2005 Environmental Sustainability Index, a statistical assessment of the linkage was conducted. The results indicate that supply chain strength is positively linked to all three dimensions of sustainable development. © 2008 Elsevier Ltd. All rights reserved.},
   author = {Stephan Vachon and Zhimin Mao},
   doi = {10.1016/j.jclepro.2008.04.012},
   issn = {09596526},
   issue = {15},
   journal = {Journal of Cleaner Production},
   keywords = {International management,Supply chain management,Sustainable development},
   month = {10},
   pages = {1552-1560},
   title = {Linking supply chain strength to sustainable development: a country-level analysis},
   volume = {16},
   year = {2008},
}
@article{Erol2011,
   abstract = {Sustainable supply chain performance measurement is aimed at addressing environmental, social and economic aspects of sustainable supply chain management. It can be argued that it is not easy to reduce all dimensions of sustainable supply chain to a single unit. Then, the issue is that all valuations should somehow be reducible to a single one-dimensional standard. Multi-criteria evaluation introduces a framework to remedy this issue. As a consequence, multi-criteria evaluation seems to supply a proper and adequate assessment framework for sustainable supply chain assessment. In this study, a multi-criteria framework based on fuzzy entropy and fuzzy multi-attribute utility (FMAUT) is proposed in order to evaluate and compare the company performances in terms of sustainable supply chain. However, note that reducing all aspects of sustainable supply chain to a single unit using a multi-criteria framework may not be sufficient to satisfy all the needs of decision makers although it is used to evaluate sustainability performance of supply chains with respect to three aspects. Therefore, in this research, an alert management system is also developed to satisfy further requirements of users. The proposed frameworks are tested using data obtained from one of the middle sized Turkish grocery retailers. © 2011 Elsevier B.V.},
   author = {Ismail Erol and Safiye Sencer and Ramazan Sari},
   doi = {10.1016/J.ECOLECON.2011.01.001},
   issn = {09218009},
   issue = {6},
   journal = {Ecological Economics},
   keywords = {Fuzzy arithmetic,Multi-criteria decision making,Performance analysis,Sustainable supply chain},
   month = {4},
   pages = {1088-1100},
   title = {A new fuzzy multi-criteria framework for measuring sustainability performance of a supply chain},
   volume = {70},
   year = {2011},
}
@article{Pittaway2004,
   abstract = {Recent work on competitiveness has emphasized the importance of business networking for innovativeness. Until recently, insights into the dynamics of this relationship have been fragmented. This paper presents a systematic review of research linking the networking behaviour of firms with their innovative capacity. We find that the principal benefits of networking as identified in the literature include: risk sharing; obtaining access to new markets and technologies; speeding products to market; pooling complementary skills; safeguarding property rights when complete or contingent contracts are not possible; and acting as a key vehicle for obtaining access to external knowledge. The evidence also illustrates that those firms which do not co-operate and which do not formally or informally exchange knowledge limit their knowledge base long term and ultimately reduce their ability to enter into exchange relationships. At an institutional level, national systems of innovation play an important role in the diffusion of innovations in terms of the way in which they shape networking activity. The paper provides evidence suggesting that network relationships with suppliers, customers and intermediaries such as professional and trade associations are important factors affecting innovation performance and productivity. Where networks fail, it is due to inter-firm conflict, displacement, lack of scale, external disruption and lack of infrastructure. The review identifies several gaps in the literature that need to be filled. For instance, there is a need for further exploration of the relationship between networking and different forms of innovation, such as process and organisational innovation. Similarly, we need better understanding of network dynamics and network configurations, as well as the role of third parties such as professional and trade associations. Our study highlights the need for interdisciplinary research in these areas.},
   author = {Luke Pittaway and Maxine Robertson and Kamal Munir and David Denyer and Andy Neely},
   doi = {10.1111/J.1460-8545.2004.00101.X},
   issn = {14608545},
   issue = {3-4},
   journal = {International Journal of Management Reviews},
   month = {9},
   pages = {137-168},
   title = {Networking and innovation: A systematic review of the evidence},
   volume = {5-6},
   year = {2004},
}
@article{Zhang2014,
   abstract = {Sustainable supply chains are the need of modern times. In order to make them successful, listening to voice of the customer and integrating them in the design and development phases of supply chains are very important. In this paper, we propose a technique called sustainability function deployment (SFD) developed on the concept of quality function deployment (QFD) to model customer and technical requirements, establish relationship between them and prioritise them for developing sustainable supply chains. SFD gathers customer and technical requirements for sustainable supply chain planning using C-REQ and T-REQ surveys, generates priority of customer requirements using Priority Matrix, performs screening of technical requirements using gap analysis, and generates requirements weights using QFD, thereby providing a structured approach to the decision-makers to select the right areas of improvement. Both the customer and technical requirements are captured from a socio-economic- environmental perspective. Results of application of the proposed technique are provided. The strength of the proposed approach is its novelty in integration of priority matrix and gap analysis in prioritising requirements based on their importance and performance in SFD. Besides, SFD facilitates modelling of complex, interrelated requirements in sustainable supply chain planning. © 2014 Taylor & Francis.},
   author = {Zhonghua Zhang and Anjali Awasthi},
   doi = {10.1080/00207543.2014.899717},
   issn = {1366588X},
   issue = {17},
   journal = {International Journal of Production Research},
   keywords = {gap analysis,priority matrix,quality function deployment,sustainability function deployment,sustainable supply chains,voice of the customer},
   month = {9},
   pages = {5131-5154},
   publisher = {Taylor and Francis Ltd.},
   title = {Modelling customer and technical requirements for sustainable supply chain planning},
   volume = {52},
   year = {2014},
}
@article{Wang2010,
   abstract = {With the urgency of global warming, green supply chain management, logistics in particular, has drawn the attention of researchers. Although there are closed-loop green logistics models in the literature, most of them do not consider the uncertain environment in general terms. In this study, a generalized model is proposed where the uncertainty is expressed by fuzzy numbers. An interval programming model is proposed by the defined means and mean square imprecision index obtained from the integrated information of all the level cuts of fuzzy numbers. The resolution for interval programming is based on the decision maker (DM)'s preference. The resulting solution provides useful information on the expected solutions under a confidence level containing a degree of risk. The results suggest that the more optimistic the DM is, the better is the resulting solution. However, a higher risk of violation of the resource constraints is also present. By defining this probable risk, a solution procedure was developed with numerical illustrations. This provides a DM trade-off mechanism between logistic cost and the risk. © 2010 Elsevier Ltd.},
   author = {Hsiao Fan Wang and Hsin Wei Hsu},
   doi = {10.1016/j.jenvman.2010.05.009},
   issn = {03014797},
   issue = {11},
   journal = {Journal of Environmental Management},
   keywords = {Fuzzy number,Green supply chain,Interval programming,Mean and mean square imprecision index,Risk analysis,Trade-off analysis},
   month = {11},
   pages = {2148-2162},
   pmid = {20547439},
   title = {Resolution of an uncertain closed-loop logistics model: An application to fuzzy linear programs with risk analysis},
   volume = {91},
   year = {2010},
}
@article{Nasir2017,
   abstract = {In the last decades, green and sustainable supply chain management practices have been developed in efforts to try and reduce the negative consequences of production and consumption processes on the environment. In parallel to this, the circular economy discourse has been propagated in the industrial ecology and production economics literature and, lately, in business and practice. The ideals of circular economy principles suggests that the frontiers of environmental sustainability can be pushed by emphasising the idea of transforming products in such a way that there are workable relationships between ecological systems and economic growth. By arguing for these ideals to be integrated into green supply chain management theory and practice, the paper uses a case study from the construction industry to demonstrate the environmental gains in terms of carbon emissions that can be achieved through some circular economy principles as against traditional linear production systems. The paper therefore asserts that an integration of circular economy principles within sustainable supply chain management can provide clear advantages from an environmental point view despite some external supply chain influences and scenarios. Further to this, emerging supply chain management challenges and market dynamics are also highlighted and discussed.},
   author = {Mohammed Haneef Abdul Nasir and Andrea Genovese and Adolf A. Acquaye and S. C.L. Koh and Fred Yamoah},
   doi = {10.1016/J.IJPE.2016.06.008},
   issn = {0925-5273},
   journal = {International Journal of Production Economics},
   keywords = {Carbon emissions,Circular economy,Construction,Linear supply chain},
   month = {1},
   pages = {443-457},
   publisher = {Elsevier},
   title = {Comparing linear and circular supply chains: A case study from the construction industry},
   volume = {183},
   year = {2017},
}
@article{Gaur2017,
   abstract = {Closed-Loop Supply Chain Management (CLSCM) is considered as a strategic response to the call for corporate sustainability while further expanding the scope of value creation to include product reconstruction. The Closed-Loop Supply Chain (CLSC) performance is directly related to the CLSC network design. The CLSC network design, with long-term and strategic connotations, involves selection of an integrated network of partner organizations to be engaged on one hand in the forward supply chain processes relevant to families of existing and new products and also involved in reverse supply chain activities relevant to reconstruction of the returned products. At the tactical level, Closed-Loop Supply Chain Configuration (CLSCC) attempts to address issues pertinent to launch of a new product and its reconstruction. The CLSC network design is well studied in the current literature, but addressing the CLSCC is neglected. To study the CLSCC problem we: (a) develop an integrated optimization model for problem; (b) present a real-world case study of a battery manufacturer; (c) based on the case study, we conduct a comprehensive set of computational experiments followed by a series of what-if analyses to compare profitability of the Forward Supply Chain Configuration (FSCC) versus the CLSCC; and (c) discuss the key observations and managerial implications drawn from the computational experiments, applicable to other real-world instances. The significant outcomes of the study suggest that: (i) performance of the firm׳s base case integrated CLSCC model is significantly better than the current supply chain model (ii) the sales-price ratio of new battery is found to be negatively related with the maximum acquiring price of used batteries; (iii) combination of sales price ratios of new and reconditioned batteries determines the total net profit for a given return rate. Finally, important managerial insights and scope for future research are discussed.},
   author = {Jighyasu Gaur and Mehdi Amini and A. K. Rao},
   doi = {10.1016/j.omega.2015.11.008},
   issn = {03050483},
   journal = {Omega (United Kingdom)},
   keywords = {Case study,Marketing,Mathematical programming,Operations management,Product life cycle,Production planning and control},
   month = {1},
   pages = {212-223},
   publisher = {Elsevier Ltd},
   title = {Closed-loop supply chain configuration for new and reconditioned products: An integrated optimization model},
   volume = {66},
   year = {2017},
}
@article{Genovese2017,
   abstract = {In the last decades, green and sustainable supply chain management practices have been developed, trying to integrate environmental concerns into organisations by reducing unintended negative consequences on the environment of production and consumption processes. In parallel to this, the circular economy discourse has been propagated in the industrial ecology literature and practice. Circular economy pushes the frontiers of environmental sustainability by emphasising the idea of transforming products in such a way that there are workable relationships between ecological systems and economic growth. Therefore, circular economy is not just concerned with the reduction of the use of the environment as a sink for residuals but rather with the creation of self-sustaining production systems in which materials are used over and over again. Through two case studies from different process industries (chemical and food), this paper compares the performances of traditional and circular production systems across a range of indicators. Direct, indirect and total lifecycle emissions, waste recovered, virgin resources use, as well as carbon maps (which provide a holistic visibility of the entire supply chain) are presented. The paper asserts that an integration of circular economy principles within sustainable supply chain management can provide clear advantages from an environmental point view. Emerging supply chain management challenges and market dynamics are also highlighted and discussed.},
   author = {Andrea Genovese and Adolf A. Acquaye and Alejandro Figueroa and S. C.Lenny Koh},
   doi = {10.1016/J.OMEGA.2015.05.015},
   issn = {0305-0483},
   journal = {Omega},
   keywords = {Circular economy,Decision support,Environmental sustainability,Green supply chain management,Product lifecycle analysis},
   month = {1},
   pages = {344-357},
   publisher = {Pergamon},
   title = {Sustainable supply chain management and the transition towards a circular economy: Evidence and some applications},
   volume = {66},
   year = {2017},
}
@article{Mohammadi2022,
   abstract = {Considering the complexity, interactions, and dynamics that permeate the Supply Chain (SC), computational modeling and simulation promote determining the system's behavior and decision-making. Among simulation techniques, system dynamics (SD) investigate on recognition of variables and their dynamic behavior trend throughout time in the SC. This paper presents a comprehensive SD model related to upstream steel SC management up to four echelons: concentrate, pellet, sponge iron, and steel. The model noticed Causal-Loops Diagrams (CLD) and Stock-Flow Diagrams (SFD) and provided a simulation framework. The robustness of the proposed model was evaluated by implementing the defined model in a multi-echelon steel complex in Iran. Various scenarios were analyzed applying stochastic simulation to include selected random variables. Iron ore grade and tonnage play the most critical role in the network's performance. With an approximately 4% increase in the iron ore grade, steel production costs decreased 2.4%. The influence of the simultaneous uncertainty in iron ore grade and iron ore supply in the range of extreme levels of actual historical data resulted in an increase and decrease of +14% and −32% on the total steel production costs, respectively. Furthermore, removing energy subsidies and increasing five times in price results in a rise in total expenses up to 60% and a fall in the marginal profit up to 48%.},
   author = {Mir Ahmad Mohammadi and Ahmad Reza Sayadi and Mahsa Khoshfarman and Ali Husseinzadeh Kashan},
   doi = {10.1016/j.resourpol.2022.102690},
   issn = {03014207},
   journal = {Resources Policy},
   keywords = {Iron and steel industry,Modeling,Simulation,Supply chain management,System dynamics},
   month = {8},
   publisher = {Elsevier Ltd},
   title = {A systems dynamics simulation model of a steel supply chain-case study},
   volume = {77},
   year = {2022},
}
@article{forestThroughCS,
   abstract = {Complex systems science provides a transdisciplinary framework to study systems characterized by (1) heterogeneity, (2) hierarchy, (3) self-organization, (4) openness, (5) adaptation, (6) memory, (7) non-linearity, and (8) uncertainty. Complex systems thinking has inspired both theory and applied strategies for improving ecosystem resilience and adaptability, but applications in forest ecology and management are just beginning to emerge. We review the properties of complex systems using four well-studied forest biomes (temperate, boreal, tropical and Mediterranean) as examples. The lens of complex systems science yields insights into facets of forest structure and dynamics that facilitate comparisons among ecosystems. These biomes share the main properties of complex systems but differ in specific ecological properties, disturbance regimes, and human uses. We show how this approach can help forest scientists and managers to conceptualize forests as integrated social-ecological systems and provide concrete examples of how to manage forests as complex adaptive systems. © 2014 Filotas et al.},
   author = {Elise Filotas and Lael Parrott and Philip J. Burton and Robin L. Chazdon and K. David Coates and Lluís Coll and Sybille Haeussler and Kathy Martin and Susanna Nocentini and Klaus J. Puettmann and Francis E. Putz and Suzanne W. Simard and Christian Messier},
   doi = {10.1890/ES13-00182.1},
   issn = {21508925},
   issue = {1},
   journal = {Ecosphere},
   keywords = {Adaptation,Complex systems,Forests,Resilience,Social-ecological systems},
   month = {1},
   publisher = {Ecological Society of America},
   title = {Viewing forests through the lens of complex systems science},
   volume = {5},
   year = {2014},
}
@article{ecomodelling,
   abstract = {Reprint. Originally published: New York : Wiley, ©1977.},
   author = {Calvin Dytham and Charles A.S. Hall and John W. Day},
   doi = {10.2307/5528},
   issn = {00218790},
   issue = {1},
   journal = {The Journal of Animal Ecology},
   month = {2},
   pages = {234},
   publisher = {JSTOR},
   title = {Ecosystem Modeling in Theory and Practice: An Introduction with Case Histories},
   volume = {61},
   year = {1992},
}
@book{pmbook,
   abstract = {This second edition of An Introduction to Predictive Maintenance helps plant, process, maintenance and reliability managers and engineers to develop and implement a comprehensive maintenance management program, providing proven strategies for regularly monitoring critical process equipment and systems, predicting machine failures, and scheduling maintenance accordingly.

Since the publication of the first edition in 1990, there have been many changes in both technology and methodology, including financial implications, the role of a maintenance organization, predictive maintenance techniques, various analyses, and maintenance of the program itself. This revision includes a complete update of the applicable chapters from the first edition as well as six additional chapters outlining the most recent information available.

Having already been implemented and maintained successfully in hundreds of manufacturing and process plants worldwide, the practices detailed in this second edition of An Introduction to Predictive Maintenance will save plants and corporations, as well as U.S. industry as a whole, billions of dollars by minimizing unexpected equipment failures and its resultant high maintenance cost while increasing productivity.},
   author = {R. Keith Mobley},
   doi = {10.1016/B978-0-7506-7531-4.X5000-3},
   isbn = {9780750675314},
   publisher = {Elsevier},
   title = {An Introduction to Predictive Maintenance},
   volume = {1},
   year = {2002},
}
@article{sbpm,
   abstract = {Recently, with the emergence of Industry 4.0 (I4.0), smart systems, machine learning (ML) within artificial intelligence (AI), predictive maintenance (PdM) approaches have been extensively applied in industries for handling the health status of industrial equipment. Due to digital transformation towards I4.0, information techniques, computerized control, and communication networks, it is possible to collect massive amounts of operational and processes conditions data generated form several pieces of equipment and harvest data for making an automated fault detection and diagnosis with the aim to minimize downtime and increase utilization rate of the components and increase their remaining useful lives. PdM is inevitable for sustainable smart manufacturing in I4.0. Machine learning (ML) techniques have emerged as a promising tool in PdM applications for smart manufacturing in I4.0, thus it has increased attraction of authors during recent years. This paper aims to provide a comprehensive review of the recent advancements of ML techniques widely applied to PdM for smart manufacturing in I4.0 by classifying the research according to the ML algorithms, ML category, machinery, and equipment used, device used in data acquisition, classification of data, size and type, and highlight the key contributions of the researchers, and thus offers guidelines and foundation for further research.},
   author = {Zeki Murat Çinar and Abubakar Abdussalam Nuhu and Qasim Zeeshan and Orhan Korhan and Mohammed Asmael and Babak Safaei},
   doi = {10.3390/su12198211},
   issn = {20711050},
   issue = {19},
   journal = {Sustainability (Switzerland)},
   keywords = {Artificial intelligence,Industrial maintenance,Machine learning,Predictive maintenance},
   month = {10},
   publisher = {MDPI},
   title = {Machine learning in predictive maintenance towards sustainable smart manufacturing in industry 4.0},
   volume = {12},
   year = {2020},
}
@web_page{mendixForum,
   author = {Andrej Gajduk},
   month = {10},
   title = {Mendix Forum - Questions},
   url = {https://forum.mendix.com/link/questions/96386},
   year = {2019},
}
@web_page{jssqrt,
   author = {Amit Diwan},
   month = {9},
   title = {Finding square root of a number without using library functions - JavaScript},
   url = {https://www.tutorialspoint.com/finding-square-root-of-a-number-without-using-library-functions-javascript},
   year = {2020},
}
@article{pmlt,
   abstract = {This study covers new trends and techniques in the field of predictive maintenance, which has been superseding traditional management policies, at least in part. It also presents suggestions for how to implement a predictive maintenance programme in a factory/premise and so on. Predictive maintenance primarily involves foreseeing breakdown of the system to be maintained by detecting early signs of failure in order to make maintenance work more proactive. In addition to the aim of acting before failure, it also aims to attend to any fault, even if there is no immediate danger of failure, to ensure smooth operation and reduce energy consumption. Predictive maintenance has been adopted by various sectors in manufacturing and service industries in order to improve reliability, safety, availability, efficiency and quality as well as to protect the environment. It also has created a separate sector, which specializes in developing predictive maintenance instruments, offering dedicated predictive maintenance solutions and training predictive maintenance experts. Predictive maintenance techniques are closely associated with sensor technologies but for efficient predictive maintenance applications, a comprehensive approach, which integrates sensing with subsequent maintenance activities, is needed to be adapted in accordance with the needs of the particular organization. Recent advances in information, communication and computer technologies, such as Internet of Things and radio-frequency identifications, have been enabling predictive maintenance applications to be more efficient, applicable, affordable, and consequently more common and available for all sorts of industries. Researches on remote maintenance and e-maintenance have been supporting predictive maintenance activities especially in unsafe working environments and scattered locations.},
   author = {Sule Selcuk},
   doi = {10.1177/0954405415601640},
   issn = {20412975},
   issue = {9},
   journal = {Proceedings of the Institution of Mechanical Engineers, Part B: Journal of Engineering Manufacture},
   keywords = {Internet of Things,Predictive maintenance,Wireless Network Systems,availability,e-maintenance,quality,radio-frequency identification,reliability,remote maintenance},
   month = {7},
   pages = {1670-1679},
   publisher = {SAGE Publications Ltd},
   title = {Predictive maintenance, its implementation and latest trends},
   volume = {231},
   year = {2017},
}
@report{,
   title = {Week-By-Week log},
}
@report{pdmI4,
   author = {Mark Haarman and Michel Mulders and Costas Vassiliadis},
   city = {Amsterdam},
   institution = {pwc},
   title = {PdM 4.0},
   url = {https://www.pwc.nl/nl/assets/documents/pwc-predictive-maintenance-4-0.pdf},
   year = {2017},
}
@article{Zonta2020,
   abstract = {Industry 4.0 is collaborating directly for the technological revolution. Both machines and managers are daily confronted with decision making involving a massive input of data and customization in the manufacturing process. The ability to predict the need for maintenance of assets at a specific future moment is one of the main challenges in this scope. The possibility of performing predictive maintenance contributes to enhancing machine downtime, costs, control, and quality of production. We observed that surveys and tutorials about Industry 4.0 focus mainly on addressing data analytics and machine learning methods to change production procedures, so not comprising predictive maintenance methods and their organization. In this context, this article presents a systematic literature review of initiatives of predictive maintenance in Industry 4.0, identifying and cataloging methods, standards, and applications. As the main contributions, this survey discusses the current challenges and limitations in predictive maintenance, in addition to proposing a novel taxonomy to classify this research area considering the needs of the Industry 4.0. We concluded that computer science, including artificial intelligence and distributed computing fields, is more and more present in an area where engineering was the dominant expertise, so detaching the importance of a multidisciplinary approach to address Industry 4.0 effectively.},
   author = {Tiago Zonta and Cristiano André da Costa and Rodrigo da Rosa Righi and Miromar José de Lima and Eduardo Silveira da Trindade and Guann Pyng Li},
   doi = {10.1016/j.cie.2020.106889},
   issn = {03608352},
   journal = {Computers and Industrial Engineering},
   keywords = {Artificial intelligence,Conditional-based maintenance,Industry 4.0,Predictive Maintenance,Remaining Useful Life},
   month = {12},
   publisher = {Elsevier Ltd},
   title = {Predictive maintenance in the Industry 4.0: A systematic literature review},
   volume = {150},
   year = {2020},
}
@article{Geng2018,
   abstract = {Cloud manufacturing is a specific implementation form of the 'Internet + manufacturing' strategy. Why and how to develop cloud manufacturing platform (CMP), however, remains the key concern of both platform operators and users. A microscopic model is proposed to investigate advantages and diffusion forces of CMP through exploration of its diffusion process and mechanism. Specifically, a three-stage basic evolution process of CMP is innovatively proposed. Then, based on this basic process, a more complex CMP evolution model has been established in virtue of complex network theory, with five diffusion forces identified. Thereafter, simulations on CMP diffusion have been conducted. The results indicate that, CMP possesses better resource utilization, user satisfaction, and enterprise utility. Results of simulation on impacts of different diffusion forces show that both the time required for CMP to reach an equilibrium state and the final network size are affected simultaneously by the five diffusion forces. All these analyses indicate that CMP could create an open online cooperation environment and turns out to be an effective implementation of the 'Internet + manufacturing' strategy.},
   author = {Chao Geng and Shiyou Qu and Yingying Xiao and Mei Wang and Guoqiang Shi and Tingyu Lin and Junjie Xue and Zhengxuan Jia},
   doi = {10.21629/JSEE.2018.02.13},
   issn = {16711793},
   issue = {2},
   journal = {Journal of Systems Engineering and Electronics},
   keywords = {Gale-Shapley algorithm,cloud manufacturing,complex network,cooperative game theory,innovation diffusion,network effect},
   month = {4},
   pages = {321-335},
   publisher = {Beijing Institute of Aerospace Information},
   title = {Diffusion mechanism simulation of cloud manufacturing complex network based on cooperative game theory},
   volume = {29},
   year = {2018},
}
@article{corrmancost,
   abstract = {Maintenance can represent a significant portion of the cost in asset intensive organisations, as breakdowns have an impact on the capacity, quality and cost of operation. However, the formulation of a maintenance strategy depends on a number of factors, including the cost of down time, reliability characteristics and redundancy of assets. Consequently, the balance between preventive maintenance (PM) and corrective maintenance (CM) for minimising costs varies between organisations and assets. Nevertheless, there are some rules of thumb on the balance between PM and CM, such as the 80/20 rule. Studies on the relationship between PM and CM in practice are rare. Therefore, PM and CM costs are studied in this article by analysing historical maintenance data. A case study of rail infrastructure historical data is carried out to determine the shares of PM and CM, together with a cost–benefit analysis (CBA) to assess the value of PM. The results show that the PM represents (Formula presented.) 10% to 30% of the total maintenance cost when user costs, i.e. train delays, are included as a CM cost. The CBA shows the benefit of PM is positive with a benefit–cost ratio at 3.3. However, the results depend on the inclusion/exclusion of user costs, besides individual organisational parameters.},
   author = {Christer Stenström and Per Norrbin and Aditya Parida and Uday Kumar},
   doi = {10.1080/15732479.2015.1032983},
   issn = {17448980},
   issue = {5},
   journal = {Structure and Infrastructure Engineering},
   keywords = {cost analysis,cost–benefit ratios,maintenance,maintenance costs,railway systems},
   month = {5},
   pages = {603-617},
   publisher = {Taylor and Francis Ltd.},
   title = {Preventive and corrective maintenance – cost comparison and cost–benefit analysis},
   volume = {12},
   year = {2016},
}
@article{productComplex,
   abstract = {Development is a process of transforming a country's economic structure towards the production and export of more complex products. We use Hidalgo and Hausmann's (2009) method of reflections to compute measures of product and country complexity, and rank 5107 products and 124 countries. We find that: (i) the most complex products are in machinery, chemicals, and metals, while the least complex products are raw materials and commodities, wood, textiles, and agricultural products; (ii) the most complex economies in the world are Japan, Germany, and Sweden, and the least complex, Cambodia, Papua New Guinea, and Nigeria; (iii) the major exporters of the more complex products are the high-income countries, while the major exporters of the less complex products are the low-income countries; and (iv) export shares of the more complex products increase with income, while export shares of the less complex products decrease with income. © 2011 Elsevier B.V.},
   author = {Jesus Felipe and Utsav Kumar and Arnelyn Abdon and Marife Bacate},
   doi = {10.1016/j.strueco.2011.08.003},
   issn = {0954349X},
   issue = {1},
   journal = {Structural Change and Economic Dynamics},
   keywords = {Capabilities,Diversification,Economic complexity,Product complexity,Ubiquity},
   month = {3},
   pages = {36-68},
   title = {Product complexity and economic development},
   volume = {23},
   year = {2012},
}
@report{,
   author = {David J Edwards and Gary D Holt and F C Harris},
   title = {Predictive maintenance techniques and their relevance to construction plant},
}
@inproceedings{Killeen2019,
   abstract = {In recent years, the Internet of Things (IoT) and big data have been hot topics. With all this data being produced, new applications such as predictive maintenance are possible. Consensus self-organized models approach (COSMO) is an example of a predictive maintenance system for a fleet of public transport buses, which attempts to diagnose faulty buses that deviate from the rest of the bus fleet. The present work proposes a novel IoT architecture for predictive maintenance and proposes a semi-supervised machine learning algorithm that attempts to improve the sensor selection performed in COSMO. With the help of the Société de Transport de l’Outaouais, a minimally viable prototype of the architecture has been deployed and J1939 sensor data have been acquired.},
   author = {Patrick Killeen and Bo Ding and Iluju Kiringa and Tet Yeap},
   doi = {10.1016/j.procs.2019.04.184},
   issn = {18770509},
   journal = {Procedia Computer Science},
   keywords = {Fleet management,Internet of things,J1939,Predictive analytics,Predictive maintenance,Sensor selection},
   pages = {607-613},
   publisher = {Elsevier B.V.},
   title = {IoT-based predictive maintenance for fleet management},
   volume = {151},
   year = {2019},
}
@article{NE,
   abstract = {Cloud manufacturing is a specific implementation form of the 'Internet + manufacturing' strategy. Why and how to develop cloud manufacturing platform (CMP), however, remains the key concern of both platform operators and users. A microscopic model is proposed to investigate advantages and diffusion forces of CMP through exploration of its diffusion process and mechanism. Specifically, a three-stage basic evolution process of CMP is innovatively proposed. Then, based on this basic process, a more complex CMP evolution model has been established in virtue of complex network theory, with five diffusion forces identified. Thereafter, simulations on CMP diffusion have been conducted. The results indicate that, CMP possesses better resource utilization, user satisfaction, and enterprise utility. Results of simulation on impacts of different diffusion forces show that both the time required for CMP to reach an equilibrium state and the final network size are affected simultaneously by the five diffusion forces. All these analyses indicate that CMP could create an open online cooperation environment and turns out to be an effective implementation of the 'Internet + manufacturing' strategy.},
   author = {Chao Geng and Shiyou Qu and Yingying Xiao and Mei Wang and Guoqiang Shi and Tingyu Lin and Junjie Xue and Zhengxuan Jia},
   doi = {10.21629/JSEE.2018.02.13},
   issn = {16711793},
   issue = {2},
   journal = {Journal of Systems Engineering and Electronics},
   keywords = {Gale-Shapley algorithm,cloud manufacturing,complex network,cooperative game theory,innovation diffusion,network effect},
   month = {4},
   pages = {321-335},
   publisher = {Beijing Institute of Aerospace Information},
   title = {Diffusion mechanism simulation of cloud manufacturing complex network based on cooperative game theory},
   volume = {29},
   year = {2018},
}
@article{iotBD,
   abstract = {The manufacturing industry is currently in the midst of a data-driven revolution, which promises to transform traditional manufacturing facilities in to highly optimised smart manufacturing facilities. These smart facilities are focused on creating manufacturing intelligence from real-time data to support accurate and timely decision-making that can have a positive impact across the entire organisation. To realise these efficiencies emerging technologies such as Internet of Things (IoT) and Cyber Physical Systems (CPS) will be embedded in physical processes to measure and monitor real-time data from across the factory, which will ultimately give rise to unprecedented levels of data production. Therefore, manufacturing facilities must be able to manage the demands of exponential increase in data production, as well as possessing the analytical techniques needed to extract meaning from these large datasets. More specifically, organisations must be able to work with big data technologies to meet the demands of smart manufacturing. However, as big data is a relatively new phenomenon and potential applications to manufacturing activities are wide-reaching and diverse, there has been an obvious lack of secondary research undertaken in the area. Without secondary research, it is difficult for researchers to identify gaps in the field, as well as aligning their work with other researchers to develop strong research themes. In this study, we use the formal research methodology of systematic mapping to provide a breadth-first review of big data technologies in manufacturing.},
   author = {Peter O'donovan and Kevin Leahy and Ken Bruton and Dominic T J O'sullivan},
   doi = {10.1186/s40537-015-0028-x},
   keywords = {Big data,Big data analytics,Big data systems,Cyber physical systems,Distributed computing,Engineering informatics,Industry 40,Internet of things, loT,Machine learning,Manufacturing,Smart manufacturing},
   title = {Performance analysis of OFDM modulation on indoor broadband PLC channels},
   year = {2011},
}
@inproceedings{iotpm,
   abstract = {In recent years, the Internet of Things (IoT) and big data have been hot topics. With all this data being produced, new applications such as predictive maintenance are possible. Consensus self-organized models approach (COSMO) is an example of a predictive maintenance system for a fleet of public transport buses, which attempts to diagnose faulty buses that deviate from the rest of the bus fleet. The present work proposes a novel IoT architecture for predictive maintenance and proposes a semi-supervised machine learning algorithm that attempts to improve the sensor selection performed in COSMO. With the help of the Société de Transport de l’Outaouais, a minimally viable prototype of the architecture has been deployed and J1939 sensor data have been acquired.},
   author = {Patrick Killeen and Bo Ding and Iluju Kiringa and Tet Yeap},
   doi = {10.1016/j.procs.2019.04.184},
   issn = {18770509},
   journal = {Procedia Computer Science},
   keywords = {Fleet management,Internet of things,J1939,Predictive analytics,Predictive maintenance,Sensor selection},
   pages = {607-613},
   publisher = {Elsevier B.V.},
   title = {IoT-based predictive maintenance for fleet management},
   volume = {151},
   year = {2019},
}
@report{,
   author = {David J Edwards and Gary D Holt and F C Harris},
   title = {Predictive maintenance techniques and their relevance to construction plant},
}
@article{initPm,
   abstract = {For the past recent years, Industry 4.0 (I40) also known as smart manufacturing, together with advanced manufacturing techniques, has been introduced in the industrial manufacturing sector to improve and stabilize processes. Nevertheless, practical applications of these advanced technologies are still in their early stages resulting in slow adoption of the I40 concepts, especially for small- to medium-scale enterprises (SMEs). This paper proposes the design of an experimental method to integrate the practical use of Industry 4.0 in a small bottling plant; especially by detecting early faults or threats in conveyor motors and generating accordingly a predictive maintenance schedule. Using advanced programming functions of a Siemens S7-1200 programmable logic controller (PLC) controlling the bottling plant, vibration speed data is monitored through vibration sensors mounted on the motor and an efficient predictive maintenance plan is generated. The running PLC communicates with a supervisory control and data acquisition (SCADA) graphical user interface (GUI) which instantaneously displays maintenance schedules and allows, whenever required, flexible configuration of new maintenance rules. This paper also proposes a decentralized monitoring system from which vibration speed states can be monitored on a cloud-based report accessible via the Internet; the decentralized monitoring system also sends instant email notifications to the intended supervisor for every maintenance schedule generated. By its results, this research shows different possibilities of the practical use of Industry 4.0 basic concepts to better manufacturing operations within SMEs and opens a path for more improvement in this sector.},
   author = {Kahiomba Sonia Kiangala and Zenghui Wang},
   doi = {10.1007/s00170-018-2093-8},
   issn = {14333015},
   issue = {9-12},
   journal = {International Journal of Advanced Manufacturing Technology},
   keywords = {Cloud-based dashboard report,Email notification,Industry 4.0,Motor vibration speed,Predictive maintenance,SCADA,Siemens S7-1200 PLC},
   month = {8},
   pages = {3251-3271},
   publisher = {Springer London},
   title = {Initiating predictive maintenance for a conveyor motor in a bottling plant using industry 4.0 concepts},
   volume = {97},
   year = {2018},
}
@report{pdm4.0,
   title = {PdM 4.0},
   year = {2017},
}
@article{litRev,
   abstract = {Industry 4.0 is collaborating directly for the technological revolution. Both machines and managers are daily confronted with decision making involving a massive input of data and customization in the manufacturing process. The ability to predict the need for maintenance of assets at a specific future moment is one of the main challenges in this scope. The possibility of performing predictive maintenance contributes to enhancing machine downtime, costs, control, and quality of production. We observed that surveys and tutorials about Industry 4.0 focus mainly on addressing data analytics and machine learning methods to change production procedures, so not comprising predictive maintenance methods and their organization. In this context, this article presents a systematic literature review of initiatives of predictive maintenance in Industry 4.0, identifying and cataloging methods, standards, and applications. As the main contributions, this survey discusses the current challenges and limitations in predictive maintenance, in addition to proposing a novel taxonomy to classify this research area considering the needs of the Industry 4.0. We concluded that computer science, including artificial intelligence and distributed computing fields, is more and more present in an area where engineering was the dominant expertise, so detaching the importance of a multidisciplinary approach to address Industry 4.0 effectively.},
   author = {Tiago Zonta and Cristiano André da Costa and Rodrigo da Rosa Righi and Miromar José de Lima and Eduardo Silveira da Trindade and Guann Pyng Li},
   doi = {10.1016/J.CIE.2020.106889},
   issn = {0360-8352},
   journal = {Computers & Industrial Engineering},
   keywords = {Artificial intelligence,Conditional-based maintenance,Industry 4.0,Predictive Maintenance,Remaining Useful Life},
   month = {12},
   pages = {106889},
   publisher = {Pergamon},
   title = {Predictive maintenance in the Industry 4.0: A systematic literature review},
   volume = {150},
   year = {2020},
}
@article{Wang2022,
   abstract = {Smart manufacturing refers to an advanced mode of manufacturing, which incorporates computer-integrated manufacturing (CIM) and artificial intelligence (AI) for data-enabled adaptability throughout the production cycle, from product design over process scheduling, control, and optimization to product quality assurance. Enabling this mode of manufacturing are two essential techniques: smart scheduling and predictive maintenance. For Industry 4.0 (I4.0)-based manufacturing systems, all resources (e.g., machines, robots, vehicles, materials, etc.) in a smart factory are represented as cyber-physical systems (CPS), i.e., physical entities equipped with digital identification such as RFIDs, sensors, edge computing electronics, etc. Supported by AI, this new manufacturing paradigm provides new opportunities for scheduling production resources and predictive maintenance. This chapter focuses on describing cutting-edge tools, technologies, and infrastructure that enable the transition from traditional production scheduling and time-based maintenance to smart scheduling and predictive maintenance, which ultimately enable smart manufacturing.},
   author = {Jinjiang Wang and Robert X. Gao},
   doi = {10.1016/B978-0-12-823657-4.00007-5},
   isbn = {9780128236574},
   journal = {Design and Operation of Production Networks for Mass Personalization in the Era of Cloud Technology},
   month = {1},
   pages = {181-207},
   publisher = {Elsevier},
   title = {Innovative smart scheduling and predictive maintenance techniques},
   year = {2022},
}
@article{Clements2001,
   abstract = {The author describes the development, organization and initial results of a predictive maintenance program for the compressor stations of natural gas pipelines. The immediate goals of the program are: (1) to eliminate scheduled overhaul of engines; (2) to achieve maximum life of parts, and (3) to detect and deal with problems while still minor, with the ultimate result being maximum efficiency and utilization of materials and increased reliability of the pipeline system.},
   author = {Edward H. Clements},
   doi = {10.1016/B978-075067328-0/50052-5},
   journal = {Gas Compressor Inst, 22nd Annu, Sel Pap, Liberal, Kans},
   month = {1},
   pages = {867-888},
   publisher = {Butterworth-Heinemann},
   title = {Predictive Maintenance},
   year = {2001},
}
@article{litRevML,
   abstract = {The amount of data extracted from production processes has increased exponentially due to the proliferation of sensing technologies. When processed and analyzed, data can bring out valuable information and knowledge from manufacturing process, production system and equipment. In industries, equipment maintenance is an important key, and affects the operation time of equipment and its efficiency. Thus, equipment faults need to be identified and solved, avoiding shutdown in the production processes. Machine Learning (ML) methods have been emerged as a promising tool in Predictive Maintenance (PdM) applications to prevent failures in equipment that make up the production lines in the factory floor. However, the performance of PdM applications depends on the appropriate choice of the ML method. The aim of this paper is to present a systematic literature review of ML methods applied to PdM, showing which are being explored in this field and the performance of the current state-of-the-art ML techniques. This review focuses on two scientific databases and provides a useful foundation on the ML techniques, their main results, challenges and opportunities, as well as it supports new research works in the PdM field.},
   author = {Thyago P. Carvalho and Fabrízzio A.A.M.N. Soares and Roberto Vita and Roberto da P. Francisco and João P. Basto and Symone G.S. Alcalá},
   doi = {10.1016/J.CIE.2019.106024},
   issn = {0360-8352},
   journal = {Computers & Industrial Engineering},
   keywords = {Artificial intelligence,Machine learning,PdM,Predictive maintenance,Systematic literature review},
   month = {11},
   pages = {106024},
   publisher = {Pergamon},
   title = {A systematic literature review of machine learning methods applied to predictive maintenance},
   volume = {137},
   year = {2019},
}
@article{Jardine2006,
   abstract = {Condition-based maintenance (CBM) is a maintenance program that recommends maintenance decisions based on the information collected through condition monitoring. It consists of three main steps: data acquisition, data processing and maintenance decision-making. Diagnostics and prognostics are two important aspects of a CBM program. Research in the CBM area grows rapidly. Hundreds of papers in this area, including theory and practical applications, appear every year in academic journals, conference proceedings and technical reports. This paper attempts to summarise and review the recent research and developments in diagnostics and prognostics of mechanical systems implementing CBM with emphasis on models, algorithms and technologies for data processing and maintenance decision-making. Realising the increasing trend of using multiple sensors in condition monitoring, the authors also discuss different techniques for multiple sensor data fusion. The paper concludes with a brief discussion on current practices and possible future trends of CBM. © 2005 Elsevier Ltd. All rights reserved.},
   author = {Andrew K.S. Jardine and Daming Lin and Dragan Banjevic},
   doi = {10.1016/J.YMSSP.2005.09.012},
   issn = {0888-3270},
   issue = {7},
   journal = {Mechanical Systems and Signal Processing},
   keywords = {Condition monitoring,Condition-based maintenance,Diagnostics,Prognostics,Sensor data fusion,Signal processing},
   month = {10},
   pages = {1483-1510},
   publisher = {Academic Press},
   title = {A review on machinery diagnostics and prognostics implementing condition-based maintenance},
   volume = {20},
   year = {2006},
}
@article{SoA,
   abstract = {Condition-based maintenance techniques for industrial equipment and processes are described in this paper together with examples of their use and discussion of their benefits. These techniques are divided here into three categories. The first category uses signals from existing process sensors, such as resistance temperature detectors (RTDs), thermocouples, or pressure transmitters, to help verify the performance of the sensors and process-to-sensor interfaces and also to identify problems in the process. The second category depends on signals from test sensors (e.g., accelerometers) that are installed on plant equipment (e.g., rotating machinery) in order to measure such parameters as vibration amplitude. The vibration amplitude is then trended to identify the onset of degradation or failure. This second category also includes the use of wireless sensors to provide additional points for collection of data or allow plants to measure multiple parameters to cover not only vibration amplitude but also ambient temperature, pressure, humidity, etc. With each additional parameter that can be measured and correlated with equipment condition, the diagnostic capabilities of the category can increase exponentially. The first and second categories just mentioned are passive, which means that they do not involve any perturbation of the equipment or the process being monitored. In contrast, the third category is active. That is, the third category involves injecting a test signal into the equipment (sensors, cables, etc.) to measure its response and thereby diagnose its performance. For example, the response time of temperature sensors (RTDs and thermocouples) can be measured by the application of the step current signal to the sensor and analysis of the sensor response to the application of the step current. Cable anomalies can be located by a similar procedure referred to as the time domain reflectometry (TDR). This test involves a signal that is sent through the cable to the end device. Its reflection is then recorded and compared to a baseline to identify impedance changes along the cable and thereby identify and locate anomalies. Combined with measurement of cable inductance (L), capacitance (C), and loop resistance (R), or LCR testing, the TDR method can identify and locate anomalies along a cable, identify moisture in a cable or end device, and even reveal gross problems in the cable insulation material. There are also frequency domain reflectometry (FDR) methods, reverse TDR, trending of insulation resistance (IR) measurement, and other techniques which can be used in addition to or instead of TDR and LCR to provide a wide spectrum of tools for cable condition monitoring. The three categories of techniques described in this paper are the subject of current research and development projects conducted by the author and his colleagues at the AMS Corporation with funding from the U.S. Department of Energy (DOE) under the Small Business Innovation Research (SBIR) program. © 2010 IEEE.},
   author = {H. M. Hashemian},
   doi = {10.1109/TIM.2010.2047662},
   issn = {00189456},
   issue = {1},
   journal = {IEEE Transactions on Instrumentation and Measurement},
   keywords = {LCR testing,loop current step response (LCSR) method,predictive maintenance,time domain reflectrometry (TDR) test,wireless sensor},
   month = {1},
   pages = {226-236},
   title = {State-of-the-art predictive maintenance techniques},
   volume = {60},
   url = {https://ieeexplore.ieee.org/document/5659691},
   year = {2011},
}
@report{handsOn,
   author = {Boston Farnham and Sebastopol Tokyo and Beijing Boston and Farnham Sebastopol and Tokyo Beijing},
   title = {Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow Concepts, Tools, and Techniques to Build Intelligent Systems SECOND EDITION},
}
@web_page{sklearn,
   title = {Scikit-learn: Machine Learning in Python},
   url = {https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html},
}
@web_page{dimensionalityTechniques,
   title = {Dimensionality Reduction Techniques | Python},
   url = {https://www.analyticsvidhya.com/blog/2018/08/dimensionality-reduction-techniques-python/},
}
@web_page{fiixsoftware,
   title = {What is Predictive Maintenance? [Benefits & Examples] | Fiix},
   url = {https://www.fiixsoftware.com/maintenance-strategies/predictive-maintenance/},
}
@web_page{govtAutomation,
   month = {9},
   title = {£300 million to boost UK manufacturing productivity by 30%},
   url = {https://www.gov.uk/government/news/300-million-to-boost-uk-manufacturing-productivity-by-30},
   year = {2020},
}
@article{automationLevelAero,
   abstract = {Composites have become the go-to material of the aerospace industry during the past decades and a significant uptake in composite materials for aerospace applications was evident in recent years. Both expert academics and industry practitioners believe, to meet the future demand, the level of automation in the aerospace composite manufacturing process chains must be improved. The main focus of automation in composites so far has been given to automate siloed operations but limited attention has been paid to end-to-end integration of the process chains leading to inefficiencies, rising operational costs, and low productivity. This paper intends to compare and contrast the level of automation (LOA) in different aerospace composite manufacturing process chains to identify where the LOA triumphs and lacks. For this purpose, core-process and sub-process tasks involved in commonly used manufacturing process chains (i.e. Filament Winding, Automated Tape Layup, Automated Fiber Placement, Resin Transfer Molding, and Pultrusion) are identified by conducting a detailed literature review and verified by the experts. Then, the process chains are mapped and visualized to understand the workflow. Later, these tasks are evaluated based on an established LOA taxonomy developed for manufacturing processes. The study reveals that even the popular ‘automated’ processes are developed in silos and do not show consistent higher LOA throughout their process chain. While core-process tasks show intermediate LOA (Level 5–6), most non-value-added activities show poor LOA (Level 1–4). Most importantly, none of the tasks involved in the existing composite manufacturing process chains have reached a higher LOA (Level 7). The paper reveals that focusing on sub-process tasks, and tasks that lack automation should be the next step towards achieving fully automated composite manufacturing and presents a two-pronged approach to realize Industry 4.0.},
   author = {Deepesh Jayasekara and Nai Yeen Gavin Lai and Kok Hoong Wong and Kulwant Pawar and Yingdan Zhu},
   doi = {10.1016/J.JMSY.2021.10.015},
   issn = {0278-6125},
   journal = {Journal of Manufacturing Systems},
   keywords = {Aerospace,Composite manufacturing,End-to-end integration,Industry 4.0,Level of automation},
   month = {1},
   pages = {44-61},
   publisher = {Elsevier},
   title = {Level of automation (LOA) in aerospace composite manufacturing: Present status and future directions towards industry 4.0},
   volume = {62},
   year = {2022},
}
@web_page{manufacturingAutomation,
   title = {Automation in manufacturing: What you need to know},
   url = {https://roboticsandautomationnews.com/2021/09/10/automation-in-manufacturing-what-you-need-to-know/46243/},
}
@article{cuttingInvestigation,
   abstract = {Knowing the stringent operating conditions to which superalloys are subjected to in automobile, aerospace and gas turbine industries, their efficient machining and generation of machined surfaces with high integrity assumes a lot of importance. Therefore, this paper presents an experimental investigation into the effect of various process and tool-dependent parameters on cutting forces, an indirect measure of machined surface integrity besides a detailed microstructural analysis of the machined surface damage, in high-speed machining of superalloy Inconel 718. Accordingly, the effect of cutting speed, feed rate, depth of cut and tool cutting edge geometry on cutting forces, surface roughness and surface damage in high-speed turning of Inconel 718 using PCBN tools has been discussed. The input parameters were varied as: V = 125-475 m min-1, f = 0.05-0.15 mm rev-1, d = 0.50-1.0 mm and edge geometry as: 30° chamfer, 20° chamfer and 30° chamfer plus honed. The results show that the radial and feed force components are almost equal and the main cutting force component is two to three times that of feed and radial force components. The honed plus chamfered cutting edge was influential in reducing cutting forces significantly. It was noted that specimens showing larger cutting forces generated poor surface finish as well as extensive surface damage. © 2007 Elsevier B.V. All rights reserved.},
   author = {R. S. Pawade and Suhas S. Joshi and P. K. Brahmankar and M. Rahman},
   doi = {10.1016/j.jmatprotec.2007.04.049},
   issn = {09240136},
   journal = {Journal of Materials Processing Technology},
   title = {An investigation of cutting forces and surface damage in high-speed turning of Inconel 718},
   volume = {192-193},
   year = {2007},
}
@web_page{flightTest,
   title = {Flight Test},
   url = {https://www.aerosociety.com/get-involved/specialist-groups/engineering-design/flight-test/},
}
@report{F111transonic,
   author = {Edward L Friend and Glenn M Sakamoto},
   title = {Flight Comparison of the Transonic Agility of the F-111A Airplane and the F-111 Supercritical Wing Airplane},
   year = {1978},
}
@web_page{aeroToolbox,
   title = {Aircraft Wing Area and Aspect Ratio | AeroToolbox},
   url = {https://aerotoolbox.com/intro-wing-design/},
}
@report{lecture4,
   author = {Mila Mihaylova},
   title = {Aircraft Static Longitudinal Stability-Part I},
   year = {2022},
}
@report{aircraftFlight,
   author = {R H Barnard and D R Philpott},
   title = {Aircraft Flight Aircraft Flight Aircraft Flight},
   url = {www.pearsoned.co.uk},
   year = {2010},
}
@report{performanceBook,
   author = {Russel J B},
   title = {Performance and Stability of Aircraft},
   year = {1996},
}
@report{StaticTutorial,
   author = {Lyudmila Mihaylova},
   title = {Problems on Longitudinal Static Stability of Aircraft},
   year = {2022},
}
@report{F111MAW,
   author = {John W Smith and Wilton P Lock and Gordon A Payne},
   journal = {NASA Technical Memorandum},
   title = {Variable-Camber Systems Integration and Operational Performance of the AFTI/F-111 Mission Adaptive Wing},
   volume = {4370},
   year = {1992},
}
@article{F111CS,
   author = {Richard R Larson},
   issue = {88267},
   journal = {NASA Technical Memorandum },
   title = {F111 control system and redundancy},
   year = {1987},
}
@web_page{,
   title = {General Dynamics F-111 Aardvark},
   url = {https://www.militaryfactory.com/aircraft/detail.php?aircraft_id=72},
}
@web_page{,
   title = {Factsheets : General Dynamics F-111D to F},
   url = {https://web.archive.org/web/20100531135621/http://www.nationalmuseum.af.mil/factsheets/factsheet.asp?id=2322},
}
@report{flightHandbook,
   author = {Lyudmila Mihaylova},
   title = {Yorkshire Aero Club AER324 Flight Dynamics & Control in Collaboration with},
   year = {2022},
}
@article{Priyadarshan2021,
   abstract = {The present research paper deals with AlSi10Mg alloy/MWCNT metal matrix composite brake pads with varying weight percentages. The Development of new brake pad materials has been done at compacting load (10 Tons) and sintering temperature (450 °C) using the powder metallurgy process. The input parameters of 40 N normal load, 500 rpm, 150 °C pin temperature as the near-optimal combination of parameters for minimum wear and maximum coefficient of friction compared to other test conditions have been observed by the design of experiment (DOE). The overall average percentage error in the output against experiment output is less than 1%. Analysis of variance (ANOVA) indicates that load is a most significant factor than speed and temperature for wear and CoF. Artificial Neural Network (ANN) is used to develop a prediction model to calculate wear and coefficient of friction for different loads, pin heating temperature and speed. The model developed shows a strong correlation with experimental output. The experimental and predictive model developed from artificial neural network are strongly correlated with a correlation factor of 0.99447 for the training algorithm of Levenberg-Marquardt technique. ANN prediction model and Taguchi L16 array reveal that the experimental and predicted data for mass loss and CoF have less than 3% and 4% error, respectively. The closeness between the artificial neural network and experiment results enhances the scope of ANN for predicting the wear of materials. The model will help engineers to predict the failure of components with reference to running time and can be applied in automobile or manufacturing sectors to study wear, thus saving their time and cost in carrying out experiments.},
   author = {Santosh Kumar Priyadarshan and Subrata Kumar Ghosh},
   doi = {10.1016/j.matchemphys.2021.125136},
   issn = {02540584},
   journal = {Materials Chemistry and Physics},
   keywords = {Artificial neural network,Brake pad material,Coefficient of friction,Wear},
   month = {11},
   publisher = {Elsevier Ltd},
   title = {Statistical and artificial neural network technique for prediction of performance in AlSi10Mg-MWCNT based composite materials},
   volume = {273},
   year = {2021},
}
@article{Mumali2022,
   abstract = {The use of artificial neural network models to enrich the analytical and predictive capabilities of decision support systems in manufacturing has increased. The growing complexity and uncertainty in the manufacturing sector demand improved decision-making to ensure low operations costs, high productivity, and sustainable use of resources. Artificial neural networks have the inherent capacity to analyze the most uncertain and complex patterns in unstructured decision problems. This review aims to synthesize and provide a comprehensive summary of recent studies on artificial neural network-based decision support systems as applied in manufacturing processes. First, the specific processes in manufacturing where artificial neural network-based decision support systems are used are analyzed. A total of 99 multi-disciplinary publications on artificial neural network-based decision support systems published between 2011 and 2021 are retrieved and processed following a rigorous execution of the designated acceptance criteria and quality assessment. A review of the selected studies indicates a growing interest in applying artificial neural networks in decision support systems. Product and process design, performance evaluation, and predictive maintenance are the main application areas identified. A growing tendency to combine artificial neural network models with other intelligent tools, notably fuzzy logic, and genetic algorithm, is noted to overcome drawbacks such as slow convergence when training the algorithms. Further research should extend to other tools for enriching the performance of artificial neural networks in manufacturing processes.},
   author = {Fredrick Mumali},
   doi = {10.1016/j.cie.2022.107964},
   issn = {03608352},
   journal = {Computers and Industrial Engineering},
   keywords = {Artificial neural networks,Decision support systems,Intelligent decision support,Manufacturing,Systematic literature review},
   month = {3},
   publisher = {Elsevier Ltd},
   title = {Artificial neural network-based decision support systems in manufacturing processes: A systematic literature review},
   volume = {165},
   year = {2022},
}
@article{TCMApproach,
   abstract = {Manufacturing has changed markedly in recent years. The trend is for saving on the cost of production because of market pressure. In order to achieve this goal, greater consideration has been given to automation in manufacturing. In this regard, a fundamental step is to know the condition of the cutting tool, which requires a reliable system to monitor the condition of the tool. Experimental investigation of cutting tool wear and a model for tool wear estimation is reported in the current paper. The changes in the values of cutting forces, vibrations, and acoustic emissions with cutting tool wear are recorded and analysed. On the basis of experimental results a model is developed for tool wear estimation in turning operations using an adaptive neuro fuzzy inference system (ANFIS). Acoustic emission (ring down count), vibrations (acceleration), and cutting forces, along with time, have been used to formulate the model. This model is capable of estimating the wear rate of the cutting tool. The wear estimation results obtained by the model are compared with the practical results and are presented. The model performed quite satisfactorily and gave good results with the actual and predicted tool wear values. © IMechE 2007.},
   author = {V. S. Sharma and S. K. Sharma and A. K. Sharma},
   doi = {10.1243/09544054JEM765},
   issn = {09544054},
   issue = {4},
   journal = {Proceedings of the Institution of Mechanical Engineers, Part B: Journal of Engineering Manufacture},
   keywords = {Adaptive neuro fuzzy inference system,Fuzzy logic,Tool wear},
   pages = {635-646},
   title = {An approach for condition monitoring of a turning tool},
   volume = {221},
   year = {2007},
}
@article{Tseng2002,
   abstract = {The main topics discussed in this paper include sensor integration, data extraction, data processing, monitoring the cutting tool, safety of the tool machinery, and quality of the components in processing. The detection method used in this paper is to extract the workload of a spindle motor from a CNC controller, and then transmit the data via a I/O card for further processing. The computer is connected to the CNC by DNC and is able to detect abnormal conditions and transmit, through DNC, to CNC the NC program to stop the machine or to replace the cutting tool. The systematic architectural instrument develops tools with object-oriented professional software and establishes software structure using a visual component library. The software component structure is made easy for maintaining and extending programs and for the operating system with its graphics user interface. © 2001 Elsevier Science Ltd. All rights reserved.},
   author = {P. C. Tseng and A. Chou},
   doi = {10.1016/S0890-6955(01)00091-8},
   issn = {08906955},
   issue = {1},
   journal = {International Journal of Machine Tools and Manufacture},
   keywords = {Cutting tool,Monitoring,Quality processing,Sensor integration},
   month = {1},
   pages = {89-97},
   title = {The intelligent on-line monitoring of end milling},
   volume = {42},
   year = {2002},
}
@article{hobbing,
   abstract = {Intelligent monitoring and diagnosis of tool status are of great significance for improving the manufacturing efficiency and accuracy of the workpiece. It is difficult to quickly and accurately predict the wear state of worm gear hob under different working conditions. This paper proposes a novel approach to predict hob wear status based on CNC real-time monitoring data. Based on the open platform communication unified architecture (OPC UA) technology and orthogonal test, the machine data of motor power, current, etc. related to tool wear are collected online in the worm gear machining process. And then, an improved deep belief network (DBN) is used to generate a tool wear model by training data. A growing DBN with transfer learning is introduced to automatically decide its best model structure, which can accelerate its learning process, improve training efficiency and model performance. The experiment results show that the proposed method can effectively predict hob wear status under multi-cutting conditions. To show the advantages of the proposed approach, the performance of the DBN is compared with the traditional back propagation neural network (BP) method in terms of the mean-squared error (MSE). The compared results show that this tool wear prediction method has better prediction accuracy than the traditional BP method during worm gear hobbing.},
   author = {Dashuang Wang and Rongjing Hong and Xiaochuan Lin},
   doi = {10.1016/j.precisioneng.2021.08.010},
   issn = {01416359},
   journal = {Precision Engineering},
   keywords = {Deep belief network,OPC UA,Tool wear,Transfer learning,Worm gear hobbing},
   month = {11},
   pages = {847-857},
   publisher = {Elsevier Inc.},
   title = {A method for predicting hobbing tool wear based on CNC real-time monitoring data and deep learning},
   volume = {72},
   year = {2021},
}
@article{Scheffer2003,
   abstract = {This paper describes an in-depth study on the development of a system for monitoring tool wear in hard turning. Hard turning is used in the manufacturing industry as an economic alternative to grinding, but the reliability of hard turning processes is often unpredictable. One of the main factors affecting the reliability of hard turning is tool wear. Conventional wear-monitoring systems for turning operations cannot be used for monitoring tools used in hard turning because a conglomeration of phenomena, such as chip formation, tool wear and surface finish during hard turning, exhibits unique behavior not found in regular turning operations. In this study, various aspects associated with hard turning were investigated with the aim of designing an accurate tool wear-monitoring system for hard turning. The findings of the investigation showed that the best method to monitor tool wear during hard turning would be by means of force-based monitoring with an Artificial Intelligence (AI) model. The novel formulation of the proposed AI model enables it to provide an accurate solution for monitoring crater and flank wear during hard turning. The suggested wear-monitoring system is simple and flexible enough for online implementation, which will allow more reliable hard turning in industry. © 2003 Elsevier Science Ltd. All rights reserved.},
   author = {C. Scheffer and H. Kratz and P. S. Heyns and F. Klocke},
   doi = {10.1016/S0890-6955(03)00110-X},
   issn = {08906955},
   issue = {10},
   journal = {International Journal of Machine Tools and Manufacture},
   keywords = {Cutting forces,Hard turning,Neural networks,Process monitoring,Tool wear},
   month = {8},
   pages = {973-985},
   title = {Development of a tool wear-monitoring system for hard turning},
   volume = {43},
   year = {2003},
}
@article{TCMAI,
   abstract = {This paper describes an application of three artificial intelligence (AI) methods to estimate tool wear in lathe turning. The first two are "conventional" AI methods - the feed forward back propagation neural network and the fuzzy decision support system. The third is a new artificial neural network based-fuzzy inference system with moving consequents in if-then rules. Tool wear estimation is based on the measurement of cutting force components. This paper discusses a comparison of usability of these methods in practice. © 2002 Elsevier Science Ltd. All rights reserved.},
   author = {Marek Balazinski and Ernest Czogala and Krzysztof Jemielniak and Jacek Leski},
   doi = {10.1016/S0952-1976(02)00004-0},
   issn = {09521976},
   issue = {1},
   journal = {Engineering Applications of Artificial Intelligence},
   keywords = {Artificial intelligence,Cutting force,Tool monitoring},
   month = {2},
   pages = {73-80},
   title = {Tool condition monitoring using artificial intelligence methods},
   volume = {15},
   year = {2002},
}
@article{forceTorque,
   abstract = {In a modern machining system, tool condition monitoring systems are needed to get higher quality production and to prevent the downtime of machine tools due to catastrophic tool failures. Also, in precision machining processes surface quality of the manufactured part can be related to the conditions of the cutting tools. This increases industrial interest for in-process tool condition monitoring (TCM) systems. TCM supported modern unmanned manufacturing process is an integrated system composed of sensors, signal processing interface and intelligent decision making strategies. This study includes key considerations for development of an online TCM system for milling of Inconel 718 superalloy. An effective and efficient strategy based on artificial neural networks (ANN) is presented to estimate tool flank wear. ANN based decision making model was trained by using real time acquired three axis (Fx, Fy, Fz) cutting force and torque (Mz) signals and also with cutting conditions and time. The presented ANN model demonstrated a very good statistical performance with a high correlation and extremely low error ratio between the actual and predicted values of flank wear. © 2010 Elsevier Ltd. All rights reserved.},
   author = {Bulent Kaya and Cuneyt Oysu and Huseyin M. Ertunc},
   doi = {10.1016/J.ADVENGSOFT.2010.12.002},
   issn = {0965-9978},
   issue = {3},
   journal = {Advances in Engineering Software},
   keywords = {Cutting forces,Milling,Neural networks,On-line monitoring,Torque,Wear},
   month = {3},
   pages = {76-84},
   publisher = {Elsevier},
   title = {Force-torque based on-line tool wear estimation system for CNC milling of Inconel 718 using neural networks},
   volume = {42},
   year = {2011},
}
@article{wearingToolsNN,
   abstract = {The on-line supervision of a tool's wear is the most difficult task in the context of tool monitoring. The influence of the wear's tool on the created surface is presented. Based on in-process acquisition of digital image of the surface it is possible to estimate or classify wear parameters by means of neural networks.},
   author = {Anna Zawada-Tomkiewicz},
   doi = {10.1016/S0924-0136(00)00815-3},
   issn = {09240136},
   issue = {3},
   journal = {Journal of Materials Processing Technology},
   month = {2},
   pages = {300-304},
   publisher = {Elsevier Science S.A.},
   title = {Classifying the wear of turning tools with neural networks},
   volume = {109},
   year = {2001},
}
@article{Shah2021,
   abstract = {During the past decade, the development of offshore wind energy has transitioned from near shore with shallow water to offshore middle-depth water regions. Consequently, the energy conversion technology has shifted from bottom-fixed wind turbines to floating offshore wind turbines. Floating offshore wind turbines are considered more suitable, but their cost is still very high. One of the main reasons for this is that the system dynamics control method is not well-adapted, thereby affecting the performance and reliability of the wind turbine system. The additional motion of the platform tends to compromise the system's performance in terms of power maximization, power regulation, and load mitigation. To provide a recommendation based on the advantages and disadvantages of different control methods, we systematically analyze feasible control methods for existing floating offshore wind turbine designs. Based on a brief overview of floating offshore wind turbine system dynamics, we present several promising control methods by classifying them as blade-pitch-based and mass–spring–damper-based. Furthermore, we emphasize on the incoming wind and wave forecasting associated with the control methods. We then compare different methods by evaluating a matrix involving platform motion minimization, load mitigation, and power regulation and identify the advantages and disadvantages. Finally, recommendations and suggestions for further research are provided by integrating the advantageous control algorithm and forecasting technologies to reduce costs.},
   author = {Kamran Ali Shah and Fantai Meng and Ye Li and Ryozo Nagamune and Yarong Zhou and Zhengru Ren and Zhiyu Jiang},
   doi = {10.1016/J.RSER.2021.111525},
   issn = {1364-0321},
   journal = {Renewable and Sustainable Energy Reviews},
   keywords = {Cost reduction,Floating offshore wind turbines,Floating platforms,System dynamics,Wind power,Wind turbine control},
   month = {11},
   pages = {111525},
   publisher = {Pergamon},
   title = {A synthesis of feasible control methods for floating offshore wind turbine system dynamics},
   volume = {151},
   year = {2021},
}
@book_section{momp,
   abstract = {The Twin-Control product takes a holistic approach to modelling and simulation of machining processes, incorporating machine controller, machine structure, part program, part geometry, and process forces and dynamics into a single system. The focus of this chapter is on the process models used to simulate cutting forces, torques, form error, and tool dynamics throughout a part program. These predictions provide process planners with valuable information about an operation before it is executed on machine, allowing for potential issues to be identified and reduced or eliminated before the first part is produced.},
   author = {Luke Berglind and Erdem Ozturk},
   doi = {10.1007/978-3-030-02203-7_4},
   journal = {Twin-Control},
   pages = {57-93},
   publisher = {Springer International Publishing},
   title = {Modelling of Machining Processes},
   year = {2019},
}
@report{analyticalmodel,
   author = {Snahungshu Sikder},
   title = {ANALYTICAL MODEL FOR FORCE PREDICTION WHEN MACHINING METAL MATRIX COMPOSITES},
}
@article{oneminther,
   abstract = {Determination of the temperatures during machining is one of the most important challenges for accurate milling simulations. Coupled with excessive shearing, plastic deformation and friction in a small region of cutting, the temperatures in milling may have very significant impact on parts and tools such as dimensional errors, residual stresses and tool wear. Temperature exhibits a non-linear complex-modelling problem in milling process. In this article, for the first time, a novel thermal modelling is introduced for fast and accurate prediction of temperatures in end milling processes. A theoretical modelling approach and experimental validations are presented for various cutting conditions. © 2014 CIRP.},
   author = {Ismail Lazoglu and Bircan Bugdayci},
   doi = {10.1016/J.CIRP.2014.03.072},
   issn = {0007-8506},
   issue = {1},
   journal = {CIRP Annals},
   keywords = {Milling,Modelling,Temperature},
   month = {1},
   pages = {113-116},
   publisher = {Elsevier},
   title = {Thermal modelling of end milling},
   volume = {63},
   year = {2014},
}
@article{thermalmodelling,
   abstract = {Machine tools equipped with linear motors can achieve high feed speed as well as high accuracy. However, the direct feed drive system generates heat through power loss and friction. In combination with environmental influences such as machine shop climate, this can lead to a local deformation of the machine tool structure and induce a direct positioning error. This paper presents a thermal model, using the finite element method, to simulate the thermal behaviour of a high-speed cutting machining centre equipped with linear motors. This model considers the complex boundary conditions such as heat sources, contact and convective heat transfer. Transient changes in temperatures and deformations are allowed in the solution. The comparison of the experiments show that this model can predict the temperature distribution and positioning error under specified operating conditions very well. © 2012 German Academic Society for Production Engineering (WGP).},
   author = {Eckart Uhlmann and Jiangmin Hu},
   doi = {10.1007/s11740-012-0406-6},
   issn = {09446524},
   issue = {6},
   journal = {Production Engineering},
   keywords = {FEM,Machine tool,Thermal error},
   month = {12},
   pages = {603-610},
   title = {Thermal modelling of an HSC machining centre to predict thermal error of the feed system},
   volume = {6},
   year = {2012},
}
@article{Attanasio2009,
   abstract = {In this paper, an adopted abrasive-diffusive wear model is proposed and implemented into a 3D Finite Element code to study the tool wear phenomenon. In particular, the Authors found that FE procedure based only on diffusive mechanism shown some problems when the extension on crater area was investigated. This can be related to the absence of the wear abrasion term on the utilized model. Therefore, in this work, the Authors improved the previous utilized tool wear model introducing into the sub-routine the abrasive term on the basis of Usui's model. A series of 3D FEM simulations were conducted in order to estimate the tool wear development in turning operations. The adopted abrasive-diffusive wear model will give the possibility of correctly evaluating the tool wear of actual turning operations during both the initial transient phase, where the abrasive mechanism is dominant, and the steady-state phase, in which the diffusion is the main wear mechanism. The FEM results were compared with experimental data, obtained turning AISI 1045 steel with WIDIA P40 inserts, showing a satisfactory agreement. © Springer/ESAFORM 2009.},
   author = {A. Attanasio and D. Umbrello},
   doi = {10.1007/S12289-009-0475-Z},
   issn = {19606206},
   issue = {SUPPL. 1},
   journal = {International Journal of Material Forming},
   keywords = {3D FEM,Cutting,Tool wear},
   month = {12},
   pages = {543-546},
   title = {Abrasive and diffusive tool wear FEM simulation},
   volume = {2},
   year = {2009},
}
@article{advprocess,
   abstract = {During the last few decades, there has been significant progress in developing industry-driven predictive models for machining operations. This paper presents the state-of-the-art in predictive performance models for machining, and identifies the strengths and weaknesses of current models. This includes a critical assessment of the relevant modelling techniques and their applicability and/or limitations for the prediction of the complex machining operations performed in industry. This paper includes contributions from academia and industry, and is expected to serve as a comprehensive report of recent progress, as well as a roadmap for future directions. Process models often target the prediction of fundamental variables such as stresses, strains, strain-rates, temperatures etc. However, to be useful to industry, these variables must be correlated to performance measures: product quality (accuracy, dimensional tolerances, finish, etc.), surface and subsurface integrity, tool-wear, chip-form/breakability, burr formation, machine stability, etc. The adoption of machining models by industry critically depends on the capability of a model to make this link and predict machining performance. Therefore, this paper would identify and discuss several key research topics closely associated with predictive model development for machining operations, primarily targeting industry applications. © 2013 CIRP.},
   author = {P. J. Arrazola and T. Özel and D. Umbrello and M. Davies and I. S. Jawahir},
   doi = {10.1016/J.CIRP.2013.05.006},
   issn = {0007-8506},
   issue = {2},
   journal = {CIRP Annals},
   keywords = {Chip formation,Machining,Modelling},
   month = {1},
   pages = {695-718},
   publisher = {Elsevier},
   title = {Recent advances in modelling of metal machining processes},
   volume = {62},
   year = {2013},
}
@report{bliskman,
   author = {A. Callejaa and H. Gonzálezb and R. Polvorosac and G. Gómezb and I. Ayestac and M. Bartond and L.N. López de Lacallec},
   note = {<br/>},
   title = {Blisk blades manufacturing technologies analysis},
   url = {www.sciencedirect.com},
   year = {2019},
}
@article{superhardlife,
   abstract = {The hard turning process is generally performed by PCBN or mixed ceramics tools, which have the mechanical properties that withstand the tribological conditions of the process imposed by the hardened machined material. Coated cemented carbides are also an option for hard machining processes, although relying on the coating deterioration. This paper aims to evaluate the tool wear rate in function of the cutting speed in the hard turning of the steel AISI 52100 with a hardness of 50 HRC for three different types of cutting tool materials: coated cemented carbide, mixed ceramic, and PCBN. The methodology applied to assess the tool wear is based on three-dimensional parameters (volumetric) obtained from a focus variation microscope (FVM). The tool wear rate (WRRM) is calculated based on ordinary least squares (OLS) adopting five values of WRM in five machining time intervals. The face turning experiments were performed at four cutting speeds: vc = 120, 150, 187.5, and 234 m/min. At vc = 120 m/min, the PCBN presented the lowest tool wear rate (182 μm3/s); at vc = 150 m/min, the coated cemented carbide (tool wear rate on the coating) had the best performance (417 μm3/s). The mixed ceramic tool presented a better performance at the higher cutting speeds of vc = 187.5 and 234 m/min, 1206 and 1878 μm3/s, respectively. The methodology applied was reliable to understand and discuss the performance of the machining process through the tool wear rate (WRRM), which is based on the volume of removed material from the tool (WRM). The three-dimensional tool wear parameters can also be applied to machining process optimization, cutting tool wear model creation, benchmarking, and development of new cutting tool materials and grades. Furthermore, the methodology can be considered more agile and precise when compared to the current industrial methodology of tool performance evaluation. Thus, this innovative methodology promotes important information for cutting tool manufacturers and for its customers such as automotive and aeronautic industry.},
   author = {Denis Boing and Leonardo Zilli and Carlos Ernani Fries and Rolf Bertrand Schroeter},
   doi = {10.1007/s00170-019-04295-9},
   issn = {14333015},
   issue = {9-12},
   journal = {International Journal of Advanced Manufacturing Technology},
   keywords = {Coated cemented carbide,Hard turning,Mixed ceramic,PCBN,Tool wear rate,Volumetric tool wear parameters},
   month = {10},
   pages = {4697-4704},
   publisher = {Springer London},
   title = {Tool wear rate of the PCBN, mixed ceramic, and coated cemented carbide in the hard turning of the AISI 52100 steel},
   volume = {104},
   year = {2019},
}
@article{ceramiclife,
   abstract = {The demand for increasing productivity when machining heat resistant alloys has resulted in the use of new tool materials such as cubic boron nitride (CBN) or ceramics. However, CBN tools are mostly used by the automotive industry in hard turning, and the wear of those tools is not sufficiently known in aerospace materials. In addition, the grade of these tools is not optimized for superalloys due to these being a small part of the market, although expanding (at 20% a year). So this investigation has been conducted to show which grade is optimal and what the wear mechanisms are during finishing operations of Inconel 718. It is shown that a low CBN content with a ceramic binder and small grains gives the best results. The wear mechanisms on the rake and flank faces were investigated. Through SEM observations and chemical analysis of the tested inserts, it is shown that the dominant wear mechanisms are adhesion and diffusion due to chemical affinity between elements from workpiece and insert. © 2006 Elsevier Ltd. All rights reserved.},
   author = {J. P. Costes and Y. Guillet and G. Poulachon and M. Dessoly},
   doi = {10.1016/J.IJMACHTOOLS.2006.09.031},
   issn = {0890-6955},
   issue = {7-8},
   journal = {International Journal of Machine Tools and Manufacture},
   keywords = {CBN,Diffusion,Inconel 718,Tool wear,Turning},
   month = {6},
   pages = {1081-1087},
   publisher = {Pergamon},
   title = {Tool-life and wear mechanisms of CBN tools in machining of Inconel 718},
   volume = {47},
   year = {2007},
}
@article{carbidelife,
   abstract = {Metal matrix composites (MMCs) are materials which have been widely used in the aerospace and automobile industries since the 1980s and have been classified as hard-to-machine materials. During the intervening years, only a limited amount of research has been conducted into the cutting action of MMCs. As with traditional materials, it is important to understand the wear mechanisms that contribute to tool wear which reduces tool life. The objective of this research is to evaluate the machinability characteristics for these hard-to-machine material MMCs. This review will also establish the optimum machining parameters vital to maximizing tool life whilst producing parts at the desired quantity and quality.},
   author = {Carl J. Nicholls and Brian Boswell and Ian J. Davies and M. N. Islam},
   doi = {10.1007/s00170-016-9558-4},
   issn = {14333015},
   issue = {9-12},
   journal = {International Journal of Advanced Manufacturing Technology},
   keywords = {Hard-to-machine material,Machining parameters,Metal matrix composites,Tool life,Wear mechanisms},
   month = {6},
   pages = {2429-2441},
   publisher = {Springer London},
   title = {Review of machining metal matrix composites},
   volume = {90},
   year = {2017},
}
@article{pcbninconel,
   abstract = {Inconel 718 is a Ni superalloy widely used in high responsibility components requiring excellent mechanical properties at high temperature and elevated corrosion resistance. Inconel 718 is a difficult to cut material due to the elevated temperature generated during cutting, its low thermal conductivity, and the strong abrasive tool wear during cutting process. Finishing operations should ensure surface integrity of the component commonly requiring the use of hard metal tools with sharp tool edges and moderate cutting speeds. Polycrystalline cubic boron nitride (PCBN) tools recently developed an enhanced toughness suitable for these final operations. This paper focuses on the study of PCBN tools performance in finishing turning of Inconel 718. Several inserts representative of different manufacturers were tested and compared to a reference carbide tool. The evolution of tool wear, surface roughness, and cutting forces was analyzed and discussed. PCBN tools demonstrated their suitability for finishing operations, presenting reasonable removal rates and surface quality.},
   author = {José Díaz-álvarez and Víctor Criado and Henar Miguélez and José Luis Cantero},
   doi = {10.3390/met8080582},
   issn = {20754701},
   issue = {8},
   journal = {Metals},
   keywords = {Finishing turning,Inconel 718,PCBN tools,Tool wear},
   month = {8},
   publisher = {MDPI AG},
   title = {PCBN performance in high speed finishing turning of inconel 718},
   volume = {8},
   year = {2018},
}
@report{toolspeeds,
   author = {KeithM},
   title = {INTERNAL TOOL INC. ITI Carbide Cutting Tools SPEEDS AND FEEDS FOR CARBIDE ENDMILLS},
   url = {http://internaltool.com/docs/reference/speeds-and-feeds.pdf},
}
@book{machiningbook2,
   author = {David A Stephenson and John S Agapiou},
   publisher = {CRC press},
   title = {Metal cutting theory and practice},
   year = {2018},
}
@book{machiningbook1,
   author = {AB Chattopadhyay},
   publisher = {John Wiley \& Sons},
   title = {Machining and machine tools},
   year = {2011},
}
@book_section{superhard,
   abstract = {Materials with superior hardness can be categorized as ultrahard (Vickers hardness, Hv ≥ 80 GPa) and superhard (Hv ≥ 40 GPa). These materials are commonly used as cutting tools and abrasives in the machining and manufacturing industries. With its extreme hardness, diamond is the best known and most used ultrahard material for industrial applications. However, it is ineffective at cutting and drilling ferrous alloys due to diamond's high reactivity with iron and poor thermal stability in air. Additionally, the synthesis of diamond requires both high pressure (HP) and high temperature, making it an expensive process. These limitations have driven the search for alternative superhard materials that are capable of cutting steels and other materials at lower costs. This article reviews the concept of hardness and summarizes advancements in the synthesis and mechanical properties of hard materials. It begins with a review of methods to measure hardness, adding HP diffraction methods to more conventional hardness measurements. It then considers new ultrahard materials that exist within the B–C–N ternary system, with hardness approaching diamond but improved chemical stability. Finally, it surveys superhard nitrides, oxides, and borides as potential alternative materials, focusing on transition metal boride systems where the synthesis can be readily achieved at ambient pressure and scaled for industrial applications. We hope that this article serves as an overview of hard materials and guide for the comparison of data reported in the literature.},
   author = {Lisa E. Pangilinan and Shanlin Hu and Georgiy Akopov and Sabina C. Cabrera and Michael T. Yeung and Reza Mohammadi and Sarah H. Tolbert and Richard B. Kaner},
   doi = {10.1002/9781119951438.eibc2076.pub2},
   journal = {Encyclopedia of Inorganic and Bioinorganic Chemistry},
   month = {3},
   publisher = {Wiley},
   title = {Superhard Materials: Advances in the Search and Synthesis of New Materials},
   year = {2021},
}
@web_page{aluminahardness,
   title = {Alumina | Ceramic | Al2O3 | Aluminon},
   url = {https://www.syalons.com/materials/alumina/},
}
@article{Kim2019,
   abstract = {Particulate SiC-reinforced aluminium (SiCp/Al) metal matrix composite (MMC) is ideal for various aerospace applications thanks to its improved thermo-mechanical properties. However, conventional machining of this material remains a challenge due to its high strength. A newly developed hybrid turning process was used to show tangible improvements in the machining rate and finish quality of the MMC. Experimental analysis on ultrasonically assisted turning (UAT) of SiCp/Al workpiece was performed and compared with conventional turning (CT). Minimum Quantity Lubrication (MQL) was also used in the experiment and added as an experimental parameter. Prior to the experiment, the microstructure of SiCp/Al sample was analysed. An experimental result showed improvement in UAT compared to CT for both cutting force and surface topography. A prospective 2D numerical model was also proposed, which can predict the response of conventional dry machining reasonably. The model can be used to design setups and determine optimized machining parameters to find a solution to current obstacles limiting MMC machining. The experimental cutting force result in CT was compared with the simulation.},
   author = {Jin Kim and Wei Bai and Anish Roy and Lewis C.R. Jones and Sabino Ayvar-Soberanis and Vadim V. Silberschmidt},
   doi = {10.1016/J.PROCIR.2019.04.162},
   issn = {2212-8271},
   journal = {Procedia CIRP},
   keywords = {Metal matrix composites,Minimum quantity lubrication,Non-conventional turning,Ultrasonic assisted machining},
   month = {1},
   pages = {184-189},
   publisher = {Elsevier},
   title = {Hybrid machining of metal-matrix composite},
   volume = {82},
   year = {2019},
}
@article{Sikder2012,
   abstract = {This paper presents an analytical force model to predict the forces generated during machining of metal matrix composites and investigates the effect of particle size on machining forces. Several aspects of the cutting mechanics such as shear force, ploughing force, and particle fracture force are considered to estimate the generated cutting forces. Chip formation force is obtained using the Johnson-Cook constitutive model. The ploughing force is formulated using the slip-line field theory, while the fracture force is calculated using Griffith Theory. The predicted results are compared to experimentally measured data under different conditions. The results show acceptable agreement between the theoretically predicted and experimentally measured cutting forces. © 2012 Elsevier Ltd. All rights reserved.},
   author = {S. Sikder and H. A. Kishawy},
   doi = {10.1016/J.IJMECSCI.2012.03.010},
   issn = {0020-7403},
   issue = {1},
   journal = {International Journal of Mechanical Sciences},
   keywords = {Cutting force,MMC,Modeling},
   month = {6},
   pages = {95-103},
   publisher = {Pergamon},
   title = {Analytical model for force prediction when machining metal matrix composite},
   volume = {59},
   year = {2012},
}
@generic{mec304-5,
   author = {Slatter Tom},
   city = {Sheffield},
   month = {10},
   publisher = {University of Sheffield},
   title = {Manufacturing Systems MEC304 Topic 05 Machining Process Modelling and Development},
   year = {2021},
}
@generic{mec304-6,
   author = {Slatter Tom},
   city = {Sheffield},
   month = {10},
   publisher = {University of Sheffield},
   title = {Manufacturing Systems MEC304 Topic 06 Machining Process Modelling and Development Case Study-Ceramic Tooling},
   year = {2021},
}
@web_page{ctemag,
   title = {Ceramic composite cutting tools: are they tough enough? | Cutting Tool Engineering},
   url = {https://www.ctemag.com/news/articles/ceramic-composite-cutting-tools-are-they-tough-enough#},
}
@article{Choudhury1998,
   abstract = {Nickel-base super alloys (Inconel) are generally known to be one of the most difficult materials to machine because of their high hardness, high strength at high temperature, affinity to react with the tool materials, and low thermal diffusivity. This paper presents a general review of their material characteristics and properties together with their machinability assessment when using different cutting tools. The advantages and disadvantages of different tool materials with regard to the machining Inconel are highlighted. © 1998 Elsevier Science S.A. All rights reserved.},
   author = {I. A. Choudhury and M. A. El-Baradie},
   doi = {10.1016/S0924-0136(97)00429-9},
   issn = {0924-0136},
   issue = {1-3},
   journal = {Journal of Materials Processing Technology},
   keywords = {Machinability,Nickel-base,Super alloys},
   month = {5},
   pages = {278-284},
   publisher = {Elsevier},
   title = {Machinability of nickel-base super alloys: a general review},
   volume = {77},
   year = {1998},
}
@article{Ramaswamy1972,
   abstract = {An electron microscope study has been made of precipitation in two commercial nickel-base superalloys. Alloy C263 is strengthened by the precipitation of fine dispersions of coherent γ', and M23C6 precipitation occurs at grain boundaries (M is predominantly Cr). Inconel 718 is strengthened by the precipitation of b.c.t. Ni3Nb (γ'') while overageing is associated with particle coarsening and with the formation of orthorhombic Ni3Nb. NbC precipitation occurs at grain boundaries. © 1972.},
   author = {V. Ramaswamy and P. R. Swann and D. R.F. West},
   doi = {10.1016/0022-5088(72)90100-2},
   issn = {0022-5088},
   issue = {1},
   journal = {Journal of the Less Common Metals},
   month = {4},
   pages = {17-26},
   publisher = {Elsevier},
   title = {Observations on intermetallic compound and carbide precipitation in two commercial nickel-base superalloys},
   volume = {27},
   year = {1972},
}
@web_page{inconel600,
   title = {Inconel 600® | Nickelvac® 600 | Ferrochronin® 600},
   url = {https://www.magellanmetals.com/inconel-600},
}
@web_page{inconel718,
   title = {Inconel 718® Nickel-Chromium Alloy | Inconel 718 Super Alloy},
   url = {https://www.magellanmetals.com/inconel-718},
}
@web_page{,
   title = {Inconel 600® | Nickelvac® 600 | Ferrochronin® 600},
   url = {https://www.magellanmetals.com/inconel-600},
}
@web_page{,
   title = {Inconel 718® Nickel-Chromium Alloy | Inconel 718 Super Alloy},
   url = {https://www.magellanmetals.com/inconel-718},
}
@web_page{inconel625,
   title = {Advanced Alloy Inconel 625® | Inconel 625® Alloy Material},
   url = {https://www.magellanmetals.com/inconel-625},
}
@web_page{inconelmain,
   title = {Inconel Super Alloys | Inconel Nickel-Chromium Alloys},
   url = {https://www.magellanmetals.com/inconel},
}
@web_page{airbusorders,
   title = {Airbus Orders Surge in August | The Motley Fool},
   url = {https://www.fool.com/investing/2021/09/12/airbus-orders-surge-in-august/},
}
@web_page{nickelstats,
   title = {Nickel Statistics and Information},
   url = {https://www.usgs.gov/centers/nmic/nickel-statistics-and-information},
}
@web_page{,
   title = {Machining difficult aerospace materials - Aerospace Manufacturing and Design},
   url = {https://www.aerospacemanufacturinganddesign.com/article/machining-difficult-aerospace-materials/},
}
@article{Slatter21,
   abstract = {Inconel 718 has found use in many demanding applications due to its high temperature fatigue strength, hardness and low thermal conductivity. These material properties present a challenge to productive and high quality (surface finish) machining and promote rapid tool wear. The work presented here describes the chip formation and wear mechanisms of silicon nitride (Si3N4) based ceramic SiAlON and silicon carbide whiskers reinforced alumina (WRA) (Al2O3 + SiCw) round (RNGN) inserts, when turning solution-annealed Inconel 718 (27<HRC<30) with a 10% concentration cutting fluid. Four sets of cutting trials were conducted at a cutting speed of 250 m/min and another four at 300 m/min. The results show that using SiAlON turning inserts at a cutting speed of 300 m/min delivered the best results in terms of tool flank wear, total tool life, and work-piece surface finish. The morphology of the Inconel 718 chip, at cutting speeds above 250 m/min, presents an intense shear band localisation in the primary shear zone of the chip/tool interface, which leads to chip segmentation.},
   author = {Luke Osmond and David Curtis and Tom Slatter},
   doi = {10.1016/J.WEAR.2021.204128},
   issn = {0043-1648},
   journal = {Wear},
   month = {12},
   pages = {204128},
   publisher = {Elsevier},
   title = {Chip formation and wear mechanisms of SiAlON and whisker-reinforced ceramics when turning Inconel 718},
   volume = {486-487},
   year = {2021},
}
@article{Degen2014,
   abstract = {In this paper a turning process is presented which is enhanced by a third tool movement axis. Besides the two already existing translational X- And Z-axes an additional rotational axis is added to the process. This is realized by integrating the already existing but yet unused B-axis of 5-axis turn-/mill-centers into the turning process. By moving the B-axis simultaneously during turning operations it is possible to adapt the tool/ workpiece engagement arbitrary to the individual machining case. Thus, the flexibility and the efficiency of turning can be increased significantly. This gives major benefits regarding tool consumption and machining time when turning geometrical complex parts made of hard to machine materials such as turbomachinery components. To gain a basic understanding of simultaneous three axis turning it is investigated which effect the additional tool movement has on the turning process regarding uncut chip parameters, process forces and tool wear development. Process parameters of the B-axis like rotational speed, rotational range and direction of rotation are varied. It is shown that the uncut chip geometry in three axis turning differs from the geometry in conventional turning. Especially the rotational speed and the direction of rotation have a major effect on the shape and size of the uncut chip. Besides this, the process forces are also affected by the B-axis movement. While the process forces are constant in conventional turning, these alter over time in three axis turning. It can be observed that the process forces are significantly shifted towards higher and lower levels in dependence of the rotational speed and direction. This effect increases with decreasing tool cutting edge angles. However, the influence of the B-axis movement on the tool wear is also investigated. It is shown that the tool wear can be distributed arbitrarily over the tool edge by changing the position of the B-axis. Thus, the tool life can be more than doubled.},
   author = {Florian Degen and Fritz Klocke and Thomas Bergs},
   issue = {C},
   journal = {Procedia CIRP},
   keywords = {B-axis turning, uncut chip geometry,Process forces,Simultaneous three axis turning,Tool wear distribution,Turning},
   note = {https://www.youtube.com/watch?v=XZqpgLopQ0g<br/><br/>},
   pages = {32-37},
   publisher = {Elsevier B.V.},
   title = {New production technologies in aerospace industry - 5th machining innovations conference (MIC 2014) presentation of a novel "simultaneous three axis turning" process for time and cost efficient machining of rotational symmetric turbomachinery components},
   volume = {24},
   year = {2014},
}
@article{Hosokawa2010,
   abstract = {Turning with a spinning insert called actively driven rotary tool (ADRT), where the cutting tool revolves by a powered and programmable spindle, is investigated from the thermal aspects. Dry and MQL external turning tests of austenitic stainless steel (AISI 304) and heat-resistant Ni-based alloy (Inconel 718) are carried out. The tool temperature at the flank face is measured using a newly assembled fiber-coupled two-color pyrometer. In dry turning of AISI 304 steel, the tool temperature decreases from approximately 730 °C to 640 °C as the tool rotation speed increases from 10 m/min to 200 m/min. © 2010 CIRP.},
   author = {A. Hosokawa and T. Ueda and R. Onishi and R. Tanaka and T. Furumoto},
   issue = {1},
   journal = {CIRP Annals - Manufacturing Technology},
   keywords = {Cutting tool,Temperature,Turning},
   note = {<br/>},
   pages = {89-92},
   title = {Turning of difficult-to-machine materials with actively driven rotary tool},
   volume = {59},
   year = {2010},
}
@article{Kishawy2004,
   abstract = {This paper presents an experimental investigation to evaluate the performance of self-propelled rotary tools and machined surface quality during machining waspaloy and titanium alloys. A new rotary tool design with different insert diameters and material/coatings is utilized. The criteria used in the assessment of tool performance are modes of tool wear and roughness as well as topography of machined surface. The performance of the self-propelled rotary tool is compared to that of other conventional tool designs with the same circular tool as well as cutting angles. The experimental results are used to define the optimum and safe cutting conditions for high performance machining of aerospace material using self-propelled tools. © 2003 Published by Elsevier B.V.},
   author = {H. A. Kishawy and C. E. Becze and D. G. McIntosh},
   issue = {3},
   journal = {Journal of Materials Processing Technology},
   keywords = {Chip,Force,Quality,Wear},
   month = {10},
   pages = {266-271},
   title = {Tool performance and attainable surface quality during the machining of aerospace alloys using self-propelled rotary tools},
   volume = {152},
   year = {2004},
}
@article{Dessoly2004,
   abstract = {This paper addresses modeling of the tool temperature distribution in self-propelled rotary tool (SPRT) machining of hardened steels. Since tool life is significantly influenced by cutting temperatures, a model is developed to analyze the heat transfer and temperature distribution in rotary tool turning of hardened 52100 steel (58 HRC). The model is based on the moving heat source theory of conduction and employs the finite element method (FEM) for its solution. The model is experimentally verified through measurements of the cutting tool temperature distribution using an infrared camera under different cutting conditions. Finally, both rotary and equivalent fixed tool cutting processes are compared in terms of cutting tool temperatures generated. © 2004 Elsevier Ltd. All rights reserved.},
   author = {Vincent Dessoly and Shreyes N. Melkote and Christophe Lescalier},
   issue = {14},
   journal = {International Journal of Machine Tools and Manufacture},
   keywords = {Hard turning,Rotary tool,Temperature distribution},
   month = {11},
   pages = {1463-1470},
   title = {Modeling and verification of cutting tool temperatures in rotary tool turning of hardened steel},
   volume = {44},
   year = {2004},
}
@web_page{,
   author = {Joanna Kossakowska and Krzysztof Jemielniak},
   journal = {5th CIRP Conference on High Performance Cutting},
   title = {Application of Self-Propelled Rotary Tools for Turning of Difficult-to-machine Materials - ScienceDirect},
   url = {https://www.sciencedirect.com/science/article/pii/S2212827112000777},
   year = {2012},
}
@article{Ezugwu2007,
   abstract = {Approximately two-thirds of all the superalloys produced are consumed by the aerospace industry for the manufacture of jet engines and associated components, mainly in the hot end of aircraft engines and land-based turbines. The remaining third of superalloy consumption is used by the chemical, medical and structural industries in applications requiring high temperature properties and/or exceptional corrosion resistance. Ability to retain high mechanical and chemical properties at elevated temperatures make superalloys ideal materials for use in both rotating and stationary components in the hot end of jet engines. These materials as well as structural ceramic and hardened steels pose formidable challenges for cutting tool materials during machining, hence they are referred to as difficult-to-cut. The basic difference between rotary cutting and conventional cutting is the movement of the cutting edge in addition to the main cutting and feed motions. Self-propelled rotary tools (SPRT) employs round inserts rotating continuously about its axis as a result of the driving motion impacted by the cutting force, thus minimising the effect of thermal energy along the entire edge and preventing excessive heating of a particular portion of the insert edge. Major benefits provided by SPRT include several hundred-fold increase in tool life, lower cutting temperature, higher metal removal rate, generation of fine surface finishes due to the circular cutting edge and improved machinability of difficult-to-cut materials such as nickel and titanium base alloys. Extremely low rate of flank wear can be obtained when machining aerospace superalloys, particularly titanium alloys, even at higher speed conditions with very negligible or no effect on the machined surfaces. This paper will provide an overview of developments in rotary tools, the principles of rotary cutting, structure and design of SPRT, factors influencing rotary tool life and detail information of practical machining data as well as analysis of tool failure modes and tool wear mechanisms in comparison to conventional (fixed tool) machining technique. © 2006 Elsevier B.V. All rights reserved.},
   author = {E. O. Ezugwu},
   issue = {1-3},
   journal = {Journal of Materials Processing Technology},
   keywords = {Abrasion and attrition wear mechanisms,Chipping and fatigue wear,Higher metal removal rates,Improved surface finish,Low flank wear rate,Machinability,Nickel alloy,Rotary cutting,Round inserts,Titanium alloy},
   month = {4},
   pages = {60-71},
   title = {Improvements in the machining of aero-engine alloys using self-propelled rotary tooling technique},
   volume = {185},
   year = {2007},
}
@article{Kishawy2003,
   abstract = {This paper presents a performance assessment of rotary tool during machining hardened steel. The investigation includes an analysis of chip morphology and modes of tool wear. The effect of tool geometry and type of cutting tool material on the tool self-propelled motion are also investigated. Several tool materials were tested for wear resistance including carbide, coated carbide, and ceramics. The self-propelled coated carbide tools showed superior wear resistance. This was demonstrated by evenly distributed flank wear with no evidence of crater wear. The characteristics of temperature generated during machining with the rotary tool are studied. It was shown that reduced tool temperature eliminates the diffusion wear and dominates the abrasion wear. Also, increasing the tool rotational speed shifted the maximum temperature at the chip-tool interface towards the cutting edge. © 2002 Elsevier Science Ltd. All rights reserved.},
   author = {H. A. Kishawy and J. Wilcox},
   issue = {4},
   journal = {International Journal of Machine Tools and Manufacture},
   keywords = {Chip morphology,Hard turning,Rotary tool,Tool wear},
   month = {3},
   pages = {433-439},
   title = {Tool wear and chip formation during hard turning with self-propelled rotary tools},
   volume = {43},
   year = {2003},
}
@article{Neugebauer2012,
   abstract = {The main goal in manufacturing is to minimize production time, cost, energy and resources while maintaining or even improving performance. The main influence on process efficiency in metal cutting can be related to the machining parameters, while performance also depends on the material behavior such as strain rate, machining temperature and cutting forces. In the past, industry was searching for higher efficiency mainly by using higher production rates, achieved with higher cutting conditions. However, during the last decade the production process chain has been controlled more and more by the near-net-shape of semifinished components and the use of high performance processes. This paper presents established and new applications of process modifications, substitutions, optimizations as well as the use of hybrid processes. In high performance cutting (HPC) additional elements play an important role, for example tool design and tool material, wear and tool life, coolant strategies, including dry cutting, MQL, cryogenic and high pressure flushing as well as the machine tool properties. Furthermore, this paper presents and discusses examples of the influence of process parameters in turning, milling, drilling, grinding and impact cutting under HPC conditions on the energy and resources efficiency in the automotive, aerospace, dies and moulds, micromachining and heavy duty industries. © 2012 The Authors.},
   author = {R. Neugebauer and W. Drossel and R. Wertheim and C. Hochmuth and M. Dix},
   issue = {1},
   journal = {Procedia CIRP},
   keywords = {Energy Efficiency,Flushing,Hybrid,Resources},
   pages = {3-16},
   publisher = {Elsevier B.V.},
   title = {Resource and energy efficiency in machining using high-performance and hybrid processes},
   volume = {1},
   year = {2012},
}
@article{Kossakowska2012,
   abstract = {The paper presents some aspects of turning of difficult to machine materials with self-propelled rotary tools (SPRT). The main criterion of the rotary tools performance was the attainable surface quality, waviness and adhered chips. The dependence of the insert rotational speed on cutting parameters was also investigated. Original method of measurement of this speed was developed. Obtained results showed, that the speed is lower than theoretical, reported in literature. Stiffness and run out of the tools were measured. Cutting force measurements revealed substantial run-out of the rotating insert. Some important drawbacks and limitations of SPRT application were revealed. © 2012 The Authors.},
   author = {Joanna Kossakowska and Krzysztof Jemielniak},
   doi = {10.1016/J.PROCIR.2012.04.076},
   issn = {2212-8271},
   issue = {1},
   journal = {Procedia CIRP},
   keywords = {Cutting forces,Machined surface quality,Self-propelled rotary tools,Turning},
   month = {1},
   pages = {425-430},
   publisher = {Elsevier},
   title = {Application of Self-Propelled Rotary Tools for Turning of Difficult-to-machine Materials},
   volume = {1},
   year = {2012},
}
@article{Sasahara2008,
   abstract = {This paper aims to realize the high-speed rotary dry cutting of an Inconel 718 at 500 m/min on a multitasking lathe which has an additional milling spindle with an X/Y/Z-axis and inclination control. A series of experiments were conducted and are discussed with respect to the tool face temperature analysis by FEM. It was verified that it is necessary to select an optimum inclination angle, tool rotation speed and tool diameter so as to enable the main cutting force direction to align with the highest rigidity direction of an applied rotary tool. Under preferable cutting conditions, the average tool rake face temperature measured by a thermograph camera was about 300 °C even at a high cutting speed of 500 m/min under dry cutting conditions, and the tool wear decreased dramatically compared with the conventional tools. © 2007 Elsevier Ltd. All rights reserved.},
   author = {Hiroyuki Sasahara and Atsushi Kato and Hiroshi Nakajima and Hiromasa Yamamoto and Toshiyuki Muraki and Masaomi Tsutsumi},
   issue = {7-8},
   journal = {International Journal of Machine Tools and Manufacture},
   keywords = {Difficult-to-cut materials,Rotary cutting,Temperature,Tool,Wear},
   month = {6},
   pages = {841-850},
   title = {High-speed rotary cutting of difficult-to-cut materials on multitasking lathe},
   volume = {48},
   year = {2008},
}
@article{Brinken2012,
   abstract = {The machine tool consumption is moving into the emerging markets. 60 % of the world machine tool consumption will be in Asia by 2014. The article provides the statistical facts and highlights the underlying growth dynamics in these markets. This move requires, besides a new economical and marketing approach, a new look at performance machine tool design. Different aspects concerning thermal, ergonomical and electrical design are highlighted. The trend results in a need for new market adapted product design specifications. © 2012 The Authors.},
   author = {Frank Brinken},
   issue = {1},
   journal = {Procedia CIRP},
   pages = {17-21},
   publisher = {Elsevier B.V.},
   title = {The continental drift in the machine tool industry},
   volume = {1},
   year = {2012},
}
